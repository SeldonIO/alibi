{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[source]](../api/alibi.explainers.kernel_shap.rst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree SHAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tree SHAP (**SH**apley **A**dditive ex**P**lanations) algorithm is based on the paper [From local explanations to global understanding with explainable AI for trees](https://www.nature.com/articles/s42256-019-0138-9) by Lundberg et al. and builds on the open source  [shap library](https://github.com/slundberg/shap) from the paper's first author.\n",
    "\n",
    "\n",
    "The algorithm provides human interpretable explanations suitable for regression and classification of models with tree structure applied to tabular data. This method is a member of the *additive feature attribution methods* class; feature attribution refers to the fact that the change of an outcome to be explained (e.g., a class probability in a classification problem) with respect to a *baseline* (e.g., average prediction probability for that class in the training set) can be attributed in different proportions to the model input features. \n",
    "\n",
    "A simple illustration of the explanation process is shown in Figure 1. Here we see depicted a tree-based model which takes as an input features such as `Age`, `BMI` or `Blood pressure` and outputs `Mortality risk score`, a contiuous value. Lets assume that we aim to explain the difference between and observed outcome and no risk, corresponding to a base value of `0.0`. Using the Tree SHAP algorithm, we attribute the `4.0` difference to the input features. Because the sum of the attribute values equals `output - base value`, this method is _additive_. We can see for example that the `Sex` feature contributes negatively to this prediction whereas the remainder of the features have a positive contribution (i.e., increase the mortality risk). For explaining this particular data point, the `Blood Pressure` feature seems to have the largest effect, and corresponds to an increase in the mortality risk. See our example on how to perform explanations with this algorithm and visualise the results using the `shap` library visualisations [here](../examples/tree_shap_adult_xgb.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](tree_exp_overview_img.png)\n",
    "Figure 1: Cartoon ilustration of explanation models with Tree SHAP.\n",
    "\n",
    "Image Credit: Scott Lundberg (see source [here](https://www.nature.com/articles/s42256-019-0138-9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- summary of arguments to `explain` and what they mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Path-dependent perturbation algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Initialiastion and Fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interventional perturbation algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Explaining model output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Initialiastion and Fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explaining loss functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Initialisation and fit "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shapley interaction values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Initialisation and fit "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Miscellaneous"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Runtime considerations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Adjusting the size of the reference dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm automatically warns the user if a background dataset size of more than `300` samples is passed. If the runtime of an explanation with the original dataset is too large, then the algorithm can automatically subsample the background dataset during the `fit` step. This can be achieve by specifying the fit step as \n",
    "\n",
    "```python\n",
    "explainer.fit(\n",
    "    X_reference,\n",
    "    summarise_background=True,\n",
    "    n_background_samples=150,\n",
    ")\n",
    "```\n",
    "\n",
    "or \n",
    "```python\n",
    "explainer.fit(\n",
    "    X_reference,\n",
    "    summarise_background='auto'\n",
    ")\n",
    "```\n",
    "\n",
    "The `auto` option will select `300` examples, whereas using the boolean argument allows the user to directly control the size of the reference set. If categorical variables or grouping options are specified, the algorithm uses subsampling of the data. Otherwise, a kmeans clustering algorithm is used to select the background dataset and the samples are weighted according to the frequency of occurence of the cluster they are assigned to, which is reflected in the `expected_value` attribute of the explainer. \n",
    "\n",
    "As describe above, the explanations are performed with respect to the expected (or weighted-average) output over this dataset so the shap values will be affected by the dataset selection. We recommend experimenting with various ways to choose the background dataset before deploying explanations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The dimensionality of the data and the number of samples used in shap value estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How it works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Recall that, for a model $f$, the Kernel SHAP algorithm [[1]](#References) explains a certain outcome with respect to a chosen reference (or an expected value) by estimating the shap values of each feature $i \\in F = \\{1, ..., M\\} $, as follows:\n",
    "\n",
    "- enumerate all subsets $S$ of the set $F   \\setminus \\{i\\}$ \n",
    "- for each $S \\subseteq F \\setminus \\{i\\}$, compute the contribution of feature $i$ as $C(i|S) = f(S \\cup \\{i\\}) - f(S)$\n",
    "- compute the shap value according to\n",
    "$$\n",
    "\\phi_i := \\frac{1}{M} \\sum \\limits_{{S \\subseteq F \\setminus \\{i\\}}} \\frac{1}{ M - 1 \\choose |S|} C(i|S).         \n",
    "\\tag{1}\n",
    "$$\n",
    "\n",
    "\n",
    "Since most models do not accept arbitrary patterns of missing values at inference time, $f(S)$ needs to be approximated. The original formulation of the Kernel Shap algorithm [[1]](#References) proposes to compute $f(S)$ as the _observational conditional expectation_\n",
    "\n",
    "$$\n",
    "f(S) := \\mathbb{E}\\left[f(\\mathbf{x}_{S}, \\mathbf{X}_{\\bar{S}} | \\mathbf{X}_S = \\mathbf{x}_S) \\right]\n",
    "\\tag{2}\n",
    "$$\n",
    "\n",
    "where the expectation is taken over a _background dataset_ after conditioning. Computing this expectation involves drawing sufficiently many samples from $\\mathbf{X}_{\\bar{S}}$ for every sample from $\\mathbf{X}_S$, which is expensive. Instead, $(2)$ is approximated by \n",
    "\n",
    "$$\n",
    "f(S) := \\mathbb{E} \\left[f(\\mathbf{x}_{S}, \\mathbf{X}_{\\bar{S}})\\right]\n",
    "$$\n",
    "\n",
    "where features in a subset $S$ are fixed and features in $\\bar{S}$ are sampled from the background dataset. This quantity is referred to as _marginal_ or *interventional conditional expectation*, to emphasise that setting features in $S$ to the values $\\mathbf{x}_{S}$ can be viewed as an intervention on the instance to be explained.\n",
    "\n",
    "As described in [[2]](#References), if estimating impact of a feature $i$ on the function value by $\\mathbb{E} \\left[ f | X_i = x_i \\right]$, one should bear in mind that observing $X_i = x_i$ changes the distribution of the features $X_{j \\neq i}$ if there exist correlation amongst these variables. Hence, if the conditional expectation if used to estimate $f(S)$, the Shapley values might not be accurate since they also depend on the remaining variables, effect which becomes important if there are strong correlations amongst the independent variables. Furthermore, the authors show that estimating $f(S)$ using the conditional expectation violates the *sensitivity principle*, according to which the Shapley value of a redundant variable should be 0. On the other hand, the intervention breaks the dependencies, ensuring that the sensitivity holds. One potential drawback of this method is that setting a subset of values to certain values without regard to the values of the features in the complement (i.e., $\\bar{S}$) can generate instances that are outside the training data distribution, which will affect the model prediction and hence the contributions.\n",
    "\n",
    "The following sections detail how these methods work and how, unlike Kernel SHAP, compute the exact shap values in polynomial time. The algorithm estimating contributions using interventional expectations is presented, with the remaining sections being dedicated to presenting an approximate algorithm for evaluating the interventional expectation that does not require a background dataset and Shapley interaction values.\n",
    "\n",
    "<a id='source_1'></a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interventional feature perturbation\n",
    "<a id='interventional'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The process of estimating the contributions using the interventional algorithm is depicted in Figure 2. \n",
    "\n",
    "An instance to be explained $x$ is perturbed using a reference sample $r$ by the values of the features $F1$, $F3$ and $F5$ in $x$ with the corresponding values in $r$. Therefore, one defines the set $F$ in the previous section as $F = \\{ j: x_{j} \\neq r_{j}\\}$ for this case. Note that these are the only features for which one can estimate a contribution here; the same path is followed for features $F2$ and $F4$for both the original and the perturbed sample, these features do not contribute to explain the difference between the observed outcome ($v_6$) and the outcome that would have been observed if the tree had been traversed according to the reference $(v_{10})$.\n",
    "\n",
    "First one notices that the contribution of a feature at a node depends on the contribution of the features associated to the descendants of the node along a decision path. Therefore, if a contribution is assigned to each leave, one can aggregate those at internal nodes to obtain the contribution associated with each feature associated with an internal node. The algorithm terminates after the aggregation at the root is computed.\n",
    "\n",
    "Note that each leaf, as well as internal node, will have a both a positive and a negative contribution, marked in <span style=\"color:green\">green</span> and <span style=\"color:red\">red</span>, respectively, on the diagram below. To understand why, consider the subtree with the root at node $8$. To estimate the contribution of $F5$, one considers a set $S = \\varnothing$ and observes the value of node $10$, and weights that with the combinatorial factor from equation  $(1)$ where <sup>[(1)](#Footnotes) </sup> $M-1 = 1$ and $|S|=0$ and a positive contribution from node $9$. To compute the contributions of the upstream features correctly, one has to bear in mind that, for example, one needs to consider the term $-f(S={F5})$ when computing the contribution of $F1$. Hence, node $9$ will pass to its parents an additional negative contribution computed for $M-1 = 1$ and $|S|=1$. The positive and negative contributions of an internal node are computed by summing the corresponding nodes of the children nodes.     \n",
    "\n",
    "\n",
    "Thus, to compute the attribution for feature $F1$, the  summation over $-f(S)$ can be obtained from the left subtree whereas the summation over $f(S \\cup \\{1\\})$ can be computed from the right sub-tree, since the semantics of visiting node $1$ after node $0$ is that the feature $1$ is missing (i.e., is not in $S$). The negative contribution will contain a negative component obtained by considering feature $F5$ in isolation (given by node $10$), and a component that arises due to the need to take into consideration feature $F1$, in isolation. Thus, the negative contribution of node $1$ represents the negative terms in the summation in equation $(1)$, and the summation is over the elements of $\\{\\{F5\\}\\ , \\varnothing\\}$. A similar reasoning for the right-subtree allows to calculate the positive terms in the summation.\n",
    "\n",
    "\n",
    "<a id='f_1'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](interventional_tree_shap_new.png)\n",
    "Figure 2: Ilustration of the feature contribution and expected value estimation process using interventional perturbation Tree SHAP. The positive and the negative contributions of a node are represented in <span style=\"color:green\">green</span> and <span style=\"color:red\">red</span>, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Summary of what the difference to the other algorithm is - aka how we estimate the shap values using this method\n",
    "- Foreground sample and background sample concept\n",
    "- State complexity\n",
    "- Applying the interventional feature perturbation algorithm is equivalent to making an independence assumption between the set of features corresponding to the nodes where the foreground sample and the background samples are split in the same subtree and the set of features corresponding to nodes where \n",
    "- Explaining loss functions\n",
    "- Other can only explain output, we can explain a nonlinear transformation on this one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is approximated using _interventional conditional expectations_ . For Kernel SHAP, these expectations are estimated by replacing the _missing values_ ($\\bar{S}$) with values from a _background dataset_ and taking an expectation over the model output. In the context of tree models, this approximation can be performed by: \n",
    "- traversing the tree with the example to be explained (sometimes referred to as a *foreground sample*) and a sample from the background dataset. This is algorithm is known as interventional feature perturbation\n",
    "- without a background dataset, by exploiting the tree coverage information (i.e., information about how the training data was partitioned by the tree) along with the feature dependencies encoded in the tree paths\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Path dependent feature perturbation\n",
    "<a id='path_dependent'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A way to compute $f(S)$ given an instance $x$ and a set of missing features $\\bar{S}$ is to recursively follow the decision path through the tree and:\n",
    "- return the node value if a split on a feature $i \\in S$ is performed\n",
    "- take a weighted average of the values returned by children if $i \\in \\bar{S}$, where the weighing factor is equal to the proportion of training examples flowing down each branch. This proportion is a property of each node, sometimes referred to as _weight_ or _cover_. \n",
    "\n",
    "Therefore, in the path-dependent perturbation method, we compute the conditional expectations with respect to the training data distribution by weighting the leaf values according to the proportion of the training examples that flow to that leaf.\n",
    "\n",
    "To avoid repeating the above recursion $M2^M$ times, one first notices that for a single decision tree, applying a perturbation would result in the sample ending up in a different leaf. Therefore, following each path from the root to a leaf in the tree is equivalent to perturbing subsets of features of varying cardinalities. Consequently, each leaf will contain a certain proportion of all possible subsets $S \\subseteq F$. Therefore, to compute the Shap values, the following quantities are computed at each leaf, *for every feature $i$ on the path leading to that leaf*:\n",
    "- the proportion of subsets $S$ at the leaf that contain $i$ and the proportion of subsets $S$ that do not contain $i$\n",
    "- for each cardinality, the proportion of the sets of that cardinality contained at the leaf. Tracking each cardinality  as opposed to a single count of subsets falling into a given leaf is necessary since it allows to apply the weighting factor in equation (1), which depends on the subset size, $|S|$.\n",
    "\n",
    "Using the above quantities, one can compute the _contribution_ of each leaf to the Shapley value of every feature. This algorithm has complexity $O(TLD^2)$ for an ensamble of trees where $L$ is the number of leaves, $T$ the number of trees in the ensamble and $D$ the maximum tree depth. If the tree is balanced, then $D=\\log L$ and the complexity of our algorithm is $O(TL\\log^2L)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\phi_i := \\sum \\limits_{j=1}^L \\sum \\limits_{P \\in {S_t}} \\frac {w(|P|, j)}{ M_j {M_j \\choose |P|}} \\left[f(P \\cup \\{i\\}) - f(P)\\right]\n",
    "\\tag{2}\n",
    "$$\n",
    "\n",
    "where $S_t$ is the set of present feature subsets at leaf $j$, $M_j$ is the length of the path and $w(|P|, j)$ is the proportion of all subsets of cardinality $P$ at leaf $j$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Include discussion about zero and one fractions and cover\n",
    "- Include discussion about cover/Hessian \n",
    "- Make sure above is actually correct, add missing stuff to the equation and put it in context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that although a background dataset is not provided, the expected values is computed using the node cover information, stored at each node. The computation proceeds recursively, starting at the root. The contribution of a note to the expected value of the tree is a function of the expected values of the children and is computed as follows:\n",
    "\n",
    "$$\n",
    "c_j = \\frac{c_{r(j)}r_{r(j)} + c_{l(j)}r_{l(j)}}{r_j}\n",
    "$$\n",
    "\n",
    "where $j$ denotes the node index, $c_j$ denotes the node expected value, $r_j$ is the cover of the $j$th node and $r(j)$ and $l(j)$ represent the indices of the right and left children, respectively. The expected value used by the tree is simply $c_{root}$. Note that for tree ensamles, the expected values of the ensamble members is weighted according to the tree weight and the weighted expected values of all trees are summed to obtain a single value.\n",
    "\n",
    "The cover depends on the objective function and the model chosen. For example, in a gradient boosted tree trained with squared loss objective, $r_j$ is simply the number of training examples flowing through $j$. For an arbitrary objective, this is the sum of the hessian of the loss function evaluated at each point flowing through $j$, as explained [here](../../../examples/tree_shap_adult_xgb.ipynb#optimisation). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Explaining loss functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One advantage of the interventional approach is that it allows to approximately transform the shap values to account for nonlinear transformation of outputs, such as the loss function. Recall that given $\\phi_i, ..., \\phi_M$ the local accuracy property gurantees that given $\\phi_0 = \\mathbb{E}[f(x)]$\n",
    "\n",
    "$$\n",
    "f(x) = \\phi_0 + \\sum \\limits_{i=1}^M \\phi_i.\n",
    "\\tag{3}\n",
    "$$\n",
    "\n",
    "Hence, in order to account for the effect of the nonlinear transformation $h$, one has to find the functions $g_0, ..., g_M$ such that\n",
    "\n",
    "$$\n",
    "h(f(x)) = g(\\phi_0) + \\sum \\limits_{i=1}^M g_i(\\phi_i)\n",
    "\\tag{4}\n",
    "$$\n",
    "\n",
    "For simplicity, let $y=h(x)$. Then using a first-order Taylor series expansion around $\\mathbb{E}[y]$ one obtains \n",
    "\n",
    "$$\n",
    "f(y) \\approx f(\\mathbb{E}[y]) + \\frac{\\partial f(y) }{\\partial y} \\Bigr|_{y=\\mathbb{E}[y]}(y - \\mathbb{E}[y]).\n",
    "\\tag{5}\n",
    "$$\n",
    "\n",
    "Substituting $(3)$ in $(5)$ and comparing coefficients with $(4)$ yields\n",
    "\n",
    "$$\n",
    "g_0 \\approx f(\\mathbb{E}[y]) \\\\\n",
    "g_i \\approx \\phi_i \\frac{\\partial f(y) }{\\partial y} \\Bigr|_{y=\\mathbb{E}[y]} .\n",
    "$$\n",
    "\n",
    "Hence, an approximate correction is given by simply scaling the shap values using the gradient of the nonlinear function. Note that in practice one may take the Taylor series expasion at a reference point $r$ from the background dataset and average over the entire background dataset to compute the scaling factor. This introduces an additional source of noise since $f(\\mathbb{E}[y]) = \\mathbb{E}[f(y)]$ only for linear functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shapley interaction values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Describe Shapley interactions\n",
    "- Describe the algorithm to compute them with reference to the path-dependent algorithm\n",
    "- State computational complexity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison to other methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Feature importances (traditional)\n",
    "    - inconsistency and arbitrary choice of explanation concept\n",
    "    - global only, not that much info\n",
    "- Kernel SHAP\n",
    "    - post-hoc modelling so slow, but can be shown to converge "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Footnotes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Footnotes'></a>\n",
    "\n",
    "[(1)](#f_1): Note that, $N$ in the diagram denotes $M$ in equation $(1)$. The value of $1$ occurs because there are only $N = 2$ features where the reference and instance to be explained differ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='References'></a>\n",
    "\n",
    "[[1]](#source_1) Lundberg, S.M. and Lee, S.I., 2017. A unified approach to interpreting model predictions. In Advances in neural information processing systems (pp. 4765-4774).\n",
    "\n",
    "[[2]](#source_2) Janzing, D., Minorics, L. and Bl√∂baum, P., 2019. Feature relevance quantification in explainable AI: A causality problem. arXiv preprint arXiv:1910.13413."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuous Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Introductory example: Kernel SHAP on Wine dataset](../examples/kernel_shap_wine_intro.nblink)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Comparison with interpretable models](../examples/kernel_shap_wine_lr.nblink)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mixed Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Handling categorical variables with Kernel SHAP: an income prediction application](../examples/kernel_shap_adult_lr.nblink)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Handlling categorical variables with Kernel SHAP: fitting explainers on data before pre-processing](../examples/kernel_shap_adult_categorical_preproc.nblink)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
