{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Counterfactual with Reinforcement Learning (CFRL) on MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method is described in [Model-agnostic and Scalable Counterfactual Explanations via Reinforcement Learning](https://arxiv.org/abs/2106.02597) and can generate counterfactual instances for any black-box model. The usual optimization procedure is transformed into a learnable process allowing to generate batches of counterfactual instances in a single forward pass even for high dimensional data. The training pipeline is model-agnostic and relies only on prediction feedback by querying the black-box model. Furthermore, the method allows target and feature conditioning. \n",
    "\n",
    "**We exemplify the use case for the TensorFlow backend. This means that all models: the autoencoder, the actor and the critic are TensorFlow models. Our implementation supports PyTorch backend as well.**\n",
    "\n",
    "CFRL uses [Deep Deterministic Policy Gradient (DDPG)](https://arxiv.org/abs/1509.02971) by interleaving a state-action function approximator called critic, with a learning an approximator called actor to predict the optimal action. The method assumes that the critic is differentiable with respect to the action argument, thus allowing to optimize the actor's parameters efficiently through gradient-based methods.\n",
    "\n",
    "The DDPG algorithm requires two separate networks, an actor $\\mu$ and a critic $Q$. Given the encoded representation of the input instance $z = enc(x)$, the model prediction $y_M$, the target prediction\n",
    "$y_T$ and the conditioning vector $c$, the actor outputs the counterfactual’s latent representation $z_{CF} = \\mu(z, y_M, y_T, c)$. The decoder then projects the embedding $z_{CF}$ back to the original input space,\n",
    "followed by optional post-processing.\n",
    "\n",
    "The training step consists of simultaneously optimizing the actor and critic networks. The critic regresses on the reward $R$ determined by the model prediction, while the actor maximizes the critic’s output for the given instance through $L_{max}$. The actor also minimizes two objectives to encourage the generation of sparse, in-distribution counterfactuals. The sparsity loss $L_{sparsity}$ operates on the decoded counterfactual $x_{CF}$ and combines the $L_1$ loss over the standardized numerical features and the $L_0$ loss over the categorical ones. The consistency loss $L_{consist}$ aims to encode the counterfactual $x_{CF}$ back to the same latent representation where it was decoded from and helps to produce in-distribution counterfactual instances.Formally, the actor's loss can be written as:\n",
    "$L_{actor} = L_{max} + \\lambda_{1}L_{sparsity} + \\lambda_{2}L_{consistency}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "Note\n",
    "    \n",
    "To enable support for CounterfactualRLTabular with tensorflow backend, you may need to run\n",
    "    \n",
    "```bash\n",
    "pip install alibi[tensorflow]\n",
    "```\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-25 19:54:00.712332: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Dict\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "from alibi.explainers import CounterfactualRL\n",
    "from alibi.models.tensorflow import AE\n",
    "from alibi.models.tensorflow import Actor, Critic\n",
    "from alibi.models.tensorflow import MNISTEncoder, MNISTDecoder, MNISTClassifier\n",
    "from alibi.explainers.cfrl_base import Callback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11490434/11490434 [==============================] - 0s 0us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-25 19:54:12.023820: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Define constants.\n",
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 1024\n",
    "\n",
    "# Load MNIST dataset.\n",
    "(X_train, Y_train), (X_test, Y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Expand dimensions and normalize.\n",
    "X_train = np.expand_dims(X_train, axis=-1).astype(np.float32) / 255.\n",
    "X_test = np.expand_dims(X_test, axis=-1).astype(np.float32) / 255.\n",
    "\n",
    "# Define trainset.\n",
    "trainset_classifier = tf.data.Dataset.from_tensor_slices((X_train, Y_train))\n",
    "trainset_classifier = trainset_classifier.shuffle(buffer_size=BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "\n",
    "# Define testset.\n",
    "testset_classifier = tf.data.Dataset.from_tensor_slices((X_test, Y_test))\n",
    "testset_classifier = testset_classifier.shuffle(buffer_size=BUFFER_SIZE).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define and train CNN classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "938/938 [==============================] - 35s 37ms/step - loss: 0.2448 - sparse_categorical_accuracy: 0.9239\n",
      "Epoch 2/5\n",
      "938/938 [==============================] - 40s 42ms/step - loss: 0.0860 - sparse_categorical_accuracy: 0.9728\n",
      "Epoch 3/5\n",
      "938/938 [==============================] - 36s 39ms/step - loss: 0.0621 - sparse_categorical_accuracy: 0.9797\n",
      "Epoch 4/5\n",
      "938/938 [==============================] - 42s 45ms/step - loss: 0.0517 - sparse_categorical_accuracy: 0.9838\n",
      "Epoch 5/5\n",
      "938/938 [==============================] - 39s 42ms/step - loss: 0.0422 - sparse_categorical_accuracy: 0.9862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: tensorflow/MNIST_classifier/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: tensorflow/MNIST_classifier/assets\n"
     ]
    }
   ],
   "source": [
    "# Number of classes.\n",
    "NUM_CLASSES = 10\n",
    "EPOCHS = 5\n",
    "\n",
    "# Define classifier path and create dir if it doesn't exist.\n",
    "classifier_path = os.path.join(\"tensorflow\", \"MNIST_classifier\")\n",
    "if not os.path.exists(classifier_path):\n",
    "    os.makedirs(classifier_path)\n",
    "\n",
    "# Construct classifier. This is the classifier used in the paper experiments.\n",
    "classifier = MNISTClassifier(output_dim=NUM_CLASSES)\n",
    "\n",
    "# Define optimizer and loss function\n",
    "optimizer = keras.optimizers.Adam(learning_rate=1e-3)\n",
    "loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "# Complile the model.\n",
    "classifier.compile(optimizer=optimizer, \n",
    "                   loss=loss,\n",
    "                   metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n",
    "\n",
    "\n",
    "if len(os.listdir(classifier_path)) == 0:\n",
    "    # Fit and save the classifier.\n",
    "    classifier.fit(trainset_classifier, epochs=EPOCHS)\n",
    "    classifier.save(classifier_path)\n",
    "else:\n",
    "    # Load the classifier if already fitted.\n",
    "    classifier = keras.models.load_model(classifier_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 2s 8ms/step - loss: 0.0338 - sparse_categorical_accuracy: 0.9889\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the classifier\n",
    "loss, accuracy = classifier.evaluate(testset_classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the predictor (black-box)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've trained the CNN classifier, we can define the black-box model. Note that the output of the black-box is a distribution which can be either a soft-label distribution (probabilities/logits for each class) or a hard-label distribution (one-hot encoding). Internally, CFRL takes the `argmax`. Moreover the output **DOES NOT HAVE TO BE DIFFERENTIABLE**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define predictor function (black-box) used to train the CFRL\n",
    "def predictor(X: np.ndarray):\n",
    "    Y = classifier(X).numpy()\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define and train autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of directly modeling the perturbation vector in the potentially high-dimensional input space, we first train an autoencoder. The weights of the encoder are frozen and the actor applies the\n",
    "counterfactual perturbations in the latent space of the encoder. The pre-trained decoder maps the counterfactual embedding back to the input feature space. \n",
    "\n",
    "The autoencoder follows a standard design. The model is composed from two submodules, the encoder and the decoder. The forward pass consists of passing the input to the encoder, obtain the input embedding and pass the embedding through the decoder.\n",
    "\n",
    "```python\n",
    "class AE(keras.Model):\n",
    "    def __init__(self, encoder: keras.Model, decoder: keras.Model, **kwargs) -> None:\n",
    "        super().__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def call(self, x: tf.Tensor, **kwargs):\n",
    "        z = self.encoder(x)\n",
    "        x_hat = self.decoder(z)\n",
    "        return x_hat\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define autoencoder trainset.\n",
    "trainset_ae = tf.data.Dataset.from_tensor_slices(X_train)\n",
    "trainset_ae = trainset_ae.map(lambda x: (x, x))\n",
    "trainset_ae = trainset_ae.shuffle(buffer_size=BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "\n",
    "# Define autoencode testset.\n",
    "testset_ae = tf.data.Dataset.from_tensor_slices(X_test)\n",
    "testset_ae = testset_ae.map(lambda x: (x, x))\n",
    "testset_ae = testset_ae.shuffle(buffer_size=BUFFER_SIZE).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "938/938 [==============================] - 42s 44ms/step - loss: 0.2013\n",
      "Epoch 2/50\n",
      "938/938 [==============================] - 43s 46ms/step - loss: 0.1352\n",
      "Epoch 3/50\n",
      "938/938 [==============================] - 41s 44ms/step - loss: 0.1247\n",
      "Epoch 4/50\n",
      "938/938 [==============================] - 40s 42ms/step - loss: 0.1188\n",
      "Epoch 5/50\n",
      "938/938 [==============================] - 41s 43ms/step - loss: 0.1147\n",
      "Epoch 6/50\n",
      "938/938 [==============================] - 40s 43ms/step - loss: 0.1118\n",
      "Epoch 7/50\n",
      "938/938 [==============================] - 43s 46ms/step - loss: 0.1096\n",
      "Epoch 8/50\n",
      "938/938 [==============================] - 42s 44ms/step - loss: 0.1078\n",
      "Epoch 9/50\n",
      "938/938 [==============================] - 43s 46ms/step - loss: 0.1064\n",
      "Epoch 10/50\n",
      "938/938 [==============================] - 43s 46ms/step - loss: 0.1051\n",
      "Epoch 11/50\n",
      "938/938 [==============================] - 43s 46ms/step - loss: 0.1040\n",
      "Epoch 12/50\n",
      "938/938 [==============================] - 41s 44ms/step - loss: 0.1030\n",
      "Epoch 13/50\n",
      "938/938 [==============================] - 44s 47ms/step - loss: 0.1022\n",
      "Epoch 14/50\n",
      "938/938 [==============================] - 43s 46ms/step - loss: 0.1015\n",
      "Epoch 15/50\n",
      "938/938 [==============================] - 42s 45ms/step - loss: 0.1009\n",
      "Epoch 16/50\n",
      "938/938 [==============================] - 41s 44ms/step - loss: 0.1002\n",
      "Epoch 17/50\n",
      "938/938 [==============================] - 41s 43ms/step - loss: 0.0997\n",
      "Epoch 18/50\n",
      "682/938 [====================>.........] - ETA: 11s - loss: 0.0989"
     ]
    }
   ],
   "source": [
    "# Define autoencoder path and create dir if it doesn't exist.\n",
    "ae_path = os.path.join(\"tensorflow\", \"MNIST_autoencoder\")\n",
    "if not os.path.exists(ae_path):\n",
    "    os.makedirs(ae_path)\n",
    "\n",
    "# Define latent dimension.\n",
    "LATENT_DIM = 64\n",
    "EPOCHS = 50\n",
    "    \n",
    "# Define autoencoder.\n",
    "ae = AE(encoder=MNISTEncoder(latent_dim=LATENT_DIM),\n",
    "        decoder=MNISTDecoder())\n",
    "\n",
    "# Define optimizer and loss function.\n",
    "optimizer = keras.optimizers.Adam(learning_rate=1e-3)\n",
    "loss = keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "\n",
    "# Compile autoencoder.\n",
    "ae.compile(optimizer=optimizer, loss=loss)\n",
    "\n",
    "if len(os.listdir(ae_path)) == 0:\n",
    "    # Fit and save autoencoder.\n",
    "    ae.fit(trainset_ae, epochs=EPOCHS)\n",
    "    ae.save(ae_path)\n",
    "else:\n",
    "    # Load the model.\n",
    "    ae = keras.models.load_model(ae_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define number of samples to be displayed\n",
    "NUM_SAMPLES = 5\n",
    "\n",
    "# Get some random samples from test\n",
    "np.random.seed(0)\n",
    "indices = np.random.choice(X_test.shape[0], NUM_SAMPLES)\n",
    "inputs = [X_test[i].reshape(1, 28, 28, 1) for i in indices]\n",
    "inputs = np.concatenate(inputs, axis=0)\n",
    "\n",
    "# Pass samples through the autoencoder\n",
    "inputs_hat = ae(inputs).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot inputs and reconstructions.\n",
    "plt.rcParams.update({'font.size': 22})\n",
    "fig, ax = plt.subplots(2, NUM_SAMPLES, figsize=(25, 10))\n",
    "\n",
    "for i in range(NUM_SAMPLES):\n",
    "    ax[0][i].imshow(inputs[i], cmap='gray')\n",
    "    ax[1][i].imshow(inputs_hat[i], cmap='gray')\n",
    "    \n",
    "text1 = ax[0][0].set_ylabel(\"x\")\n",
    "text2 = ax[1][0].set_ylabel(\"x_hat\")        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counterfactual with Reinforcement Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants\n",
    "COEFF_SPARSITY = 7.5               # sparisty coefficient\n",
    "COEFF_CONSISTENCY = 0              # consisteny coefficient -> no consistency\n",
    "TRAIN_STEPS = 50000                # number of training steps -> consider increasing the number of steps\n",
    "BATCH_SIZE = 100                   # batch size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define and fit the explainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define explainer.\n",
    "explainer = CounterfactualRL(predictor=predictor,\n",
    "                             encoder=ae.encoder,\n",
    "                             decoder=ae.decoder,\n",
    "                             latent_dim=LATENT_DIM,\n",
    "                             coeff_sparsity=COEFF_SPARSITY,\n",
    "                             coeff_consistency=COEFF_CONSISTENCY,\n",
    "                             train_steps=TRAIN_STEPS,\n",
    "                             batch_size=BATCH_SIZE,\n",
    "                             backend=\"tensorflow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the explainer\n",
    "explainer = explainer.fit(X=X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test explainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate counterfactuals for some test instances.\n",
    "explanation = explainer.explain(X_test[0:200], Y_t=np.array([2]), batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, NUM_SAMPLES, figsize=(25, 10))\n",
    "\n",
    "for i in range(NUM_SAMPLES):\n",
    "    ax[0][i].imshow(explanation.data['orig']['X'][i], cmap='gray')\n",
    "    ax[1][i].imshow(explanation.data['cf']['X'][i], cmap='gray')\n",
    "    \n",
    "    ax[0][i].set_xlabel(\"Label: \" + str(explanation.data['orig']['class'][i]))\n",
    "    ax[1][i].set_xlabel(\"Label: \" + str(explanation.data['cf']['class'][i]))\n",
    "    \n",
    "\n",
    "text1 = ax[0][0].set_ylabel(\"X\")\n",
    "text2 = ax[1][0].set_ylabel(\"X_hat\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logging is clearly important when dealing with deep learning models. Thus, we provide an interface to write custom callbacks for logging purposes after each training step which we defined [here](../api/alibi.explainers.cfrl_base.rst#alibi.explainers.cfrl_base.Callback). In the following cells we provide some example to log in **Weights and Biases**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logging reward callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RewardCallback(Callback):\n",
    "    def __call__(self,\n",
    "                 step: int, \n",
    "                 update: int, \n",
    "                 model: CounterfactualRL,\n",
    "                 sample: Dict[str, np.ndarray],\n",
    "                 losses: Dict[str, float]):\n",
    "        if step % 100 != 0:\n",
    "            return\n",
    "        \n",
    "        # Get the counterfactual and target.\n",
    "        X_cf = sample[\"X_cf\"]\n",
    "        Y_t = sample[\"Y_t\"]\n",
    "        \n",
    "        # Get prediction label.\n",
    "        Y_m_cf = predictor(X_cf)\n",
    "        \n",
    "        # Compute reward\n",
    "        reward = np.mean(model.params[\"reward_func\"](Y_m_cf, Y_t))\n",
    "        wandb.log({\"reward\": reward})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logging images callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImagesCallback(Callback):\n",
    "    def __call__(self,\n",
    "                 step: int, \n",
    "                 update: int, \n",
    "                 model: CounterfactualRL,\n",
    "                 sample: Dict[str, np.ndarray],\n",
    "                 losses: Dict[str, float]):\n",
    "        # Log every 100 steps\n",
    "        if step % 100 != 0:\n",
    "            return\n",
    "        \n",
    "        # Defie number of samples to be displayed.\n",
    "        NUM_SAMPLES = 10\n",
    "        \n",
    "        X = sample[\"X\"][:NUM_SAMPLES]        # input instance\n",
    "        X_cf = sample[\"X_cf\"][:NUM_SAMPLES]  # counterfactual\n",
    "        diff = np.abs(X - X_cf)              # differences\n",
    "        \n",
    "        Y_m = sample[\"Y_m\"][:NUM_SAMPLES].astype(int)   # input labels\n",
    "        Y_t = sample[\"Y_t\"][:NUM_SAMPLES].astype(int)   # target labels\n",
    "        Y_m_cf = predictor(X_cf).astype(int)            # counterfactual labels\n",
    "        \n",
    "        # Concatentate images,\n",
    "        X = np.concatenate(X, axis=1)\n",
    "        X_cf = np.concatenate(X_cf, axis=1)\n",
    "        diff = np.concatenate(diff, axis=1)\n",
    "        \n",
    "        # Construct full image.\n",
    "        img = np.concatenate([X, X_cf, diff], axis=0)\n",
    "            \n",
    "        # Construct caption.\n",
    "        caption = \"\"\n",
    "        caption += \"Input:\\t%s\\n\" % str(list(np.argmax(Y_m, axis=1)))\n",
    "        caption += \"Target:\\t%s\\n\" % str(list(np.argmax(Y_t, axis=1)))\n",
    "        caption += \"Predicted:\\t%s\\n\" % str(list(np.argmax(Y_m_cf, axis=1)))\n",
    "        \n",
    "        # Log image.\n",
    "        wandb.log({\"samples\": wandb.Image(img, caption=caption)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logging losses callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossCallback(Callback):\n",
    "    def __call__(self,\n",
    "                 step: int, \n",
    "                 update: int, \n",
    "                 model: CounterfactualRL,\n",
    "                 sample: Dict[str, np.ndarray],\n",
    "                 losses: Dict[str, float]):\n",
    "        # Log evary 100 updates.\n",
    "        if (step + update) % 100 == 0:\n",
    "            wandb.log(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having defined the callbacks, we can define a new explainer that will include logging.\n",
    "\n",
    "```python\n",
    "import wandb\n",
    "\n",
    "# Initialize wandb.\n",
    "wandb_project = \"MNIST Counterfactual with Reinforcement Learning\"\n",
    "wandb.init(project=wandb_project)\n",
    "\n",
    "# Define explainer as before and include callbacks.\n",
    "explainer = CounterfactualRL(...,\n",
    "                             callbacks=[RewardCallback(), ImagesCallback()])\n",
    "\n",
    "# Fit the explainer.\n",
    "explainer.fit(X=X_train)\n",
    "\n",
    "# Close wandb.\n",
    "wandb.finish()\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
