{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[source]](../api/alibi.explainers.kernel_shap.rst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree SHAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tree SHAP (**SH**apley **A**dditive ex**P**lanations) algorithm is based on the paper [From local explanations to global understanding with explainable AI for trees](https://www.nature.com/articles/s42256-019-0138-9) by Lundberg et al. and builds on the open source  [shap library](https://github.com/slundberg/shap) from the paper's first author.\n",
    "\n",
    "\n",
    "The algorithm provides human interpretable explanations suitable for regression and classification of models with tree structure applied to tabular data. This method is a member of the *additive feature attribution methods* class; feature attribution refers to the fact that the change of an outcome to be explained (e.g., a class probability in a classification problem) with respect to a *baseline* (e.g., average prediction probability for that class in the training set) can be attributed in different proportions to the model input features. \n",
    "\n",
    "A simple illustration of the explanation process is shown in Figure 1. Here we see depicted a tree-based model which takes as an input features such as `Age`, `BMI` or `Blood pressure` and outputs `Mortality risk score`, a contiuous value. Lets assume that we aim to explain the difference between and observed outcome and no risk, corresponding to a base value of `0.0`. Using the Tree SHAP algorithm, we attribute the `4.0` difference to the input features. Because the sum of the attribute values equals `output - base value`, this method is _additive_. We can see for example that the `Sex` feature contributes negatively to this prediction whereas the remainder of the features have a positive contribution (i.e., increase the mortality risk). For explaining this particular data point, the `Blood Pressure` feature seems to have the largest effect, and corresponds to an increase in the mortality risk. See our example on how to perform explanations with this algorithm and visualise the results using the `shap` library visualisations [here](../examples/tree_shap_adult_xgb.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](tree_exp_overview_img.png)\n",
    "Figure 1: Cartoon ilustration of explanation models with Tree SHAP.\n",
    "\n",
    "Image Credit: Scott Lundberg (see source [here](https://www.nature.com/articles/s42256-019-0138-9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- summary of arguments to `explain` and what they mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Path-dependent perturbation algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Initialiastion and Fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interventional perturbation algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Explaining model output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Initialiastion and Fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explaining loss functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Initialisation and fit "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shapley interaction values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Initialisation and fit "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Miscellaneous"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Runtime considerations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Adjusting the size of the reference dataset [PENDING REVIEW]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm automatically warns the user if a background dataset size of more than `300` samples is passed. If the runtime of an explanation with the original dataset is too large, then the algorithm can automatically subsample the background dataset during the `fit` step. This can be achieve by specifying the fit step as \n",
    "\n",
    "```python\n",
    "explainer.fit(\n",
    "    X_reference,\n",
    "    summarise_background=True,\n",
    "    n_background_samples=150,\n",
    ")\n",
    "```\n",
    "\n",
    "or \n",
    "```python\n",
    "explainer.fit(\n",
    "    X_reference,\n",
    "    summarise_background='auto'\n",
    ")\n",
    "```\n",
    "\n",
    "The `auto` option will select `300` examples, whereas using the boolean argument allows the user to directly control the size of the reference set. If categorical variables or grouping options are specified, the algorithm uses subsampling of the data. Otherwise, a kmeans clustering algorithm is used to select the background dataset and the samples are weighted according to the frequency of occurence of the cluster they are assigned to, which is reflected in the `expected_value` attribute of the explainer. \n",
    "\n",
    "As describe above, the explanations are performed with respect to the expected (or weighted-average) output over this dataset so the shap values will be affected by the dataset selection. We recommend experimenting with various ways to choose the background dataset before deploying explanations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The dimensionality of the data and the number of samples used in shap value estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theoretical overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Recall that, for a model $f$, the Kernel SHAP algorithm [[1]](#References) explains a certain outcome with respect to a chosen reference (or an expected value) by estimating the shap values of each feature $i \\in F = \\{1, ..., M\\} $, as follows:\n",
    "\n",
    "- enumerate all subsets $S$ of the set $F   \\setminus \\{i\\}$ \n",
    "- for each $S \\subseteq F \\setminus \\{i\\}$, compute the contribution of feature $i$ as $C(i|S) = f(S \\cup \\{i\\}) - f(S)$\n",
    "- compute the shap value according to\n",
    "$$\n",
    "\\phi_i := \\frac{1}{M} \\sum \\limits_{{S \\subseteq F \\setminus \\{i\\}}} \\frac{1}{ M - 1 \\choose |S|} C(i|S).         \n",
    "\\tag{1}\n",
    "$$\n",
    "\n",
    "\n",
    "Since most models do not accept arbitrary patterns of missing values at inference time, $f(S)$ needs to be approximated. The original formulation of the Kernel Shap algorithm [[1]](#References) proposes to compute $f(S)$ as the _observational conditional expectation_\n",
    "\n",
    "$$\n",
    "f(S) := \\mathbb{E}\\left[f(\\mathbf{x}_{S}, \\mathbf{X}_{\\bar{S}} | \\mathbf{X}_S = \\mathbf{x}_S) \\right]\n",
    "\\tag{2}\n",
    "$$\n",
    "\n",
    "where the expectation is taken over a _background dataset_ after conditioning. Computing this expectation involves drawing sufficiently many samples from $\\mathbf{X}_{\\bar{S}}$ for every sample from $\\mathbf{X}_S$, which is expensive. Instead, $(2)$ is approximated by \n",
    "\n",
    "$$\n",
    "f(S) := \\mathbb{E} \\left[f(\\mathbf{x}_{S}, \\mathbf{X}_{\\bar{S}})\\right]\n",
    "$$\n",
    "\n",
    "where features in a subset $S$ are fixed and features in $\\bar{S}$ are sampled from the background dataset. This quantity is referred to as _marginal_ or *interventional conditional expectation*, to emphasise that setting features in $S$ to the values $\\mathbf{x}_{S}$ can be viewed as an intervention on the instance to be explained.\n",
    "\n",
    "As described in [[2]](#References), if estimating impact of a feature $i$ on the function value by $\\mathbb{E} \\left[ f | X_i = x_i \\right]$, one should bear in mind that observing $X_i = x_i$ changes the distribution of the features $X_{j \\neq i}$ if there exist correlation amongst these variables. Hence, if the conditional expectation if used to estimate $f(S)$, the Shapley values might not be accurate since they also depend on the remaining variables, effect which becomes important if there are strong correlations amongst the independent variables. Furthermore, the authors show that estimating $f(S)$ using the conditional expectation violates the *sensitivity principle*, according to which the Shapley value of a redundant variable should be 0. On the other hand, the intervention breaks the dependencies, ensuring that the sensitivity holds. One potential drawback of this method is that setting a subset of values to certain values without regard to the values of the features in the complement (i.e., $\\bar{S}$) can generate instances that are outside the training data distribution, which will affect the model prediction and hence the contributions.\n",
    "\n",
    "The following sections detail how these methods work and how, unlike Kernel SHAP, compute the exact shap values in polynomial time. The algorithm estimating contributions using interventional expectations is presented, with the remaining sections being dedicated to presenting an approximate algorithm for evaluating the interventional expectation that does not require a background dataset and Shapley interaction values.\n",
    "\n",
    "<a id='source_1'></a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interventional feature perturbation\n",
    "<a id='interventional'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process of estimating the contributions using the interventional algorithm is depicted in Figure 2. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](interventional_tree_shap_new.png)\n",
    "Figure 2: Ilustration of the feature contribution and expected value estimation process using interventional perturbation Tree SHAP. The positive and the negative contributions of a node are represented in <span style=\"color:green\">green</span> and <span style=\"color:red\">red</span>, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An instance to be explained $x$ is perturbed using a reference sample $r$ by the values of the features $F1$, $F3$ and $F5$ in $x$ with the corresponding values in $r$. Therefore, one defines the set $F$ in the previous section as $F = \\{ j: x_{j} \\neq r_{j}\\}$ for this case. Note that these are the only features for which one can estimate a contribution here; the same path is followed for features $F2$ and $F4$for both the original and the perturbed sample, these features do not contribute to explain the difference between the observed outcome ($v_6$) and the outcome that would have been observed if the tree had been traversed according to the reference $(v_{10})$.\n",
    "\n",
    "First one notices that the contribution of a feature at a node depends on the contribution of the features associated to the descendants of the node along a decision path. Therefore, if a contribution is assigned to each leave, one can aggregate those at internal nodes to obtain the contribution associated with each feature associated with an internal node. The algorithm terminates after the aggregation at the root is computed.\n",
    "\n",
    "Note that each leaf, as well as internal node, will have a both a positive and a negative contribution, marked in <span style=\"color:green\">green</span> and <span style=\"color:red\">red</span>, respectively, on the diagram below. To understand why, consider the subtree with the root at node $8$. To estimate the contribution of $F5$, one considers a set $S = \\varnothing$ and observes the value of node $10$, and weights that with the combinatorial factor from equation  $(1)$ where <sup>[(1)](#Footnotes) </sup> $M-1 = 1$ and $|S|=0$ and a positive contribution from node $9$. To compute the contributions of the upstream features correctly, one has to bear in mind that, for example, one needs to consider the term $-f(S={F5})$ when computing the contribution of $F1$. Hence, node $9$ will pass to its parents an additional negative contribution computed for $M-1 = 1$ and $|S|=1$. The positive and negative contributions of an internal node are computed by summing the corresponding nodes of the children nodes.     \n",
    "\n",
    "\n",
    "Thus, to compute the attribution for feature $F1$, the  summation over $-f(S)$ can be obtained from the left subtree whereas the summation over $f(S \\cup \\{1\\})$ can be computed from the right sub-tree, since the semantics of visiting node $1$ after node $0$ is that the feature $1$ is missing (i.e., is not in $S$). The negative contribution will contain a negative component obtained by considering feature $F5$ in isolation (given by node $10$), and a component that arises due to the need to take into consideration feature $F1$, in isolation. Thus, the negative contribution of node $1$ represents the negative terms in the summation in equation $(1)$, and the summation is over the elements of $\\{\\{F5\\}\\ , \\varnothing\\}$. A similar reasoning for the right-subtree allows to calculate the positive terms in the summation.\n",
    "\n",
    "\n",
    "<a id='f_1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Explaining loss functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One advantage of the interventional approach is that it allows to approximately transform the shap values to account for nonlinear transformation of outputs, such as the loss function. Recall that given $\\phi_i, ..., \\phi_M$ the local accuracy property gurantees that given $\\phi_0 = \\mathbb{E}[f(x)]$\n",
    "\n",
    "$$\n",
    "f(x) = \\phi_0 + \\sum \\limits_{i=1}^M \\phi_i.\n",
    "\\tag{3}\n",
    "$$\n",
    "\n",
    "Hence, in order to account for the effect of the nonlinear transformation $h$, one has to find the functions $g_0, ..., g_M$ such that\n",
    "\n",
    "$$\n",
    "h(f(x)) = g(\\phi_0) + \\sum \\limits_{i=1}^M g_i(\\phi_i)\n",
    "\\tag{4}\n",
    "$$\n",
    "\n",
    "For simplicity, let $y=h(x)$. Then using a first-order Taylor series expansion around $\\mathbb{E}[y]$ one obtains \n",
    "\n",
    "$$\n",
    "f(y) \\approx f(\\mathbb{E}[y]) + \\frac{\\partial f(y) }{\\partial y} \\Bigr|_{y=\\mathbb{E}[y]}(y - \\mathbb{E}[y]).\n",
    "\\tag{5}\n",
    "$$\n",
    "\n",
    "Substituting $(3)$ in $(5)$ and comparing coefficients with $(4)$ yields\n",
    "\n",
    "$$\n",
    "g_0 \\approx f(\\mathbb{E}[y]) \\\\\n",
    "g_i \\approx \\phi_i \\frac{\\partial f(y) }{\\partial y} \\Bigr|_{y=\\mathbb{E}[y]} .\n",
    "$$\n",
    "\n",
    "Hence, an approximate correction is given by simply scaling the shap values using the gradient of the nonlinear function. Note that in practice one may take the Taylor series expasion at a reference point $r$ from the background dataset and average over the entire background dataset to compute the scaling factor. This introduces an additional source of noise since $f(\\mathbb{E}[y]) = \\mathbb{E}[f(y)]$ only for linear functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computational complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a single foreground and background sample and a single tree, the algorithm runs in $O(LD)$ time. Thus, using $R$ background samples and a model containing $T$ trees, yields a complexity of $O(TRLD)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Path dependent feature perturbation\n",
    "<a id='path_dependent'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to approximate equation $(2)$ to compute $f(S)$ given an instance $x$ and a set of missing features $\\bar{S}$ is to recursively follow the decision path through the tree and:\n",
    "- return the node value if a split on a feature $i \\in S$ is performed\n",
    "- take a weighted average of the values returned by children if $i \\in \\bar{S}$, where the weighing factor is equal to the proportion of training examples flowing down each branch. This proportion is a property of each node, sometimes referred to as _weight_ or _cover_ and measures how important is that node with regard to classifying the training data.\n",
    "\n",
    "Therefore, in the path-dependent perturbation method, we compute the expectations with respect to the training data distribution by weighting the leaf values according to the proportion of the training examples that flow to that leaf.\n",
    "\n",
    "To avoid repeating the above recursion $M2^M$ times, one first notices that for a single decision tree, applying a perturbation would result in the sample ending up in a different leaf. Therefore, following each path from the root to a leaf in the tree is equivalent to perturbing subsets of features of varying cardinalities. Consequently, each leaf will contain a certain proportion of all possible subsets $S \\subseteq F$. Therefore, to compute the shap values, the following quantities are computed at each leaf, *for every feature $i$ on the path leading to that leaf*:\n",
    "- the proportion of subsets $S$ at the leaf that contain $i$ and the proportion of subsets $S$ that do not contain $i$\n",
    "- for each cardinality, the proportion of the sets of that cardinality contained at the leaf. Tracking each cardinality  as opposed to a single count of subsets falling into a given leaf is necessary since it allows to apply the weighting factor in equation (1), which depends on the subset size, $|S|$.\n",
    "\n",
    "This intuition can be summarised as follows:\n",
    "$$\n",
    "\\phi_i := \\sum \\limits_{j=1}^L \\sum \\limits_{P \\in {S_j}} \\frac {w(|P|, j)}{ M_j {M_j - 1 \\choose |P|}} (p_o^{i,j} - p_z^{i, j}) v_j\n",
    "\\tag{6}\n",
    "$$\n",
    "\n",
    "where $S_j$ is the set of present feature subsets at leaf $j$, $M_j$ is the length of the path and $w(|P|, j)$ is the proportion of all subsets of cardinality $P$ at leaf $j$,  $p_o^{i, j}$  and $p_z^{i, j}$ represent the fractions of subsets that contain or do not contain feature $i$ respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computational complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the above quantities, one can compute the _contribution_ of each leaf to the Shapley value of every feature. This algorithm has complexity $O(TLD^2)$ for an ensamble of trees where $L$ is the number of leaves, $T$ the number of trees in the ensamble and $D$ the maximum tree depth. If the tree is balanced, then $D=\\log L$ and the complexity of our algorithm is $O(TL\\log^2L)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expected value for the path-dependent perturbation algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that although a background dataset is not provided, the expected values is computed using the node cover information, stored at each node. The computation proceeds recursively, starting at the root. The contribution of a note to the expected value of the tree is a function of the expected values of the children and is computed as follows:\n",
    "\n",
    "$$\n",
    "c_j = \\frac{c_{r(j)}r_{r(j)} + c_{l(j)}r_{l(j)}}{r_j}\n",
    "$$\n",
    "\n",
    "where $j$ denotes the node index, $c_j$ denotes the node expected value, $r_j$ is the cover of the $j$th node and $r(j)$ and $l(j)$ represent the indices of the right and left children, respectively. The expected value used by the tree is simply $c_{root}$. Note that for tree ensamles, the expected values of the ensamble members is weighted according to the tree weight and the weighted expected values of all trees are summed to obtain a single value.\n",
    "\n",
    "The cover depends on the objective function and the model chosen. For example, in a gradient boosted tree trained with squared loss objective, $r_j$ is simply the number of training examples flowing through $j$. For an arbitrary objective, this is the sum of the hessian of the loss function evaluated at each point flowing through $j$, as explained [here](../../../examples/tree_shap_adult_xgb.ipynb#optimisation). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shapley interaction values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the Shapley values provide a solution to the problem of allocating a function variation  to the input featues, in practice it might be of interest to understand how the importance of a feature depends on the other features. The Shapley interaction values can solve this problem, by allocating the change in the function amongst the individual features (*main effects*) and all pairs of features (*interaction effects*). Thus, they are defined as \n",
    "\n",
    "$$\n",
    "\\Phi_{i, j}(f, x) = \\sum_{S \\subseteq {F \\setminus \\{i, j\\}}} \\frac{1}{2|S| { M-1 \\choose |S| - 1}} \\nabla_{ij}(f, x, S), \\; i \\neq j\n",
    "\\tag{7}\n",
    "$$\n",
    "and\n",
    "\n",
    "$$\\nabla_{ij}(f, x, S) = \\underbrace{f_{x}(S \\cup \\{i, j\\}) - f_x(S \\cup \\{j\\})}_{j \\; present} - \\underbrace{[f_x(S \\cup \\{i\\}) - f_x(S)]}_{j \\; not \\; present}. \\tag{8}$$\n",
    "\n",
    "Therefore, the interaction of features $i$ and $j$ can be computed by taking the difference between the shap values of $i$ when $j$ is present and when $j$ is not present. The main effects are defined as \n",
    "\n",
    "$$\n",
    "\\Phi_{i,i}(f, x) = \\phi_i(f, x) - \\sum_{i \\neq j} \\Phi_{i, j}(f, x),\n",
    "$$\n",
    "\n",
    "Setting $\\Phi_{0, 0} = f_x(\\varnothing)$ yields the local accuracy property for Shapley interaction values:\n",
    "\n",
    "$$f(x) = \\sum \\limits_{i=0}^M \\sum \\limits_{j=0}^M \\Phi_{i, j}(f, x) $$.\n",
    "\n",
    "The interaction is split equally between feature $i$ and $j$, which is why the division by two appears in equation $(7)$. The total interaction effect is defined as $\\Phi_{i, j}(f, x) + \\Phi_{j, i}(f,x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computational complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to equation $(8)$, the interaction values can be computed by applying either the interventional or path-dependent feature perturbation algorithm twice: once by fixing the value of feature $j$ to $x_j$ and computing the shapley value for feature $i$ in this configuration, and once by fixing $x_j$ to a \"missing\" value and performing the same computation. Thus, the interaction values can be computed in $O(TMLD^2)$ with the path-dependent perturbation algorithm and O(TMLDR) with the interventional feature perturbation algorithm.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison to other methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tree-based models are widely used in areas where model interpretability is of interest because node-level statistics gathered from the training data can be used to provide insights into the behaviour of the model across the training dataset, providing a _global explanation_ technique. As shown in our [example](/../../examples/tree_shap_adult_xgb.ipynb#xgboost_importance), considering different statistics gives rise to different importance rankings. As discussed in [[1]](#References) and [[3]](#References), depending on the statistic chosen, feature importances derived from trees are not *consistent*, meaning that a model where a feature is known to have a bigger impact might fail to have a larger importance. As such, feature importances cannot be compared across models.  In contrast, both the path-dependent and interventional perturbation algorithms tackle this limitation.\n",
    "\n",
    "In contrast to feature importances derived from tree statistics, the Tree SHAP algorithms can also provide local explanations, allowing the identification of features that are globally \"not important\", but can affect specific outcomes significantly, as might be the case in healthcare applications. Aditionally, it provides a means to succintly summarise the effect magnitude and direction (positive or negative) across potentially large samples. Finally, as shown in [[1]](#References) (see [here](https://static-content.springer.com/esm/art%3A10.1038%2Fs42256-019-0138-9/MediaObjects/42256_2019_138_MOESM1_ESM.pdf), p. 26),  averaging the instance-level shap values importance to derive a global score for each feature can result in improvements in feature selection tasks. \n",
    "\n",
    "Another method to derive instance-level explanations for tree-based model has been proposed by Sabaas [here](https://github.com/andosa/treeinterpreter). This feature attribution method is similar in spirit to Shapley value, but does not account for the effect of variable order as explained [here](https://static-content.springer.com/esm/art%3A10.1038%2Fs42256-019-0138-9/MediaObjects/42256_2019_138_MOESM1_ESM.pdf) (pp. 10-11) as well as not satisfying consistency ([[3]](#References)).\n",
    "\n",
    "Finally, both Tree SHAP algorithms exploit model structure to provide exact Shapley values computation albeit using different estimates for the effect of missing features, achieving explanations in low-order polynomial time. The KernelShap method relies on post-hoc (black-box) function modelling and approximations to approximate the same quantities and given enough samples has been shown to to the exact values (see experiments [here](https://static-content.springer.com/esm/art%3A10.1038%2Fs42256-019-0138-9/MediaObjects/42256_2019_138_MOESM1_ESM.pdf) and our [example](/../../examples/tree_shap_adult_xgb.ipynb#xgboost_importance)). Our Kernel SHAP [documentation](KernelSHAP.ipynb) provides comparisons of feature attribution methods based on Shapley values with other algorithms such as LIME and [anchors](Anchors.ipynb). \n",
    "\n",
    "<a id='source_3'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Footnotes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Footnotes'></a>\n",
    "\n",
    "[(1)](#f_1): Note that, $N$ in the diagram denotes $M$ in equation $(1)$. The value of $1$ occurs because there are only $N = 2$ features where the reference and instance to be explained differ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='References'></a>\n",
    "\n",
    "[[1]](#source_1) Lundberg, S.M. and Lee, S.I., 2017. A unified approach to interpreting model predictions. In Advances in neural information processing systems (pp. 4765-4774).\n",
    "\n",
    "[[2]](#source_2) Janzing, D., Minorics, L. and Bl√∂baum, P., 2019. Feature relevance quantification in explainable AI: A causality problem. arXiv preprint arXiv:1910.13413.\n",
    "\n",
    "[[3]](#source_3) Lundberg, S.M., Erion, G.G. and Lee, S.I., 2018. Consistent individualized feature attribution for tree ensembles. arXiv preprint arXiv:1802.03888."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Explaining tree-based models for income prediction](../examples/tree_shap_adult_xgb.nblink)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
