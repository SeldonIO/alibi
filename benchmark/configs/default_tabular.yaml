dataset: adult

# Example of configuring a random forest classifier
classifier:
  name: rf
  n_estimators: 50
  seed: 0

# Example of configuring a AnchorTabular explainer
explainer:
  type: tabular
  threshold: 0.95
  disc_perc: !!python/tuple [25, 50, 75]
  parallel: false
  verbose: false
  seed: 0

# Experiment settings
experiment:
  test_only: false # disables saving output (avoid overwriting data when testing stuff)
  n_runs: 100
  instance_split: test # can be train or ~ (aka None - taken from data before splitting)
  instance_idx: 6
  ckpt_dir: /home/alex/git/work_experiments/BenchmarkTest
  ckpt: BenchmarkTestHashPassingExample2.pkl
  verbose: true
  show_covered: false  # true will raise NotImplemented Error
  data:
    preprocess: true
    preprocess_opts: ~
    split_opts:
      method: shuffle # see default_text.yaml for alternative
      seed: 0
      n_train_records: 30000
  save:
    # elapsed time included by default
    fields: [feat_ids, feat_names, precision, coverage, covered_true, covered_false]
    # By default, it is assumed that the experiment object has
    # amongst its keys the values specified in fields. If this is
    # not the case, then mapping[fields[idx]] contains a list with
    # the keys that should be accessed in the experiment object to
    # retrieve the data. In this example, the output dictionary will be:
    # {'feat_ids': [[explanation['raw']['features'], ...],
    # 'feat_names':[[explanation['raw']['names'], ....]}
    # The lengths of the the keys are = n_runs
    mapping:
      feat_ids : [raw, feature]
      feat_names: [raw, names]
      covered_true: [raw, examples, covered_true]
      covered_false: [raw, examples, covered_false]
  profile: true
  profile_dir: /home/alex/git/work_experiments/ProfileData
  profile_out: benchmark_stats.cprof
