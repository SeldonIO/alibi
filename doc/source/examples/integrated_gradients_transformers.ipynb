{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integrated gradients for transformers models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we apply the integrated gradients method to two different sentiment analysis models. The first one is a pretrained sentiment analysis model from the  [transformers](https://github.com/huggingface/transformers) library. The second model is a combination of a pretrained (distil)BERT model and a simple feed forward network. The entire model, **(distil)BERT** and feed forward network, is trained on the **IMDB reviews** dataset. \n",
    "\n",
    "In text classification models, **integrated gradients (IG)** define an attribution value for each word in the input sentence. The attributions are calculated considering the integral of the model  gradients with respect to the word embedding layer along a straight path from a baseline instance $x^\\prime$ to the input instance $x.$ A description of the method can be found [here](https://docs.seldon.io/projects/alibi/en/latest/methods/IntegratedGradients.html). Integrated gradients was originally proposed in Sundararajan et al., [\"Axiomatic Attribution for Deep Networks\"](https://arxiv.org/abs/1703.01365)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "from tqdm import tqdm\n",
    "from typing import Optional, Union, List, Dict\n",
    "from IPython.display import HTML\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from transformers import PreTrainedTokenizer\n",
    "from alibi.explainers import IntegratedGradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define some functions needed to process the data and visualize. For consistency with other [text examples](https://github.com/SeldonIO/alibi/blob/master/examples/integrated_gradients_imdb.ipynb) in alibi, we will use the **IMDB reviews** dataset provided by Keras. Since the dataset consists of reviews that are already tokenized, we need to decode each sentence and re-convert them into tokens using the **(distil)BERT** tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sentence(x: List[int], reverse_index: Dict[int, str], unk_token: str = '[UNK]') -> str:\n",
    "    \"\"\" \n",
    "    Decodes the tokenized sentences from keras IMDB dataset into plain text.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x\n",
    "        List of integers to be docoded.\n",
    "    revese_index:\n",
    "        Reverse index map, from `int` to `str`.\n",
    "    unk_token:\n",
    "        Unkown token to be used.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "        Decoded sentence.\n",
    "    \"\"\"\n",
    "    # the `-3` offset is due to the special tokens used by keras\n",
    "    # see https://stackoverflow.com/questions/42821330/restore-original-text-from-keras-s-imdb-dataset\n",
    "    return \" \".join([reverse_index.get(i - 3, unk_token) for i in x])\n",
    "\n",
    "\n",
    "def process_sentences(sentence: List[str], \n",
    "                      tokenizer: PreTrainedTokenizer, \n",
    "                      max_len: int) -> Dict[str, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Tokenize the text sentences.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    sentence:\n",
    "        Sentence to be processed.\n",
    "    tokenizer:\n",
    "        Tokenizer to be used.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "        Tokenized representation containing:\n",
    "         - input_ids\n",
    "         - attention_mask\n",
    "    \"\"\"\n",
    "    # since we are using the model for classification, we need to include special char (i.e, '[CLS]', ''[SEP]')\n",
    "    # check the example here: https://huggingface.co/transformers/v4.4.2/quicktour.html\n",
    "    z = tokenizer(sentence, \n",
    "                  add_special_tokens=True, \n",
    "                  padding='max_length', \n",
    "                  max_length=max_len, \n",
    "                  truncation=True,\n",
    "                  return_attention_mask = True,  \n",
    "                  return_tensors='np')\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def  hlstr(string: str , color: str = 'white') -> str:\n",
    "    \"\"\"\n",
    "    Return HTML markup highlighting text with the desired color.\n",
    "    \"\"\"\n",
    "    return f\"<mark style=background-color:{color}>{string} </mark>\"\n",
    "\n",
    "\n",
    "def colorize(attrs: np.ndarray, cmap: str = 'PiYG') -> List:\n",
    "    \"\"\"\n",
    "    Compute hex colors based on the attributions for a single instance.\n",
    "    Uses a diverging colorscale by default and normalizes and scales\n",
    "    the colormap so that colors are consistent with the attributions.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    attrs:\n",
    "        Attributions to be visualized.\n",
    "    cmap:\n",
    "        Matplotlib cmap type.\n",
    "    \"\"\"\n",
    "    cmap_bound = np.abs(attrs).max()\n",
    "    norm = mpl.colors.Normalize(vmin=-cmap_bound, vmax=cmap_bound)\n",
    "    cmap = mpl.cm.get_cmap(cmap)\n",
    "    return list(map(lambda x: mpl.colors.rgb2hex(cmap(norm(x))), attrs))\n",
    "\n",
    "\n",
    "def display(X: np.ndarray, \n",
    "            attrs: np.ndarray, \n",
    "            tokenizer: PreTrainedTokenizer,\n",
    "            pred: np.ndarray) -> None:\n",
    "    \"\"\"\n",
    "    Display the attribution of a given instance.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X:\n",
    "        Instance to display the attributions for.\n",
    "    attrs:\n",
    "        Attributions values for the given instance.\n",
    "    tokenizer:\n",
    "        Tokenizer to be used for decoding.\n",
    "    pred:\n",
    "        Classification label (prediction) for the given instance.\n",
    "    \"\"\"\n",
    "    pred_dict = {1: 'Positive review', 0: 'Negative review'}\n",
    "    \n",
    "    # remove padding\n",
    "    fst_pad_indices = np.where(X ==tokenizer.pad_token_id)[0]\n",
    "    if len(fst_pad_indices) > 0:\n",
    "        X, attrs = X[:fst_pad_indices[0]], attrs[:fst_pad_indices[0]]\n",
    "    \n",
    "    # decode tokens and get colors\n",
    "    tokens = [tokenizer.decode([X[i]]) for i in range(len(X))]\n",
    "    colors = colorize(attrs)\n",
    "    \n",
    "    print('Predicted label =  {}: {}'.format(pred, pred_dict[pred]))\n",
    "    return HTML(\"\".join(list(map(hlstr, tokens, colors))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automodel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will use the Tensorflow auto model for sequence classification provided by the [transformers](https://github.com/huggingface/transformers) library. \n",
    "\n",
    "The model is pretrained on the [Stanford Sentiment Treebank (SST)](https://huggingface.co/datasets/sst) dataset. The **Stanford Sentiment Treebank** is the first corpus with fully labeled parse trees that allows for a complete analysis of the compositional effects of sentiment in language.\n",
    "\n",
    "Each phrase is labeled as either negative, somewhat negative, neutral, somewhat positive or positive. The corpus with all 5 labels is referred to as **SST-5** or **SST fine-grained**. Binary classification experiments on full sentences (negative or somewhat negative vs somewhat positive or positive with neutral sentences discarded) refer to the dataset as **SST-2** or **SST binary**.  In this example, we will use a text classifier pretrained on the **SST-2** dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the layers of TFDistilBertForSequenceClassification were initialized from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFAutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "# load model and tokenizer\n",
    "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "auto_model_distilbert = TFAutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `auto_model` output is a custom object containing the output logits. We use a wrapper to transform the output into a tensor and apply a softmax function to the logits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoModelWrapper(keras.Model):\n",
    "    def __init__(self, transformer: keras.Model, **kwargs):\n",
    "        \"\"\"\n",
    "        Constructor.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        transformer:\n",
    "            Transformer to be wrapped.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.transformer = transformer\n",
    "\n",
    "    def call(self, \n",
    "             input_ids: Union[np.ndarray, tf.Tensor], \n",
    "             attention_mask: Optional[Union[np.ndarray, tf.Tensor]] = None,\n",
    "             training: bool = False):\n",
    "        \"\"\"\n",
    "        Performs forward pass throguh the model.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        input_ids:\n",
    "            Indices of input sequence tokens in the vocabulary.\n",
    "        attention_mask:\n",
    "            Mask to avoid performing attention on padding token indices.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "            Classification probabilities.\n",
    "        \"\"\"\n",
    "        out = self.transformer(input_ids=input_ids, attention_mask=attention_mask, training=training)\n",
    "        return tf.nn.softmax(out.logits, axis=-1)\n",
    "    \n",
    "    def get_config(self):\n",
    "        return {}\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_model = AutoModelWrapper(auto_model_distilbert)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate integrated gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 10000\n",
    "max_len = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we consider some simple sentences such as \"I love you, I like you\", \"I love you, I like you, but I also kind of dislike you\" ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_samples = ['I love you, I like you', \n",
    "                'I love you, I like you, but I also kind of dislike you',\n",
    "                'Everything is so nice about you']\n",
    "\n",
    "# since using the uncased model, we need to use lowercase sentences\n",
    "text_samples = [text.lower() for text in text_samples]\n",
    "\n",
    "# tokenize the sentences using the transformer's tokenizer.\n",
    "tokenized_samples = process_sentences(text_samples, tokenizer, max_len)\n",
    "X_test = tokenized_samples['input_ids'].astype(np.int32)\n",
    "\n",
    "# the values of the kwargs have to be `tf.Tensor`. \n",
    "# see transformers issue #14404: https://github.com/huggingface/transformers/issues/14404\n",
    "kwargs = {k: tf.constant(v) for k,v in tokenized_samples.items() if k == 'attention_mask'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The auto model consists of a main **distilBERT** layer (layer 0) followed by two dense layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<transformers.models.distilbert.modeling_tf_distilbert.TFDistilBertMainLayer at 0x7f30403fa5b0>,\n",
       " <keras.layers.core.dense.Dense at 0x7f2cdc56a0d0>,\n",
       " <keras.layers.core.dense.Dense at 0x7f2cdc56a340>,\n",
       " <keras.layers.core.dropout.Dropout at 0x7f2cdc56a670>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auto_model.layers[0].layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will proceed with the embedding layer from **distilBERT**. We calculate attributions to the outputs of the **embedding layer** since we can easily construct an appropriate baseline for the **IG** which will result in more interpretable attribution. If we considered a hidden layer instead, we would inevitably capture higher order interaction between the input tokens, which might be harder to interpret as we no longer have a one-to-one mapping between layer outputs and input tokens. Moreover, the embedding layer is a standard choice since we can not compute attributions for the raw input due to its discrete structure (i.e., we cannot differentiate the output of the model with respect to the discrete input representation). **That being said, you can use any other layer and compute attributions to the outputs of it instead.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Extracting the embeddings layer\n",
    "layer = auto_model.layers[0].layers[0].embeddings\n",
    "\n",
    "# # Extract the first layer from the transformer\n",
    "# layer = auto_model.layers[0].layers[0].transformer.layer[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define IG\n",
    "n_steps = 50\n",
    "internal_batch_size = 5\n",
    "method = \"gausslegendre\"\n",
    "\n",
    "ig  = IntegratedGradients(auto_model,\n",
    "                          layer=layer,\n",
    "                          n_steps=n_steps, \n",
    "                          method=method,\n",
    "                          internal_batch_size=internal_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7f3040410160>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7f3040410160>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    }
   ],
   "source": [
    "# get predictions\n",
    "predictions = auto_model(X_test, **kwargs).numpy().argmax(axis=1)\n",
    "\n",
    "# Get the baselines. Note that the baseline contains special characters (e.g, [CLS], [SEP], [UNK] [PAD]) and\n",
    "# the regular tokens are replaced by the [PAD] token which is a neutral token.\n",
    "# By including special tokens such as [CLS], [SEP], [UNK], we ensure that the attribution for those tokens\n",
    "# will be 0 if we use the embedding layer. The 0 attribution is due to integration between [x, x] which is 0.\n",
    "mask = np.isin(X_test, tokenizer.all_special_ids)\n",
    "baselines = X_test * mask + tokenizer.pad_token_id * (1 - mask)\n",
    "\n",
    "# get explanation\n",
    "explanation = ig.explain(X_test, \n",
    "                         forward_kwargs=kwargs,\n",
    "                         baselines=baselines, \n",
    "                         target=predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the attributions' shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attributions shape: (3, 128, 768)\n"
     ]
    }
   ],
   "source": [
    "# Get attributions values from the explanation object\n",
    "attrs = explanation.attributions[0]\n",
    "print('Attributions shape:', attrs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the attribution of each token corresponds to a tensor of `768` elements. We compress all this information into a single number buy summing up all `768` components. The nice thing about this is that we still remain consistent with the **Completeness Axiom**, which states that the attributions add up to the difference between the output of our model for the given instance and the output of our model for the given baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attributions shape: (3, 128)\n"
     ]
    }
   ],
   "source": [
    "attrs = attrs.sum(axis=2)\n",
    "print('Attributions shape:', attrs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted label =  0: Negative review\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<mark style=background-color:#f7f7f6>[CLS] </mark><mark style=background-color:#fcdbed>i </mark><mark style=background-color:#e181b5>love </mark><mark style=background-color:#f9d1e8>you </mark><mark style=background-color:#f9eff4>, </mark><mark style=background-color:#f9eef4>i </mark><mark style=background-color:#faeaf2>like </mark><mark style=background-color:#fce5f1>you </mark><mark style=background-color:#f3f6ed>, </mark><mark style=background-color:#c9e8a2>but </mark><mark style=background-color:#f9f1f5>i </mark><mark style=background-color:#f8f2f5>also </mark><mark style=background-color:#edf6e1>kind </mark><mark style=background-color:#e8f5d5>of </mark><mark style=background-color:#276419>dislike </mark><mark style=background-color:#fde0ef>you </mark><mark style=background-color:#f7f7f6>[SEP] </mark>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = 1\n",
    "display(X=X_test[index], attrs=attrs[index], pred=predictions[index], tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note that since the sentence is classified as negative, words like `dislike` contribute positively to the score while words like `love` contribute negatively.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment analysis on IMDB with fine-tuned model head."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Load and process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load imdb reviews datasets.\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "\n",
    "# extract sub-set of training and testing\n",
    "train_size, test_size = 25000, 1000\n",
    "x_train, y_train = x_train[:train_size], y_train[:train_size]\n",
    "x_test, y_test = x_test[:test_size], y_test[:test_size]\n",
    "\n",
    "# remove the first integer token which is a special character\n",
    "# that marks the beginning of the sentence\n",
    "x_train = [x[1:] for x in x_train]\n",
    "x_test = [x[1:] for x in x_test]\n",
    "\n",
    "# get mappings. The keys are transformed to lower case since we will use uncased models.\n",
    "reverse_index = {value: key.lower() for (key, value) in imdb.get_word_index().items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model and corresponding tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have to load the model and the corresponding tokenizer. You can chose between the **BERT** model or the **distilBERT** model. Note that we will be finetuning those models which will require access to a **GPU**. In our experiments, we trained distilBERT on a single **Quadro RTX 5000** which requires around **5GB** of memory. The entire training took around **5-6 min**. We recommend using **distilBERT** as we did not noticed a big difference in performance between the two models after finetuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose whether to use the BERT model by setting the following flag to `True`\n",
    "# Otherwise the distilBERT will be used\n",
    "use_bert = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_bert:\n",
    "    from transformers import BertTokenizerFast\n",
    "    tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "else:\n",
    "    from transformers import DistilBertTokenizerFast\n",
    "    tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertModel: ['vocab_transform', 'vocab_layer_norm', 'vocab_projector', 'activation_13']\n",
      "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "if use_bert:\n",
    "    from transformers import TFBertModel, BertConfig\n",
    "    config = BertConfig(output_hidden_states=True)\n",
    "    transformer = TFBertModel.from_pretrained('bert-base-uncased', config=config)\n",
    "else:\n",
    "    from transformers import TFDistilBertModel, DistilBertConfig\n",
    "    config = DistilBertConfig(output_hidden_states=True)\n",
    "    transformer = TFDistilBertModel.from_pretrained('distilbert-base-uncased', config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decoding each sentence in the **Keras IMDB** tokenized dataset to obtain the corresponding plain text. The dataset is already in a pretty good shape, so we don't need to do extra preprocessing. The only thing that we do is to replace the unknown tokens with the appropriate tokenizer's unknown token (i.e., `[UNK]`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = [], []\n",
    "\n",
    "# decode training sentences\n",
    "for i in range(len(x_train)):\n",
    "    tr_sentence = decode_sentence(x_train[i], reverse_index, unk_token=tokenizer.unk_token)\n",
    "    X_train.append(tr_sentence)\n",
    "\n",
    "# decode testing sentences\n",
    "for i in range(len(x_test)):\n",
    "    te_sentence = decode_sentence(x_test[i], reverse_index, unk_token=tokenizer.unk_token)\n",
    "    X_test.append(te_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retokenizing the plain text using the **(distil)BERT** tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize datasets\n",
    "X_train = process_sentences(X_train, tokenizer, max_len)\n",
    "X_test = process_sentences(X_test, tokenizer, max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we train a classification model by leveraging the pretrained **(distil)BERT** transformer. Since we are dealing with classification, we only require the output embedding corresponding to the `[CLS]` token (remember that we introduced some special tokens such as: `[CLS]`, `[SEP]`). The output embedding for the `[CLS]` token is a `768` dimensional vector which encoded the entire sentence. The model head consists of one dense layer of `128` hidden units followed by a `2` unit layer with softmax activation, and a dropout layer in-between with a rate of `0.2`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMDBClassifier(tf.keras.Model):\n",
    "    def __init__(self, \n",
    "                 transformer, \n",
    "                 hidden_dims: int = 128, \n",
    "                 output_dims: int = 2,\n",
    "                 dropout_rate: float = 0.2):\n",
    "        \"\"\"\n",
    "        Constructor\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        transformer:\n",
    "            Transformer model to be leveraged.\n",
    "        hidden_dims:\n",
    "            hidden layer's dimension.\n",
    "        output_dims:\n",
    "            Output layer's dimension.\n",
    "        dropout_rate:\n",
    "            Dropout layer's dropout rate.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.output_dims = output_dims\n",
    "        self.dropout_rate = dropout_rate\n",
    "        \n",
    "        self.transformer = transformer\n",
    "        self.dense_1 = tf.keras.layers.Dense(self.hidden_dims, activation='relu')\n",
    "        self.dropout_1 = tf.keras.layers.Dropout(self.dropout_rate)\n",
    "        self.dense_2 = tf.keras.layers.Dense(self.output_dims, activation='softmax')\n",
    "        \n",
    "    def call(self, \n",
    "             input_ids: Union[np.ndarray, tf.Tensor], \n",
    "             attention_mask: Optional[Union[np.ndarray, tf.Tensor]]=None, \n",
    "             training=False):\n",
    "        \"\"\"\n",
    "        Performs forward pass throguh the model.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        input_ids:\n",
    "            Indices of input sequence tokens in the vocabulary.\n",
    "        attention_mask:\n",
    "            Mask to avoid performing attention on padding token indices.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "            Classification probabilities.\n",
    "        \"\"\"\n",
    "        out = self.transformer(input_ids=input_ids, attention_mask=attention_mask, training=training)\n",
    "        out = out.last_hidden_state[:, 0, :]  # extract the embedding corresponding to [CLS] token \n",
    "        out = self.dense_1(out)\n",
    "        out = self.dropout_1(out, training=training)\n",
    "        out = self.dense_2(out)\n",
    "        return out  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the classification model\n",
    "model = IMDBClassifier(transformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train the entire classification model. That includes the transformer too, which can be hardware demanding.\n",
    "Training just the top layers is possible too, but in our experiments it resulted in a considerably lower accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.3514 - accuracy: 0.8422WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "\n",
      "Epoch 00001: saving model to ./model_transformers/training/cp-0001.ckpt\n",
      "782/782 [==============================] - 167s 209ms/step - loss: 0.3514 - accuracy: 0.8422 - val_loss: 0.2839 - val_accuracy: 0.8670\n",
      "Epoch 2/2\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.2279 - accuracy: 0.9074\n",
      "Epoch 00002: saving model to ./model_transformers/training/cp-0002.ckpt\n",
      "782/782 [==============================] - 163s 209ms/step - loss: 0.2279 - accuracy: 0.9074 - val_loss: 0.3096 - val_accuracy: 0.8720\n"
     ]
    }
   ],
   "source": [
    "filepath = './model_transformers/'  # change to desired save directory\n",
    "load_model = False\n",
    "\n",
    "# optimization params\n",
    "learning_rate = 2e-5\n",
    "batch_size = 32\n",
    "epochs = 2\n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=learning_rate),\n",
    "              loss=SparseCategoricalCrossentropy(from_logits=False),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "if not load_model:\n",
    "    checkpoint_path = os.path.join(filepath, \"training/cp-{epoch:04d}.ckpt\")\n",
    "    checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "    \n",
    "    # Create a callback that saves the model's weights every epoch\n",
    "    cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path, \n",
    "                                                     verbose=1, \n",
    "                                                     save_weights_only=True,\n",
    "                                                     save_freq='epoch')\n",
    "\n",
    "    # fit the model\n",
    "    model.fit(x=[X_train['input_ids'], X_train['attention_mask']], \n",
    "              y=y_train, \n",
    "              validation_data=([X_test['input_ids'], X_test['attention_mask']], y_test),\n",
    "              epochs=epochs,\n",
    "              batch_size=batch_size,\n",
    "              callbacks=[cp_callback])\n",
    "else:\n",
    "    epoch = 2\n",
    "    load_path = os.path.join(filepath, f\"training/cp-{epoch:04d}.ckpt\")\n",
    "    model.load_weights(load_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate integrated gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We pick the first 10 sentences from the test set as examples. You can easily add some of your text here too, as we exemplify it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# include IMDB reviews from the test dataset\n",
    "text_samples = [decode_sentence(x_test[i], reverse_index, unk_token=tokenizer.unk_token) for i in range(10)]\n",
    "\n",
    "# inlcude your text here\n",
    "text_samples.append(\"best movie i've ever seen nothing bad to say about it\")\n",
    "\n",
    "# tokenize text\n",
    "tokenized_samples = process_sentences(text_samples, tokenizer, max_len)\n",
    "X_test = tokenized_samples['input_ids']\n",
    "\n",
    "# the values of the kwargs have to be `tf.Tensor`. \n",
    "# see transformers issue #14404: https://github.com/huggingface/transformers/issues/14404\n",
    "kwargs = {k:tf.constant(v) for k, v in tokenized_samples.items() if k == 'attention_mask'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We calculate the attributions with respect to the first embedding layer of the **(distil)BERT**. You can choose any other layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_bert:\n",
    "    layer = model.layers[0].bert.embeddings\n",
    "    # layer = model.layers[0].bert.encoder.layer[0]\n",
    "else:\n",
    "    layer = model.layers[0].distilbert.embeddings\n",
    "    # layer = model.layers[0].distilbert.transformer.layer[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define IG\n",
    "n_steps = 50\n",
    "method = \"gausslegendre\"\n",
    "internal_batch_size = 5\n",
    "\n",
    "ig  = IntegratedGradients(model,\n",
    "                          layer=layer,\n",
    "                          n_steps=n_steps, \n",
    "                          method=method,\n",
    "                          internal_batch_size=internal_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    }
   ],
   "source": [
    "# compute model's prediction and construct baselines\n",
    "predictions = model(X_test, **kwargs).numpy().argmax(axis=1)\n",
    "\n",
    "# construct the baseline as before\n",
    "mask = np.isin(X_test, tokenizer.all_special_ids)\n",
    "baselines = X_test * mask + tokenizer.pad_token_id * (1 - mask)\n",
    "\n",
    "# get explanation\n",
    "explanation = ig.explain(X_test, \n",
    "                         forward_kwargs=kwargs,\n",
    "                         baselines=baselines, \n",
    "                         target=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attributions shape: (11, 128, 768)\n"
     ]
    }
   ],
   "source": [
    "# Get attributions values from the explanation object\n",
    "attrs = explanation.attributions[0]\n",
    "print('Attributions shape:', attrs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attributions shape: (11, 128)\n"
     ]
    }
   ],
   "source": [
    "attrs = attrs.sum(axis=2)\n",
    "print('Attributions shape:', attrs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check attributions for our example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted label =  1: Positive review\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<mark style=background-color:#f7f7f6>[CLS] </mark><mark style=background-color:#276419>best </mark><mark style=background-color:#f0f6e7>movie </mark><mark style=background-color:#f3f6ed>i </mark><mark style=background-color:#ebf6dc>' </mark><mark style=background-color:#f3f7ef>ve </mark><mark style=background-color:#d9f0bc>ever </mark><mark style=background-color:#ecf6de>seen </mark><mark style=background-color:#f1b7da>nothing </mark><mark style=background-color:#f1b7da>bad </mark><mark style=background-color:#f8f4f6>to </mark><mark style=background-color:#f7f7f7>say </mark><mark style=background-color:#f3f7ef>about </mark><mark style=background-color:#edf6e1>it </mark><mark style=background-color:#f7f7f6>[SEP] </mark>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = -1\n",
    "display(X=X_test[index], attrs=attrs[index], pred=predictions[index], tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check attribution for some test examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted label =  0: Negative review\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<mark style=background-color:#f7f7f6>[CLS] </mark><mark style=background-color:#edf6df>please </mark><mark style=background-color:#f1f6e8>give </mark><mark style=background-color:#e9f5d8>this </mark><mark style=background-color:#f0f6e7>one </mark><mark style=background-color:#f5f7f3>a </mark><mark style=background-color:#e6f5d0>miss </mark><mark style=background-color:#f3f6ed>br </mark><mark style=background-color:#f5f7f3>br </mark><mark style=background-color:#f7f7f6>[UNK] </mark><mark style=background-color:#f7f7f6>[UNK] </mark><mark style=background-color:#f1f6e8>and </mark><mark style=background-color:#f4f7f0>the </mark><mark style=background-color:#eff6e5>rest </mark><mark style=background-color:#f1f6e8>of </mark><mark style=background-color:#f9eff4>the </mark><mark style=background-color:#d2ecb0>cast </mark><mark style=background-color:#cfebaa>rendered </mark><mark style=background-color:#276419>terrible </mark><mark style=background-color:#e7f5d3>performances </mark><mark style=background-color:#edf6e1>the </mark><mark style=background-color:#c4e699>show </mark><mark style=background-color:#ecf6de>is </mark><mark style=background-color:#f5f7f3>flat </mark><mark style=background-color:#f8f4f6>flat </mark><mark style=background-color:#f7f6f7>flat </mark><mark style=background-color:#eff6e4>br </mark><mark style=background-color:#eff6e5>br </mark><mark style=background-color:#f3f6ed>i </mark><mark style=background-color:#eff6e5>don </mark><mark style=background-color:#f5f7f2>' </mark><mark style=background-color:#f7f7f7>t </mark><mark style=background-color:#f7f6f7>know </mark><mark style=background-color:#f8f4f6>how </mark><mark style=background-color:#f7f7f6>michael </mark><mark style=background-color:#fbe9f2>madison </mark><mark style=background-color:#eaf5d9>could </mark><mark style=background-color:#d0ecad>have </mark><mark style=background-color:#eff6e5>allowed </mark><mark style=background-color:#f1f6ea>this </mark><mark style=background-color:#f1f6ea>one </mark><mark style=background-color:#f7f7f6>on </mark><mark style=background-color:#f8f2f5>his </mark><mark style=background-color:#f7f6f7>plate </mark><mark style=background-color:#f7f7f6>he </mark><mark style=background-color:#f8f3f6>almost </mark><mark style=background-color:#f3f7ef>seemed </mark><mark style=background-color:#f6f7f5>to </mark><mark style=background-color:#f4f7f0>know </mark><mark style=background-color:#f3f6ed>this </mark><mark style=background-color:#eaf5d9>wasn </mark><mark style=background-color:#f3f6ed>' </mark><mark style=background-color:#f3f7ef>t </mark><mark style=background-color:#f3f6ed>going </mark><mark style=background-color:#f2f6ec>to </mark><mark style=background-color:#f8f4f6>work </mark><mark style=background-color:#eff6e5>out </mark><mark style=background-color:#f7f7f7>and </mark><mark style=background-color:#f9f0f5>his </mark><mark style=background-color:#eef6e2>performance </mark><mark style=background-color:#f0f6e7>was </mark><mark style=background-color:#faecf3>quite </mark><mark style=background-color:#f7f7f6>[UNK] </mark><mark style=background-color:#edf6e1>so </mark><mark style=background-color:#f7f6f7>all </mark><mark style=background-color:#f7f6f7>you </mark><mark style=background-color:#f9f1f5>madison </mark><mark style=background-color:#faeaf2>fans </mark><mark style=background-color:#f5f7f3>give </mark><mark style=background-color:#eff6e4>this </mark><mark style=background-color:#f0f6e7>a </mark><mark style=background-color:#e4f4cd>miss </mark><mark style=background-color:#f7f7f6>[SEP] </mark>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = 0\n",
    "display(X=X_test[index], attrs=attrs[index], pred=predictions[index], tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted label =  1: Positive review\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<mark style=background-color:#f7f7f6>[CLS] </mark><mark style=background-color:#e9f5d6>this </mark><mark style=background-color:#f4f7f0>film </mark><mark style=background-color:#f7f6f7>requires </mark><mark style=background-color:#e6f5d0>a </mark><mark style=background-color:#edf6e1>lot </mark><mark style=background-color:#ebf6dc>of </mark><mark style=background-color:#ecf6de>patience </mark><mark style=background-color:#e7f5d3>because </mark><mark style=background-color:#dff2c4>it </mark><mark style=background-color:#e9f5d6>focuses </mark><mark style=background-color:#e2f3ca>on </mark><mark style=background-color:#dbf0bf>mood </mark><mark style=background-color:#d2ecb0>and </mark><mark style=background-color:#e6f5d0>character </mark><mark style=background-color:#eff6e5>development </mark><mark style=background-color:#e8f5d5>the </mark><mark style=background-color:#edf6df>plot </mark><mark style=background-color:#b9e187>is </mark><mark style=background-color:#67a832>very </mark><mark style=background-color:#276419>simple </mark><mark style=background-color:#a9d874>and </mark><mark style=background-color:#d4edb3>many </mark><mark style=background-color:#a9d874>of </mark><mark style=background-color:#c2e596>the </mark><mark style=background-color:#c4e699>scenes </mark><mark style=background-color:#eef6e2>take </mark><mark style=background-color:#f4f7f0>place </mark><mark style=background-color:#c4e699>on </mark><mark style=background-color:#e6f5d0>the </mark><mark style=background-color:#f3f6ed>same </mark><mark style=background-color:#e9f5d6>set </mark><mark style=background-color:#e9f5d6>in </mark><mark style=background-color:#ebf6db>frances </mark><mark style=background-color:#f7f7f6>[UNK] </mark><mark style=background-color:#cfebaa>the </mark><mark style=background-color:#e4f4cd>sandy </mark><mark style=background-color:#f2f6ec>dennis </mark><mark style=background-color:#edf6df>character </mark><mark style=background-color:#eff6e5>apartment </mark><mark style=background-color:#e7f5d3>but </mark><mark style=background-color:#e8f5d5>the </mark><mark style=background-color:#e8f5d5>film </mark><mark style=background-color:#eaf5d9>builds </mark><mark style=background-color:#eff6e4>to </mark><mark style=background-color:#9ccf64>a </mark><mark style=background-color:#d0ecad>disturbing </mark><mark style=background-color:#e9f5d6>climax </mark><mark style=background-color:#e9f5d6>br </mark><mark style=background-color:#e4f4cd>br </mark><mark style=background-color:#b0dc7d>the </mark><mark style=background-color:#cfebaa>characters </mark><mark style=background-color:#eff6e4>create </mark><mark style=background-color:#95cb5c>an </mark><mark style=background-color:#a9d874>atmosphere </mark><mark style=background-color:#f7f7f6>[UNK] </mark><mark style=background-color:#9acd61>with </mark><mark style=background-color:#f9eef4>sexual </mark><mark style=background-color:#79b73d>tension </mark><mark style=background-color:#c0e593>and </mark><mark style=background-color:#c6e79c>psychological </mark><mark style=background-color:#f7f7f6>[UNK] </mark><mark style=background-color:#e6f5d0>it </mark><mark style=background-color:#e4f4cd>' </mark><mark style=background-color:#e7f5d2>s </mark><mark style=background-color:#aeda7a>very </mark><mark style=background-color:#bde38d>interesting </mark><mark style=background-color:#f7f7f6>that </mark><mark style=background-color:#eff6e4>robert </mark><mark style=background-color:#f5f7f3>alt </mark><mark style=background-color:#e7f5d3>##man </mark><mark style=background-color:#f8f3f6>directed </mark><mark style=background-color:#f7f7f6>this </mark><mark style=background-color:#f8f4f6>considering </mark><mark style=background-color:#c6e79c>the </mark><mark style=background-color:#e6f5d0>style </mark><mark style=background-color:#d2ecb0>and </mark><mark style=background-color:#9ed067>structure </mark><mark style=background-color:#cfebaa>of </mark><mark style=background-color:#c2e596>his </mark><mark style=background-color:#f5f7f3>other </mark><mark style=background-color:#ecf6de>films </mark><mark style=background-color:#f0f6e7>still </mark><mark style=background-color:#e7f5d3>the </mark><mark style=background-color:#ebf6dc>trademark </mark><mark style=background-color:#faedf3>alt </mark><mark style=background-color:#eaf5d9>##man </mark><mark style=background-color:#edf6df>audio </mark><mark style=background-color:#e4f4cd>style </mark><mark style=background-color:#eef6e2>is </mark><mark style=background-color:#d6eeb6>evident </mark><mark style=background-color:#eef6e2>here </mark><mark style=background-color:#e4f4cd>and </mark><mark style=background-color:#eff6e4>there </mark><mark style=background-color:#edf6e1>i </mark><mark style=background-color:#fbe9f2>think </mark><mark style=background-color:#f1f6e8>what </mark><mark style=background-color:#f6f7f5>really </mark><mark style=background-color:#eff6e4>makes </mark><mark style=background-color:#f6f7f5>this </mark><mark style=background-color:#e8f5d5>film </mark><mark style=background-color:#eaf5d9>work </mark><mark style=background-color:#f1f6ea>is </mark><mark style=background-color:#dff2c4>the </mark><mark style=background-color:#93c959>brilliant </mark><mark style=background-color:#ebf6db>performance </mark><mark style=background-color:#e8f5d5>by </mark><mark style=background-color:#ecf6de>sandy </mark><mark style=background-color:#f6f7f5>dennis </mark><mark style=background-color:#e8f5d5>it </mark><mark style=background-color:#eaf5d9>' </mark><mark style=background-color:#e9f5d6>s </mark><mark style=background-color:#f0f6e7>definitely </mark><mark style=background-color:#f1f6e8>one </mark><mark style=background-color:#f9eef4>of </mark><mark style=background-color:#f2f6ec>her </mark><mark style=background-color:#f1f6ea>darker </mark><mark style=background-color:#e8f5d5>characters </mark><mark style=background-color:#f6f7f5>but </mark><mark style=background-color:#f7f7f6>she </mark><mark style=background-color:#e9f5d8>plays </mark><mark style=background-color:#f0f6e7>it </mark><mark style=background-color:#d8efb9>so </mark><mark style=background-color:#c7e89f>perfectly </mark><mark style=background-color:#d4edb3>and </mark><mark style=background-color:#d8efb9>convincing </mark><mark style=background-color:#d2ecb0>##ly </mark><mark style=background-color:#f7f7f6>[SEP] </mark>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = 1\n",
    "display(X=X_test[index], attrs=attrs[index], pred=predictions[index], tokenizer=tokenizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
