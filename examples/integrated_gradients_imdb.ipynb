{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Embedding, LSTM, Input\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from alibi.explainers import IntegratedGradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%set_env CUDA_VISIBLE_DEVICES=2\n",
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.15.0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_WORDS=1000 # only use top 1000 words\n",
    "INDEX_FROM=3   # word index offset\n",
    "\n",
    "word_to_id = tf.keras.datasets.imdb.get_word_index()\n",
    "word_to_id = {k:(v+INDEX_FROM) for k,v in word_to_id.items()}\n",
    "word_to_id[\"<PAD>\"] = 0\n",
    "word_to_id[\"<START>\"] = 1\n",
    "word_to_id[\"<UNK>\"] = 2\n",
    "word_to_id[\"<UNUSED>\"] = 3\n",
    "\n",
    "id_to_word = {value:key for key,value in word_to_id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "25000 train sequences\n",
      "25000 test sequences\n",
      "Pad sequences (samples x time)\n",
      "x_train shape: (25000, 100)\n",
      "x_test shape: (25000, 100)\n"
     ]
    }
   ],
   "source": [
    "max_features = 20000\n",
    "# cut texts after this number of words (among top max_features most common words)\n",
    "maxlen = 100\n",
    "batch_size = 32\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "y_train, y_test = to_categorical(y_train), to_categorical(y_test)\n",
    "test_labels = np.argmax(y_test, axis=1)\n",
    "train_labels = np.argmax(y_train, axis=1)\n",
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corner and this helps to ensure that maléfique actually does manage to be quite frightening the film is memorable for a lot of reasons outside the central plot the characters are all very interesting in their own way and the fact that the book itself almost takes on its own character is very well done anyone worried that the film won't deliver by the end won't be disappointed either as the ending both makes sense and manages to be quite horrifying overall maléfique is a truly great horror film and one of the best of the decade highly recommended viewing\n"
     ]
    }
   ],
   "source": [
    "print(' '.join(id_to_word[id] for id in x_train[10] ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_model = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/fdz/anaconda3/envs/tf115/lib/python3.7/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "filepath = './model_imdb/'  # change to directory where model is downloaded\n",
    "if load_model:\n",
    "    model = tf.keras.models.load_model(os.path.join(filepath, 'model.h5'))\n",
    "else:\n",
    "    print('Build model...')\n",
    "    inputs = Input(shape=(x_train.shape[1:]), dtype=tf.float64)\n",
    "    x = Embedding(max_features, 128)(inputs)\n",
    "    x = LSTM(128, dropout=0.2, recurrent_dropout=0.2)(x)\n",
    "    outputs = Dense(2, activation='softmax')(x)\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    # try using different optimizers and different optimizer configs\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    print('Train...')\n",
    "    model.fit(x_train[:1000], y_train[:1000],\n",
    "              batch_size=batch_size,\n",
    "              epochs=1,\n",
    "              validation_data=(x_test[:1000], y_test[:1000]))\n",
    "    #score, acc = model.evaluate(x_test, y_test,\n",
    "    #                            batch_size=batch_size)\n",
    "    #print('Test score:', score)\n",
    "    #print('Test accuracy:', acc)\n",
    "        \n",
    "    if not os.path.exists(filepath):\n",
    "        os.makedirs(filepath)\n",
    "    model.save(os.path.join(filepath, 'model.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, 100, 128)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[1].output_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 100)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test[:1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 5\n",
    "method = \"gausslegendre\"\n",
    "return_convergence_delta = True\n",
    "return_predictions = False\n",
    "ig  = IntegratedGradients(model, layer=model.layers[1],\n",
    "                          n_steps=n_steps, \n",
    "                          method=method,\n",
    "                          return_convergence_delta=return_convergence_delta, \n",
    "                          return_predictions=return_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_samples = 5\n",
    "bs = 10\n",
    "x_test_red = x_test[:nb_samples]\n",
    "test_labels_red = test_labels[:nb_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 0, 1])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels_red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "explanation = ig.explain(x_test_red, \n",
    "                         baselines=None, \n",
    "                         target=test_labels_red, \n",
    "                         internal_batch_size=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The Explanation object is not a dictionary anymore and accessing elements should be done via attribute access. Accessing via item will stop working in a future version.\n"
     ]
    }
   ],
   "source": [
    "attrs = explanation['data']['attributions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     1,   591,   202,    14,\n",
       "          31,     6,   717,    10,    10, 18142, 10698,     5,     4,\n",
       "         360,     7,     4,   177,  5760,   394,   354,     4,   123,\n",
       "           9,  1035,  1035,  1035,    10,    10,    13,    92,   124,\n",
       "          89,   488,  7944,   100,    28,  1668,    14,    31,    23,\n",
       "          27,  7479,    29,   220,   468,     8,   124,    14,   286,\n",
       "         170,     8,   157,    46,     5,    27,   239,    16,   179,\n",
       "       15387,    38,    32,    25,  7944,   451,   202,    14,     6,\n",
       "         717], dtype=int32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test_red[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <START> please give this one a miss br br kristy swanson and the rest of the cast rendered terrible performances the show is flat flat flat br br i don't know how michael madison could have allowed this one on his plate he almost seemed to know this wasn't going to work out and his performance was quite lacklustre so all you madison fans give this a miss\n"
     ]
    }
   ],
   "source": [
    "print(' '.join(id_to_word[id] for id in x_test_red[0] ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "attrs_0 = attrs[0].sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "       -6.14434208e-09, -1.65919498e-08, -9.61821435e-09, -1.13566885e-08,\n",
       "        2.18093225e-08, -1.63480299e-09,  1.72306335e-08,  7.04142433e-08,\n",
       "        1.02810079e-07,  2.72824293e-07,  1.04251637e-07, -8.46694389e-08,\n",
       "       -5.26614775e-07,  5.16491359e-07, -1.75367101e-07, -9.60274328e-07,\n",
       "       -9.76610996e-08,  7.23789210e-07,  2.20366624e-06, -6.61745160e-07,\n",
       "       -2.51425975e-06,  8.97721920e-07, -1.43203625e-06,  8.05948355e-06,\n",
       "        9.94117064e-06,  1.22356005e-05,  1.30286470e-05,  1.63883174e-05,\n",
       "        1.45494203e-05,  1.69876445e-05,  3.04286984e-05,  4.87880860e-05,\n",
       "        5.59520560e-05,  5.10311269e-05,  8.14328492e-05,  6.86282591e-05,\n",
       "        9.28381241e-05,  1.20578157e-04,  3.16702798e-05,  1.51060846e-04,\n",
       "        2.16874482e-04,  5.13686978e-05,  3.62465012e-05,  3.95021580e-04,\n",
       "        3.66474417e-04,  1.84872390e-04,  6.28681171e-04,  8.28922431e-04,\n",
       "        1.05558460e-03,  4.65192807e-04,  3.61961082e-04,  1.98555139e-04,\n",
       "        1.80056207e-03, -6.90012760e-04,  2.09883574e-03, -4.99129607e-04,\n",
       "        3.04458792e-03,  1.77921209e-03,  1.78694674e-03,  1.97224721e-03,\n",
       "        4.20936174e-04,  3.40785463e-04,  1.90903157e-03, -3.71963380e-03,\n",
       "        6.77090659e-04,  1.90249690e-02, -1.80565850e-02, -7.98668907e-03])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attrs_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = ' '.join(id_to_word[id] for id in x_test_red[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_id['<PAD>'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = [word_to_id[w] for w in sentence.split()]\n",
    "inp = [0 for i in range(100 - len(inp))] + inp\n",
    "inp = np.asarray(inp).reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = np.argmax(model(inp).numpy(), axis=1)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The Explanation object is not a dictionary anymore and accessing elements should be done via attribute access. Accessing via item will stop working in a future version.\n"
     ]
    }
   ],
   "source": [
    "attrs = ig.explain(inp, target=pred)['data']['attributions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        6.14434890e-09,  1.65919591e-08,  9.61821494e-09,  1.13566703e-08,\n",
       "       -2.18093007e-08,  1.63481875e-09, -1.72306381e-08, -7.04142481e-08,\n",
       "       -1.02810082e-07, -2.72824276e-07, -1.04251585e-07,  8.46695051e-08,\n",
       "        5.26614861e-07, -5.16491436e-07,  1.75367173e-07,  9.60274351e-07,\n",
       "        9.76611269e-08, -7.23789080e-07, -2.20366611e-06,  6.61745087e-07,\n",
       "        2.51425956e-06, -8.97722168e-07,  1.43203633e-06, -8.05948315e-06,\n",
       "       -9.94117033e-06, -1.22356006e-05, -1.30286477e-05, -1.63883184e-05,\n",
       "       -1.45494207e-05, -1.69876461e-05, -3.04286966e-05, -4.87880884e-05,\n",
       "       -5.59520541e-05, -5.10311256e-05, -8.14328458e-05, -6.86282565e-05,\n",
       "       -9.28381216e-05, -1.20578163e-04, -3.16702739e-05, -1.51060844e-04,\n",
       "       -2.16874490e-04, -5.13686951e-05, -3.62465022e-05, -3.95021553e-04,\n",
       "       -3.66474406e-04, -1.84872378e-04, -6.28681185e-04, -8.28922422e-04,\n",
       "       -1.05558462e-03, -4.65192793e-04, -3.61961048e-04, -1.98555177e-04,\n",
       "       -1.80056215e-03,  6.90012783e-04, -2.09883571e-03,  4.99129656e-04,\n",
       "       -3.04458796e-03, -1.77921202e-03, -1.78694674e-03, -1.97224732e-03,\n",
       "       -4.20936076e-04, -3.40785674e-04, -1.90903145e-03,  3.71963383e-03,\n",
       "       -6.77090562e-04, -1.90249681e-02,  1.80565857e-02,  7.98668907e-03])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attrs[0].sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf115] *",
   "language": "python",
   "name": "conda-env-tf115-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
