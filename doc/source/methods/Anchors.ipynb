{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[source]](../api/alibi.explainers.anchor_tabular.rst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anchors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The anchor algorithm is based on the [Anchors: High-Precision Model-Agnostic Explanations](https://homes.cs.washington.edu/~marcotcr/aaai18.pdf) paper by Ribeiro et al. and builds on the open source [code](https://github.com/marcotcr/anchor) from the paper's first author.\n",
    "\n",
    "The algorithm provides model-agnostic (*black box*) and human interpretable explanations suitable for classification models applied to images, text and tabular data. The idea behind anchors is to explain the behaviour of complex models with high-precision rules called *anchors*. These anchors are locally sufficient conditions to ensure a certain prediction with a high degree of confidence.\n",
    "\n",
    "Anchors address a key shortcoming of local explanation methods like [LIME](https://arxiv.org/abs/1602.04938) which proxy the local behaviour of the model in a linear way. It is however unclear to what extent the explanation holds up in the region around the instance to be explained, since both the model and data can exhibit non-linear behaviour in the neighborhood of the instance. This approach can easily lead to overconfidence in the explanation and misleading conclusions on unseen but similar instances. The anchor algorithm tackles this issue by incorporating coverage, the region where the explanation applies, into the optimization problem. A simple example from sentiment classification illustrates this (Figure 1). Dependent on the sentence, the occurrence of the word *not* is interpreted as positive or negative for the sentiment by LIME. It is clear that the explanation using *not* is very local. Anchors however aim to maximize the coverage, and require *not* to occur together with *good* or *bad* to ensure respectively negative or positive sentiment.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![LIMEsentiment](lime_sentiment.png)\n",
    "\n",
    "Ribeiro et al., *Anchors: High-Precision Model-Agnostic Explanations*, 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As highlighted by the above example, an anchor explanation consists of *if-then rules*, called the anchors, which sufficiently guarantee the explanation locally and try to maximize the area for which the explanation holds. This means that as long as the anchor holds, the prediction should remain the same regardless of the values of the features not present in the anchor. Going back to the sentiment example: as long as *not good* is present, the sentiment is negative, regardless of the other words in the movie review."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For text classification, an interpretable anchor consists of the words that need to be present to ensure a prediction, regardless of the other words in the input. The words that are not present in a candidate anchor can be sampled in 2 ways:\n",
    "\n",
    "* Replace word token by UNK token.\n",
    "\n",
    "* Replace word token by sampled token from a corpus with the same POS tag and probability proportional to the similarity in the embedding space. By sampling similar words, we keep more context than simply using the UNK token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tabular Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anchors are also suitable for tabular data with both categorical and continuous features. The continuous features are discretized into quantiles (e.g. deciles), so they become more interpretable. The features in a candidate anchor are kept constant (same category or bin for discretized features) while we sample the other features from a training set. As a result, anchors for tabular data need access to training data. Let's illustrate this with an example. Say we want to predict whether a person makes less or more than £50,000 per year based on the person's characteristics including age (continuous variable) and marital status (categorical variable). The following would then be a potential anchor: Hugo makes more than £50,000 because he is married and his age is between 35 and 45 years."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to LIME, images are first segmented into superpixels, maintaining local image structure. The interpretable representation then consists of the presence or absence of each superpixel in the anchor. It is crucial to generate meaningful superpixels in order to arrive at interpretable explanations. The algorithm supports a number of standard image segmentation algorithms ([felzenszwalb, slic and quickshift](https://scikit-image.org/docs/dev/auto_examples/segmentation/plot_segmentations.html#sphx-glr-auto-examples-segmentation-plot-segmentations-py)) and allows the user to provide a custom segmentation function.\n",
    "\n",
    "The superpixels not present in a candidate anchor can be masked in 2 ways:\n",
    "\n",
    "* Take the average value of that superpixel.\n",
    "\n",
    "* Use the pixel values of a superimposed picture over the masked superpixels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![anchorimage](anchor_image.png)\n",
    "\n",
    "Ribeiro et al., *Anchors: High-Precision Model-Agnostic Explanations*, 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Efficiently Computing Anchors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The anchor needs to return the same prediction as the original instance with a minimal confidence of e.g. 95%. If multiple candidate anchors satisfy this constraint, we go with the anchor that has the largest coverage. Because the number of potential anchors is exponential in the feature space, we need a faster approximate solution.\n",
    "\n",
    "The anchors are constructed bottom-up in combination with [beam search](https://en.wikipedia.org/wiki/Beam_search). We start with an empty rule or anchor, and incrementally add an *if-then* rule in each iteration until the minimal confidence constraint is satisfied. If multiple valid anchors are found, the one with the largest coverage is returned.\n",
    "\n",
    "In order to select the best candidate anchors for the beam width efficiently during each iteration, we formulate the problem as a [pure exploration multi-armed bandit](https://www.cse.iitb.ac.in/~shivaram/papers/kk_colt_2013.pdf) problem. This limits the number of model prediction calls which can be a computational bottleneck.\n",
    "\n",
    "For more details, we refer the reader to the original [paper](https://homes.cs.washington.edu/~marcotcr/aaai18.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While each data type has specific requirements to initialize the explainer and return explanations, the underlying algorithm to construct the anchors is the same.\n",
    "\n",
    "In order to efficiently generate anchors, the following hyperparameters need to be set to sensible values when calling the `explain` method:\n",
    "\n",
    "* `threshold`: the previously discussed minimal confidence level. `threshold` defines the minimum fraction of samples for a candidate anchor that need to lead to the same prediction as the original instance. A higher value gives more confidence in the anchor, but also leads to more computation time. The default value is 0.95.\n",
    "\n",
    "* `tau`: determines when we assume convergence for the multi-armed bandit. A bigger value for `tau` means faster convergence but also looser anchor conditions. By default equal to 0.15.\n",
    "\n",
    "* `beam_size`: the size of the beam width. A bigger beam width can lead to a better overall anchor at the expense of more computation time.\n",
    "\n",
    "* `batch_size`: the batch size used for sampling. A bigger batch size gives more confidence in the anchor, again at the expense of computation time since it involves more model prediction calls. The default value is 100.\n",
    "\n",
    "* `coverage_samples`: number of samples used to compute the coverage of the anchor. By default set to 10000."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the explainer works on black box models, only access to a predict function is needed. The model below is a simple logistic regression trained on movie reviews with negative or positive sentiment and pre-processed with a CountVectorizer:\n",
    "\n",
    "```python\n",
    "predict_fn = lambda x: clf.predict(vectorizer.transform(x))\n",
    "```\n",
    "\n",
    "If we choose to sample similar words from a corpus, we first need to load a spaCy model:\n",
    "\n",
    "```python\n",
    "import spacy\n",
    "from alibi.utils.download import spacy_model\n",
    "\n",
    "model = 'en_core_web_md'\n",
    "spacy_model(model=model)\n",
    "nlp = spacy.load(model)\n",
    "```\n",
    "\n",
    "We can now initialize our explainer:\n",
    "\n",
    "```python\n",
    "explainer = AnchorText(nlp, predict_fn)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define the instance we want to explain and verify that the sentiment prediction on the original instance is positive:\n",
    "\n",
    "```python\n",
    "text = 'This is a good book .'\n",
    "class_names = ['negative', 'positive']\n",
    "pred = class_names[predict_fn([text])[0]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can explain the instance:\n",
    "\n",
    "```python\n",
    "explanation = explainer.explain(text, threshold=0.95, use_similarity_proba=False, \n",
    "                                use_unk=True, sample_proba=0.5)\n",
    "```\n",
    "We set the confidence `threshold` at 95%. `use_unk` equals True means that we replace words outside of the candidate anchor with UNK tokens with a sample probability equal to `sample_proba`. Instead of using UNK tokens, we can sample from the `top_n` similar words to the ground truth word in the corpus by setting `use_unk` to False.\n",
    "\n",
    "```python\n",
    "explanation = explainer.explain(text, threshold=0.95, use_unk=False, sample_proba=0.5, top_n=100)\n",
    "```\n",
    "\n",
    "It is also possible to sample words from the corpus proportional to the word similarity with the ground truth word by setting `use_similarity_proba` to True and `use_unk` to False. We can put more weight on similar words by decreasing the `temperature` argument. The following explanation perturbs original tokens with probability equal to `sample_proba`. The perturbed tokens are then sampled from the `top_n` most similar tokens in the corpus with sample probability proportional to the word similarity with the original token.\n",
    "\n",
    "\n",
    "```python\n",
    "explanation = explainer.explain(text, threshold=0.95, use_similarity_proba=True, use_unk=False, \n",
    "                                sample_proba=0.5, top_n=20, temperature=0.2)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `explain` method returns a dictionary that contains *key: value* pairs for:\n",
    "\n",
    "* *names*: the words in the anchor.\n",
    "\n",
    "* *precision*: the fraction of times the sampled instances where the anchor holds yields the same prediction as the original instance. The precision will always be $\\geq$ `threshold` for a valid anchor.\n",
    "\n",
    "* *coverage*: the fraction of sampled instances the anchor applies to.\n",
    "\n",
    "Under the *raw* key, the dictionary also contains example instances where the anchor holds and the prediction is the same as on the original instance, as well as examples where the anchor holds but the prediction changed to give the user a sense of where the anchor fails. *raw* also stores information on the *names*, *precision* and *coverage* of partial anchors. This allows the user to track the improvement in for instance the *precision* as more features (words in the case of text) are added to the anchor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tabular Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialization and fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To initialize the explainer, we provide a predict function, a list with the feature names to make the anchors easy to understand as well as an optional mapping from the encoded categorical features to a description of the category. An example for `categorical_names` would be *category_map = {0: list('married', 'divorced'), 3: list('high school diploma', 'master's degree')}*. Each key in *category_map* refers to the column index in the input for the relevant categorical variable, while the values are lists with the options for each categorical variable. To make it easy, we provide a utility function `gen_category_map` to generate this map automatically from a Pandas dataframe:\n",
    "\n",
    "```python\n",
    "from alibi.utils.data import gen_category_map\n",
    "category_map = gen_category_map(df)\n",
    "```\n",
    "\n",
    "Then initialize the explainer:\n",
    "```python\n",
    "predict_fn = lambda x: clf.predict(preprocessor.transform(x))\n",
    "explainer = AnchorTabular(predict_fn, feature_names, categorical_names=category_map)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tabular data requires a fit step to map the ordinal features into quantiles and therefore needs access to a representative set of the training data. `disc_perc` is a list with percentiles used for binning:\n",
    "\n",
    "```python\n",
    "explainer.fit(X_train, disc_perc=[25, 50, 75])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the prediction of the model on the original instance and explain:\n",
    "\n",
    "```python\n",
    "class_names = ['<=50K', '>50K']\n",
    "pred = class_names[explainer.predict_fn(X)[0]]\n",
    "explanation = explainer.explain(X, threshold=0.95)\n",
    "```\n",
    "\n",
    "The returned explanation dictionary contains the same *key: value* pairs as the text explainer, so you could explain a prediction as follows:\n",
    "\n",
    "```\n",
    "Prediction:  <=50K\n",
    "Anchor: Marital Status = Never-Married AND Relationship = Own-child\n",
    "Precision: 1.00\n",
    "Coverage: 0.13\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides the predict function, we also need to specify either a built in or custom superpixel segmentation function. The built in methods are [felzenszwalb](https://scikit-image.org/docs/dev/api/skimage.segmentation.html#skimage.segmentation.felzenszwalb), [slic](https://scikit-image.org/docs/dev/api/skimage.segmentation.html#skimage.segmentation.slic) and [quickshift](https://scikit-image.org/docs/dev/api/skimage.segmentation.html#skimage.segmentation.quickshift). It is important to create sensible superpixels in order to speed up convergence and generate interpretable explanations. Tuning the hyperparameters of the segmentation method is recommended.\n",
    "\n",
    "```python\n",
    "explainer = AnchorImage(predict_fn, image_shape, segmentation_fn='slic', \n",
    "                        segmentation_kwargs={'n_segments': 15, 'compactness': 20, 'sigma': .5}, \n",
    "                        images_background=None)\n",
    "```\n",
    "\n",
    "Example of superpixels generated for the Persian cat picture using the *slic* method:\n",
    "\n",
    "![persiancat](persiancat.png)\n",
    "![persiancatsegm](persiancatsegm.png)\n",
    "\n",
    "The following function would be an example of a custom segmentation function dividing the image into rectangles.\n",
    "\n",
    "\n",
    "```python\n",
    "def superpixel(image, size=(4, 7)):\n",
    "    segments = np.zeros([image.shape[0], image.shape[1]])\n",
    "    row_idx, col_idx = np.where(segments == 0)\n",
    "    for i, j in zip(row_idx, col_idx):\n",
    "        segments[i, j] = int((image.shape[1]/size[1]) * (i//size[0]) + j//size[1])\n",
    "    return segments\n",
    "```\n",
    "\n",
    "The `images_background` parameter allows the user to provide images used to superimpose on the masked superpixels, not present in the candidate anchor, instead of taking the average value of the masked superpixel. The superimposed images need to have the same shape as the explained instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then explain the instance in the usual way:\n",
    "\n",
    "```python\n",
    "explanation = explainer.explain(image, p_sample=.5)\n",
    "```\n",
    "\n",
    "`p_sample` determines the fraction of superpixels that are either changed to the average superpixel value or that are superimposed. \n",
    "\n",
    "The explanation dictionary again contains information about the anchor's *precision*, *coverage* and examples where the anchor does or does not hold. On top of that, it also contains a masked image with only the anchor superpixels visible under the *anchor* key (see image below) as well as the image's superpixels under *segments*.\n",
    "\n",
    "![persiancatanchor](persiancatanchor.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Anchor explanations for ImageNet](../examples/anchor_image_imagenet.nblink)\n",
    "\n",
    "[Anchor explanations for fashion MNIST](../examples/anchor_image_fashion_mnist.nblink)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tabular Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Anchor explanations on the Iris dataset](../examples/anchor_tabular_iris.nblink)\n",
    "\n",
    "[Anchor explanations for income prediction](../examples/anchor_tabular_adult.nblink)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Anchor explanations for movie sentiment](../examples/anchor_text_movie.nblink)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
