{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import os\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, LSTM\n",
    "from alibi.explainers import IntegratedGradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%set_env CUDA_VISIBLE_DEVICES=2\n",
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_WORDS=1000 # only use top 1000 words\n",
    "INDEX_FROM=3   # word index offset\n",
    "\n",
    "word_to_id = tf.keras.datasets.imdb.get_word_index()\n",
    "word_to_id = {k:(v+INDEX_FROM) for k,v in word_to_id.items()}\n",
    "word_to_id[\"<PAD>\"] = 0\n",
    "word_to_id[\"<START>\"] = 1\n",
    "word_to_id[\"<UNK>\"] = 2\n",
    "word_to_id[\"<UNUSED>\"] = 3\n",
    "\n",
    "id_to_word = {value:key for key,value in word_to_id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "25000 train sequences\n",
      "25000 test sequences\n",
      "Pad sequences (samples x time)\n",
      "x_train shape: (25000, 100)\n",
      "x_test shape: (25000, 100)\n"
     ]
    }
   ],
   "source": [
    "max_features = 20000\n",
    "# cut texts after this number of words (among top max_features most common words)\n",
    "maxlen = 100\n",
    "batch_size = 32\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corner and this helps to ensure that maléfique actually does manage to be quite frightening the film is memorable for a lot of reasons outside the central plot the characters are all very interesting in their own way and the fact that the book itself almost takes on its own character is very well done anyone worried that the film won't deliver by the end won't be disappointed either as the ending both makes sense and manages to be quite horrifying overall maléfique is a truly great horror film and one of the best of the decade highly recommended viewing\n"
     ]
    }
   ],
   "source": [
    "print(' '.join(id_to_word[id] for id in x_train[10] ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_model = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/gio/anaconda3/envs/tf115/lib/python3.7/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "filepath = './model_imdb/'  # change to directory where model is downloaded\n",
    "if load_model:\n",
    "    model = tf.keras.models.load_model(os.path.join(filepath, 'model.h5'))\n",
    "else:\n",
    "    print('Build model...')\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(max_features, 128))\n",
    "    model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    # try using different optimizers and different optimizer configs\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    print('Train...')\n",
    "    model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=1,\n",
    "              validation_data=(x_test, y_test))\n",
    "    score, acc = model.evaluate(x_test, y_test,\n",
    "                                batch_size=batch_size)\n",
    "    print('Test score:', score)\n",
    "    print('Test accuracy:', acc)\n",
    "        \n",
    "    if not os.path.exists(filepath):\n",
    "        os.makedirs(filepath)\n",
    "    model.save(os.path.join(filepath, 'model.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, None, 128)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[0].output_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 100)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test[:1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 50\n",
    "method = \"gausslegendre\"\n",
    "return_convergence_delta = True\n",
    "return_predictions = False\n",
    "ig  = IntegratedGradients(model, layer=model.layers[0],\n",
    "                          n_steps=n_steps, \n",
    "                          method=method,\n",
    "                          return_convergence_delta=return_convergence_delta, \n",
    "                          return_predictions=return_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_samples = 5\n",
    "bs = 10\n",
    "x_test_red = x_test[:nb_samples]\n",
    "test_labels_red = y_test[:nb_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 100, 128)\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "(10, 100, 128)\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "(10, 100, 128)\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "(10, 100, 128)\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "(10, 100, 128)\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "(10, 100, 128)\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "(10, 100, 128)\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "(10, 100, 128)\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "(10, 100, 128)\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "(10, 100, 128)\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "(10, 100, 128)\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "(10, 100, 128)\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "(10, 100, 128)\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "(10, 100, 128)\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "(10, 100, 128)\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "(10, 100, 128)\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "(10, 100, 128)\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "(10, 100, 128)\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "(10, 100, 128)\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "(10, 100, 128)\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "(10, 100, 128)\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "(10, 100, 128)\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "(10, 100, 128)\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "(10, 100, 128)\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "(10, 100, 128)\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "[[[ 7.18428481e-08 -1.08220299e-07 -1.27401151e-07 ...  1.51166631e-07\n",
      "    1.28184032e-08 -3.95174240e-08]\n",
      "  [ 7.41421786e-08 -1.12365107e-07 -1.34518189e-07 ...  1.55006745e-07\n",
      "    1.22185035e-08 -5.48732295e-08]\n",
      "  [ 7.69362600e-08 -1.17110631e-07 -1.42525579e-07 ...  1.59569836e-07\n",
      "    1.14270424e-08 -7.10862125e-08]\n",
      "  ...\n",
      "  [ 5.22977486e-03 -5.37609262e-03 -9.53499787e-03 ...  7.60151725e-03\n",
      "    8.75849160e-04 -1.19944625e-02]\n",
      "  [ 6.96036220e-03 -6.48593344e-03 -1.19252326e-02 ...  9.22269281e-03\n",
      "    1.63916370e-03 -1.47794243e-02]\n",
      "  [ 1.01627614e-02 -7.71790650e-03 -1.58242844e-02 ...  8.49955808e-03\n",
      "    4.44815960e-03 -2.81284489e-02]]\n",
      "\n",
      " [[ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00]\n",
      "  ...\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00]]\n",
      "\n",
      " [[ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00]\n",
      "  ...\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00]\n",
      "  ...\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00]]\n",
      "\n",
      " [[ 1.21946330e-03 -1.81547797e-03 -2.13817670e-03 ...  2.56459182e-03\n",
      "    2.19196605e-04 -1.83433440e-04]\n",
      "  [ 1.12596399e-03 -1.68832554e-03 -2.01553199e-03 ...  2.37648305e-03\n",
      "    2.40019406e-04 -6.24900218e-04]\n",
      "  [ 1.17110217e-03 -1.73436385e-03 -2.09359895e-03 ...  2.40336871e-03\n",
      "    2.05371412e-04 -4.71687934e-04]\n",
      "  ...\n",
      "  [ 1.33306002e-02 -1.62879862e-02 -1.96943507e-02 ...  2.76371818e-02\n",
      "    3.95082450e-03  9.05727595e-03]\n",
      "  [ 1.56250857e-02 -1.75507441e-02 -2.26718355e-02 ...  2.90348250e-02\n",
      "    5.06470632e-03  3.60031961e-03]\n",
      "  [ 1.81748196e-02 -1.90996192e-02 -2.57743597e-02 ...  3.38426009e-02\n",
      "    5.13905473e-03  6.04121387e-03]]\n",
      "\n",
      " [[ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00]\n",
      "  ...\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "    0.00000000e+00  0.00000000e+00]]]\n",
      "(250, 100, 128)\n",
      "<class 'numpy.ndarray'>\n",
      "(50, 5, None, 128)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-fce107ccbe73>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m                          \u001b[0mbaselines\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                          \u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_labels_red\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m                          internal_batch_size=bs)\n\u001b[0m",
      "\u001b[0;32m~/git/alibi-fork/alibi/explainers/integrated_gradients.py\u001b[0m in \u001b[0;36mexplain\u001b[0;34m(self, X, baselines, features_names, target, internal_batch_size)\u001b[0m\n\u001b[1;32m    357\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[0;31m#grads = tf.reshape(grads, orig_shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m         \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrads\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0;31m# sum integral terms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "explanation = ig.explain(x_test_red, \n",
    "                         baselines=None, \n",
    "                         target=test_labels_red, \n",
    "                         internal_batch_size=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf115] *",
   "language": "python",
   "name": "conda-env-tf115-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
