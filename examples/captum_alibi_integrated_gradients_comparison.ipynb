{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=1\n"
     ]
    }
   ],
   "source": [
    "%set_env CUDA_VISIBLE_DEVICES=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF version:  2.3.1\n",
      "Eager execution enabled:  True\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Embedding, Conv1D, GlobalMaxPooling1D, Dropout \n",
    "from tensorflow.keras.layers import MaxPooling1D, Flatten, Activation\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from alibi.explainers import IntegratedGradients\n",
    "from captum.attr import LayerIntegratedGradients, TokenReferenceBase\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "print('TF version: ', tf.__version__)\n",
    "print('Eager execution enabled: ', tf.executing_eagerly()) # True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 10000\n",
    "maxlen = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "25000 train sequences\n",
      "25000 test sequences\n",
      "Pad sequences (samples x time)\n",
      "x_train shape: (25000, 100)\n",
      "x_test shape: (25000, 100)\n"
     ]
    }
   ],
   "source": [
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "test_labels = y_test.copy()\n",
    "train_labels = y_train.copy()\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "y_train, y_test = to_categorical(y_train), to_categorical(y_test)\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "\n",
    "index = imdb.get_word_index()\n",
    "reverse_index = {value: key for (key, value) in index.items()} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sentence(x, reverse_index):\n",
    "    # the `-3` offset is due to the special tokens used by keras\n",
    "    # see https://stackoverflow.com/questions/42821330/restore-original-text-from-keras-s-imdb-dataset\n",
    "    return \" \".join([reverse_index.get(i - 3, 'UNK') for i in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a powerful study of loneliness sexual UNK and desperation be patient UNK up the atmosphere and pay attention to the wonderfully written script br br i praise robert altman this is one of his many films that deals with unconventional fascinating subject matter this film is disturbing but it's sincere and it's sure to UNK a strong emotional response from the viewer if you want to see an unusual film some might even say bizarre this is worth the time br br unfortunately it's very difficult to find in video stores you may have to buy it off the internet\n"
     ]
    }
   ],
   "source": [
    "print(decode_sentence(x_test[1], reverse_index)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "embedding_dims = 50\n",
    "filters = 250\n",
    "kernel_size = 3\n",
    "hidden_dims = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pytorch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.emb = nn.Embedding(max_features,\n",
    "                               embedding_dims)\n",
    "        self.linear1 = nn.Linear(5000, hidden_dims)\n",
    "        self.linear2 = nn.Linear(hidden_dims, 2)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.emb(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.linear1(x)\n",
    "        x = F.relu(x) # Adding relu layers makes the attributions different\n",
    "        x = self.linear2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "inputs = Input(shape=(maxlen,), \n",
    "               dtype='int32', \n",
    "               name='inputs')\n",
    "out = Embedding(max_features,\n",
    "                               embedding_dims, \n",
    "                               name='emb')(inputs)\n",
    "out = Flatten(name='Flat', data_format='channels_last')(out)\n",
    "out = Dense(hidden_dims, \n",
    "            name='linear1')(out)\n",
    "out = Activation('relu')(out) # Adding relu layers makes the attributions different\n",
    "out = Dense(2, \n",
    "                name='linear2')(out)\n",
    "model = Model(inputs=inputs, outputs=out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (emb): Embedding(10000, 50)\n",
       "  (linear1): Linear(in_features=5000, out_features=250, bias=True)\n",
       "  (linear2): Linear(in_features=250, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = Net()\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# transfer weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lnames = []\n",
    "for name in net.state_dict().keys():\n",
    "    lname = name.split('.')[0]\n",
    "    if lname not in lnames:\n",
    "        lnames.append(lname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emb (10000, 50)\n",
      "linear1 (5000, 250)\n",
      "linear2 (250, 2)\n"
     ]
    }
   ],
   "source": [
    "for name in lnames:\n",
    "\n",
    "    if 'conv' in name:\n",
    "        ws = net.state_dict()[name + '.weight'].cpu().numpy()\n",
    "        ws = np.transpose(ws, (2, 1, 0))\n",
    "        bs = net.state_dict()[name + '.bias'].cpu().numpy()\n",
    "        l = model.get_layer(name)\n",
    "        l.set_weights([ws, bs])\n",
    "    elif 'linear' in name:\n",
    "        ws = net.state_dict()[name + '.weight'].cpu().numpy()\n",
    "        ws = ws.T\n",
    "        bs = net.state_dict()[name + '.bias'].cpu().numpy()\n",
    "        l = model.get_layer(name)\n",
    "        l.set_weights([ws, bs])\n",
    "    elif 'emb' in name:\n",
    "        ws = net.state_dict()[name + '.weight'].cpu().numpy()\n",
    "        l = model.get_layer(name)\n",
    "        l.set_weights([ws])\n",
    "    print(name, ws.shape) #  , bs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_emb_pt = net.state_dict()['emb.weight']\n",
    "weights_emb_tf = model.layers[1].get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(weights_emb_pt, weights_emb_tf, rtol=1e-03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_samples = 10\n",
    "torch_X_test = torch.from_numpy(x_test)\n",
    "torch_y_test = torch.from_numpy(y_test)\n",
    "x_test_sample = torch_X_test[:nb_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 100])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test_sample.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10, 2), dtype=float32, numpy=\n",
       "array([[ 0.19532351, -0.15969233],\n",
       "       [ 0.40878993,  0.29023722],\n",
       "       [-0.263573  ,  0.17727989],\n",
       "       [-0.02696468, -0.13478509],\n",
       "       [-0.02748159,  0.15112618],\n",
       "       [-0.00592033, -0.01442789],\n",
       "       [-0.13127048,  0.12909648],\n",
       "       [-0.18316656,  0.05256993],\n",
       "       [-0.26749277,  0.10450753],\n",
       "       [-0.24014267,  0.40070525]], dtype=float32)>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(x_test_sample.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pytorch predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1953, -0.1597],\n",
       "        [ 0.4088,  0.2902],\n",
       "        [-0.2636,  0.1773],\n",
       "        [-0.0270, -0.1348],\n",
       "        [-0.0275,  0.1511],\n",
       "        [-0.0059, -0.0144],\n",
       "        [-0.1313,  0.1291],\n",
       "        [-0.1832,  0.0526],\n",
       "        [-0.2675,  0.1045],\n",
       "        [-0.2401,  0.4007]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(x_test_sample.to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intgrads comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 50\n",
    "method = \"gausslegendre\"\n",
    "internal_batch_size = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpret_sentence(model, indexed, min_len = 100, label = 1):\n",
    "\n",
    "    input_indices = indexed.to(device)\n",
    "    seq_length = min_len\n",
    "\n",
    "    # predict\n",
    "    pred = net.forward(input_indices)\n",
    "\n",
    "    # generate reference indices for each sample\n",
    "    reference_indices = torch.tensor(np.zeros(input_indices.shape), dtype=int).to(device)\n",
    "\n",
    "    # compute attributions and approximation delta using layer integrated gradients\n",
    "    attributions_ig, delta = lig.attribute(input_indices, \n",
    "                                           reference_indices, \n",
    "                                           target=label,\n",
    "                                           method=method,\n",
    "                                           n_steps=50, \n",
    "                                           return_convergence_delta=True)\n",
    "    \n",
    "    return attributions_ig, delta, reference_indices.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "lig = LayerIntegratedGradients(net, net.emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attributions shape: (10, 100, 50)\n"
     ]
    }
   ],
   "source": [
    "token_reference = TokenReferenceBase(reference_token_idx=0)\n",
    "# For simplicity, we compute the attribution relative to label = 1 for all samples\n",
    "attributions_pt, delta, reference_indices = interpret_sentence(net, \n",
    "                                                               x_test_sample, \n",
    "                                                               label=1)\n",
    "attributions_pt = attributions_pt.numpy()\n",
    "print('Attributions shape:', attributions_pt.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.layers.embeddings.Embedding at 0x7f1dc3657d90>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer = model.layers[1]\n",
    "layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "ig  = IntegratedGradients(model,\n",
    "                          layer=layer,\n",
    "                          n_steps=n_steps, \n",
    "                          method=method,\n",
    "                          internal_batch_size=internal_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model output_shape (None, 2)\n",
      "model output_shape (None, 2)\n",
      "model output_shape (None, 2)\n",
      "model output_shape (None, 2)\n",
      "model output_shape (None, 2)\n",
      "model output_shape (None, 2)\n",
      "baselines [array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int32)]\n",
      "[array([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    1,\n",
      "         591,  202,   14,   31,    6,  717,   10,   10,    2,    2,    5,\n",
      "           4,  360,    7,    4,  177, 5760,  394,  354,    4,  123,    9,\n",
      "        1035, 1035, 1035,   10,   10,   13,   92,  124,   89,  488, 7944,\n",
      "         100,   28, 1668,   14,   31,   23,   27, 7479,   29,  220,  468,\n",
      "           8,  124,   14,  286,  170,    8,  157,   46,    5,   27,  239,\n",
      "          16,  179,    2,   38,   32,   25, 7944,  451,  202,   14,    6,\n",
      "         717],\n",
      "       [   6,  976, 2078,    7, 5293,  861,    2,    5, 4182,   30, 3127,\n",
      "           2,   56,    4,  841,    5,  990,  692,    8,    4, 1669,  398,\n",
      "         229,   10,   10,   13, 2822,  670, 5304,   14,    9,   31,    7,\n",
      "          27,  111,  108,   15, 2033,   19, 7836, 1429,  875,  551,   14,\n",
      "          22,    9, 1193,   21,   45, 4829,    5,   45,  252,    8,    2,\n",
      "           6,  565,  921, 3639,   39,    4,  529,   48,   25,  181,    8,\n",
      "          67,   35, 1732,   22,   49,  238,   60,  135, 1162,   14,    9,\n",
      "         290,    4,   58,   10,   10,  472,   45,   55,  878,    8,  169,\n",
      "          11,  374, 5687,   25,  203,   28,    8,  818,   12,  125,    4,\n",
      "        3077],\n",
      "       [   4, 5673,    7,   15,    2, 9252, 3286,  325,   12,   62,   30,\n",
      "         776,    8,   67,   14,   17,    6,    2,   44,  148,  687,    2,\n",
      "         203,   42,  203,   24,   28,   69,    2, 6676,   11,  330,   54,\n",
      "          29,   93,    2,   21,  845,    2,   27, 1099,    7,  819,    4,\n",
      "          22, 1407,   17,    6,    2,  787,    7, 2460,    2,    2,  100,\n",
      "          30,    4, 3737, 3617, 3169, 2321,   42, 1898,   11,    4, 3814,\n",
      "          42,  101,  704,    7,  101,  999,   15, 1625,   94, 2926,  180,\n",
      "           5,    9, 9101,   34,    2,   45,    6, 1429,   22,   60,    6,\n",
      "        1220,   31,   11,   94, 6408,   96,   21,   94,  749,    9,   57,\n",
      "         975],\n",
      "       [   4,  452,   11,   14,   20,    9, 8654,   19,   41,  476,    8,\n",
      "           4,  213,    7, 9185,   13,  657,   13,  286,   38, 1612,   44,\n",
      "          41,    5,   41, 1729,   88,   13,   62,   28,  900,  510,    4,\n",
      "         509,   51,    6,  612,   59,   16,  193,   61, 4666,    5,  702,\n",
      "         930,  143,  285,   25,   67,   41,   81,  366,    4,  130,   82,\n",
      "           9,  259,  334,  397, 1195,    7,  149,  102,   15,   26,  814,\n",
      "          38,  465, 1627,   31,   70,  983,   67,   51,    9,  112,  814,\n",
      "          17,   35,  311,   75,   26,    2,  574,   19,    4, 1729,   23,\n",
      "           4,  268,   38,   95,  138,    4,  609,  191,   75,   28,  314,\n",
      "        1772],\n",
      "       [ 783,  254, 4386,  337,    5,   13,  447,   14,  500,   10,   10,\n",
      "          14,  500,  517, 1076,  357,   21, 1684,   72,   45,  290,   12,\n",
      "          17,  515,   17,   25,  380,  129, 3305,    4, 2191,   26,  253,\n",
      "           5,    2,   36,   80, 4357,   25,    2,  129,  330,  505,    8,\n",
      "           2,  146,   24, 3988,   14,  500,    9,   82,    2,    5,    9,\n",
      "        1293,  224,   10,   10,    8,  401,   14, 1361,  879,   13,   28,\n",
      "           8,  401,   61, 1642, 2925,   44, 1373,   21,  591,  353,   14,\n",
      "         500, 4092,   30,  290,   12,   10,   10,   65,  790,  790,  206,\n",
      "         158,  300,   45,   15,   52,    2,  158,  692,    2,  158,  856,\n",
      "         158],\n",
      "       [4651,   18, 1462,   13,  124,  285,    5, 1462,   11,   14,   20,\n",
      "         122,    6,   52,  292,    5,   13,  774, 2626,   46,  138,  910,\n",
      "        1481,  276,   14,   20,   23,  288,   42,   23, 1856,   11, 2364,\n",
      "        5687,   33,  222,   13,  774,  110,  101, 4651,   14,    9,    6,\n",
      "        3799,   52,   20,    5,  144,   30,  110,   34,   32,    4,  362,\n",
      "          11,    4,  162, 2248,   92,   79,    8,   67,   12,    5,   13,\n",
      "         104,   36,  144,   12,  144,   33,  222,   30,  276,  145,   23,\n",
      "           4, 1308,   14,   20,  152, 1833,    6,  706,    2,   12, 1015,\n",
      "           4,  147,  155,  146,   98,  150,   14,   20,   80,   30,   23,\n",
      "         288],\n",
      "       [   6,  720,   15,   47,   77, 6573,   34, 2912,   11, 2189,    2,\n",
      "          34, 3973, 8150,   11, 3829,    5, 1620,    2,   11,    4,    2,\n",
      "          14,  720,    9,  617, 3727,   39,  998, 2445, 1346, 3862,   46,\n",
      "          51,  242,   16,   64,    6,  677,  333, 1494,   10,   10,    2,\n",
      "           9,    6,   22,   15,    4, 4198, 1103,    2,   12,   18,  148,\n",
      "         460, 2534,    5,   12,   18,    4,    2,   12,    2,    7,  968,\n",
      "           5,    2,   19,    4, 4271,    2,    7,    4,    2,    2, 7936,\n",
      "          94,  833, 1488,   47,   77,   23,   22, 1537,   37,   28, 4902,\n",
      "           5,   64, 1076, 3827,   23, 3362, 5161,   11, 5421,  450, 5122,\n",
      "         596],\n",
      "       [   4,   22, 4097,   34,   19,  883,    5,   33,    4,  130,    2,\n",
      "         912,  408, 2540,   21,   37,    9,    4,  336,   10,   10,    4,\n",
      "         912,   65,    9,    4,   55,  815, 2196,   15,    9,  343,    8,\n",
      "         353,    5,  987,    6,   65,  200, 1714,   94,    6,  394,  769,\n",
      "          50,   26,  342,  293,  621, 1328,   32,    7,   63,   26, 1916,\n",
      "          78,  690,    5, 2150, 3822,   94,   43,   35,  576,  357,   22,\n",
      "        8408,   47,   99,  111,  715,   11,  257, 4130,    5, 1545,   98,\n",
      "          11,   35,  220, 2227, 1377,   94,   24,  163,  126,   21,   94,\n",
      "         981,    8,   30, 8408,    5, 1820,   28,  224,   76,  128,   74,\n",
      "          14],\n",
      "       [  28,  196,  237,  413, 1250,   14,   31,    9, 1061,   19, 2278,\n",
      "         690,    2,    2,    5, 2764,   45,    6,  371, 9687,    7, 6247,\n",
      "           5,  184,  751,    2,  639,   12,   16, 2305,   18,   94, 3268,\n",
      "         318,  302,   63,   26,  220,    2,   11,   14,  251,    5,  559,\n",
      "           2, 1424,    7, 2873, 1381,    4,   64, 1339, 1123,  792,   63,\n",
      "        6095,    9,   94,  307, 1399,    5, 7890,  619,    8,  135,    7,\n",
      "           4,  111,  108,   93,   11,   14,  512,  171,    7,   98,  216,\n",
      "          56,    8, 3802,    2,  204, 3036,    7,    2,  220,  101,   85,\n",
      "           2, 3527,   22,    9, 1731,    8,   14,   31,  151,   45,    6,\n",
      "        3358],\n",
      "       [6984, 3739,    4, 3778,  499,    7, 1405, 8681,   10,   10,   50,\n",
      "          26,   49, 1774, 5867,   11,   14,   22,   44,    4,   64, 5340,\n",
      "          13,   70,   66,  213,   46,    9,    6,  813,    8,    4,  229,\n",
      "          11,   49, 1370,   63,   13,  104,    9,  688,  669,    8,    4,\n",
      "          96,   14,   22,    9,    6,  689,    2,  548,   50,  331,  218,\n",
      "         195,   58,    8, 2887, 3739,  803,  170,   23,   10,   10, 2185,\n",
      "          14,    9,    6, 1543,   52,   22,   13,  545,  386,  149,   14,\n",
      "          11,    2,   19,    4,   86,    5,   95,    2,   18,   89,   52,\n",
      "           4,  201,  100,   28,   77,   69,   12, 3501,  467, 3555,    5,\n",
      "        2065]], dtype=int32)]\n",
      "tf.Tensor(\n",
      "[[[-0.44986415 -0.34338984  0.30391574 ...  0.12847608  0.3016706\n",
      "    2.204504  ]\n",
      "  [-0.44986415 -0.34338984  0.30391574 ...  0.12847608  0.3016706\n",
      "    2.204504  ]\n",
      "  [-0.44986415 -0.34338984  0.30391574 ...  0.12847608  0.3016706\n",
      "    2.204504  ]\n",
      "  ...\n",
      "  [-1.7014937  -0.26296812  0.3262783  ... -0.80903304 -0.03174573\n",
      "   -2.0361803 ]\n",
      "  [-0.4601323   0.42749104 -0.6782491  ... -1.8629315  -0.3101289\n",
      "    0.02466355]\n",
      "  [ 1.5164926  -0.18718825 -0.01180497 ... -1.5124063   0.61981016\n",
      "   -0.36675566]]\n",
      "\n",
      " [[-0.4601323   0.42749104 -0.6782491  ... -1.8629315  -0.3101289\n",
      "    0.02466355]\n",
      "  [ 0.22386223  0.4047161   0.36190474 ... -0.2104402  -0.44923902\n",
      "   -0.955192  ]\n",
      "  [-0.17518763 -0.05901757  1.7206639  ... -1.4382057   0.01118685\n",
      "    0.2998545 ]\n",
      "  ...\n",
      "  [-0.6606362  -0.14657073  1.1376874  ... -0.02116318  0.13036595\n",
      "    1.1403273 ]\n",
      "  [ 1.9458584   2.0081215   0.50219774 ... -1.1569309   0.42150408\n",
      "   -0.3250542 ]\n",
      "  [ 2.803527   -0.7583768  -0.9937461  ...  0.08536241  0.09812578\n",
      "   -0.12258326]]\n",
      "\n",
      " [[ 1.9458584   2.0081215   0.50219774 ... -1.1569309   0.42150408\n",
      "   -0.3250542 ]\n",
      "  [ 0.687392   -0.47659424  0.40700385 ... -0.62979573  0.7310478\n",
      "   -0.84889144]\n",
      "  [-0.4170429   0.45117092  0.36130154 ... -2.1657245   0.49955288\n",
      "   -0.30132028]\n",
      "  ...\n",
      "  [-0.97974676  0.4373087  -0.72002786 ... -0.0269361  -3.4149399\n",
      "    0.5734125 ]\n",
      "  [ 0.07791347 -0.71169835  1.0441222  ...  0.21933842  0.36601558\n",
      "    2.2905066 ]\n",
      "  [-0.2629911  -0.437399    0.9993342  ... -0.7409936   1.4352034\n",
      "    0.4954578 ]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 1.9458584   2.0081215   0.50219774 ... -1.1569309   0.42150408\n",
      "   -0.3250542 ]\n",
      "  [-0.10000162  0.11547699 -0.6690087  ... -0.62084526  0.04284883\n",
      "    0.95760417]\n",
      "  [-0.511087   -1.3005725  -0.7159893  ... -0.8362657   0.4658468\n",
      "    1.478148  ]\n",
      "  ...\n",
      "  [ 3.0394     -1.048282    0.34139428 ...  0.04581722 -0.17242822\n",
      "   -0.36885753]\n",
      "  [ 0.2651037   1.3314097  -0.45098284 ... -0.3084603  -1.9497539\n",
      "    1.2332059 ]\n",
      "  [-1.7014937  -0.26296812  0.3262783  ... -0.80903304 -0.03174573\n",
      "   -2.0361803 ]]\n",
      "\n",
      " [[-0.1068045  -0.08869153 -0.15091795 ...  0.55616164  0.10832836\n",
      "   -0.88492525]\n",
      "  [ 0.29690534 -2.3811047   0.60569453 ...  3.2701278  -1.0341009\n",
      "   -1.2486485 ]\n",
      "  [ 0.08316775 -0.7246067  -0.66871256 ... -1.0505819  -0.3262733\n",
      "    0.9651102 ]\n",
      "  ...\n",
      "  [-0.6878202  -0.66007185 -1.1495454  ...  0.46725202 -1.3417283\n",
      "   -0.51738566]\n",
      "  [-0.4601323   0.42749104 -0.6782491  ... -1.8629315  -0.3101289\n",
      "    0.02466355]\n",
      "  [ 1.3346949  -1.8920878  -2.159126   ...  1.339287    2.2111845\n",
      "   -0.61753947]]\n",
      "\n",
      " [[ 0.226327   -2.0971386   0.03564875 ... -0.26441643 -0.07338847\n",
      "   -0.55156076]\n",
      "  [-0.8509567  -1.2840438  -2.6523538  ... -0.5496999  -0.3851758\n",
      "    0.07950227]\n",
      "  [ 1.9458584   2.0081215   0.50219774 ... -1.1569309   0.42150408\n",
      "   -0.3250542 ]\n",
      "  ...\n",
      "  [ 0.62484175 -0.06074914 -1.0573009  ...  0.02367976  1.937434\n",
      "   -1.6389067 ]\n",
      "  [ 0.1647403   1.3698899  -1.2612731  ... -2.2926867  -0.0694463\n",
      "    0.29785353]\n",
      "  [-1.4620599  -0.88781536 -1.0204583  ...  1.1703639  -1.3091828\n",
      "   -0.10128217]]], shape=(10, 100, 50), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[[-0.44986415 -0.34338984  0.30391574 ...  0.12847608  0.3016706\n",
      "    2.204504  ]\n",
      "  [-0.44986415 -0.34338984  0.30391574 ...  0.12847608  0.3016706\n",
      "    2.204504  ]\n",
      "  [-0.44986415 -0.34338984  0.30391574 ...  0.12847608  0.3016706\n",
      "    2.204504  ]\n",
      "  ...\n",
      "  [-0.44986415 -0.34338984  0.30391574 ...  0.12847608  0.3016706\n",
      "    2.204504  ]\n",
      "  [-0.44986415 -0.34338984  0.30391574 ...  0.12847608  0.3016706\n",
      "    2.204504  ]\n",
      "  [-0.44986415 -0.34338984  0.30391574 ...  0.12847608  0.3016706\n",
      "    2.204504  ]]\n",
      "\n",
      " [[-0.44986415 -0.34338984  0.30391574 ...  0.12847608  0.3016706\n",
      "    2.204504  ]\n",
      "  [-0.44986415 -0.34338984  0.30391574 ...  0.12847608  0.3016706\n",
      "    2.204504  ]\n",
      "  [-0.44986415 -0.34338984  0.30391574 ...  0.12847608  0.3016706\n",
      "    2.204504  ]\n",
      "  ...\n",
      "  [-0.44986415 -0.34338984  0.30391574 ...  0.12847608  0.3016706\n",
      "    2.204504  ]\n",
      "  [-0.44986415 -0.34338984  0.30391574 ...  0.12847608  0.3016706\n",
      "    2.204504  ]\n",
      "  [-0.44986415 -0.34338984  0.30391574 ...  0.12847608  0.3016706\n",
      "    2.204504  ]]\n",
      "\n",
      " [[-0.44986415 -0.34338984  0.30391574 ...  0.12847608  0.3016706\n",
      "    2.204504  ]\n",
      "  [-0.44986415 -0.34338984  0.30391574 ...  0.12847608  0.3016706\n",
      "    2.204504  ]\n",
      "  [-0.44986415 -0.34338984  0.30391574 ...  0.12847608  0.3016706\n",
      "    2.204504  ]\n",
      "  ...\n",
      "  [-0.44986415 -0.34338984  0.30391574 ...  0.12847608  0.3016706\n",
      "    2.204504  ]\n",
      "  [-0.44986415 -0.34338984  0.30391574 ...  0.12847608  0.3016706\n",
      "    2.204504  ]\n",
      "  [-0.44986415 -0.34338984  0.30391574 ...  0.12847608  0.3016706\n",
      "    2.204504  ]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[-0.44986415 -0.34338984  0.30391574 ...  0.12847608  0.3016706\n",
      "    2.204504  ]\n",
      "  [-0.44986415 -0.34338984  0.30391574 ...  0.12847608  0.3016706\n",
      "    2.204504  ]\n",
      "  [-0.44986415 -0.34338984  0.30391574 ...  0.12847608  0.3016706\n",
      "    2.204504  ]\n",
      "  ...\n",
      "  [-0.44986415 -0.34338984  0.30391574 ...  0.12847608  0.3016706\n",
      "    2.204504  ]\n",
      "  [-0.44986415 -0.34338984  0.30391574 ...  0.12847608  0.3016706\n",
      "    2.204504  ]\n",
      "  [-0.44986415 -0.34338984  0.30391574 ...  0.12847608  0.3016706\n",
      "    2.204504  ]]\n",
      "\n",
      " [[-0.44986415 -0.34338984  0.30391574 ...  0.12847608  0.3016706\n",
      "    2.204504  ]\n",
      "  [-0.44986415 -0.34338984  0.30391574 ...  0.12847608  0.3016706\n",
      "    2.204504  ]\n",
      "  [-0.44986415 -0.34338984  0.30391574 ...  0.12847608  0.3016706\n",
      "    2.204504  ]\n",
      "  ...\n",
      "  [-0.44986415 -0.34338984  0.30391574 ...  0.12847608  0.3016706\n",
      "    2.204504  ]\n",
      "  [-0.44986415 -0.34338984  0.30391574 ...  0.12847608  0.3016706\n",
      "    2.204504  ]\n",
      "  [-0.44986415 -0.34338984  0.30391574 ...  0.12847608  0.3016706\n",
      "    2.204504  ]]\n",
      "\n",
      " [[-0.44986415 -0.34338984  0.30391574 ...  0.12847608  0.3016706\n",
      "    2.204504  ]\n",
      "  [-0.44986415 -0.34338984  0.30391574 ...  0.12847608  0.3016706\n",
      "    2.204504  ]\n",
      "  [-0.44986415 -0.34338984  0.30391574 ...  0.12847608  0.3016706\n",
      "    2.204504  ]\n",
      "  ...\n",
      "  [-0.44986415 -0.34338984  0.30391574 ...  0.12847608  0.3016706\n",
      "    2.204504  ]\n",
      "  [-0.44986415 -0.34338984  0.30391574 ...  0.12847608  0.3016706\n",
      "    2.204504  ]\n",
      "  [-0.44986415 -0.34338984  0.30391574 ...  0.12847608  0.3016706\n",
      "    2.204504  ]]], shape=(10, 100, 50), dtype=float32)\n",
      "model output_shape (None, 2)\n",
      "model output_shape (None, 2)\n",
      "Attributions shape: (10, 100, 50)\n"
     ]
    }
   ],
   "source": [
    "x_test_sample = x_test_sample.numpy()\n",
    "predictions = model(x_test_sample).numpy().argmax(axis=1)\n",
    "explanation = ig.explain(x_test_sample, \n",
    "                         baselines=reference_indices, \n",
    "                         target=1)\n",
    "# Get attributions values from the explanation object\n",
    "attributions_tf = explanation.attributions[0]\n",
    "print('Attributions shape:', attributions_tf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(attributions_tf, attributions_pt, rtol=1e-03)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, TYPE_CHECKING, Union, List, Tuple, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _run_forward(model: Union[tf.keras.models.Model, 'keras.models.Model'],\n",
    "                 x: Union[List[tf.Tensor], List[np.ndarray]],\n",
    "                 target: Union[None, tf.Tensor, np.ndarray, list]) -> tf.Tensor:\n",
    "    \"\"\"\n",
    "    Returns the output of the model. If the target is not `None`, only the output for the selected target is returned.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model\n",
    "        Tensorflow or keras model.\n",
    "    x\n",
    "        Input data point.\n",
    "    target\n",
    "        Target for which the gradients are calculated for classification models.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        Model output or model output after target selection for classification models.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def _select_target(ps, ts):\n",
    "        if ts is not None:\n",
    "            if isinstance(ps, tf.Tensor):\n",
    "                ps = tf.linalg.diag_part(tf.gather(ps, ts, axis=1))\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "        else:\n",
    "            raise ValueError(\"target cannot be `None` if `model` output dimensions > 1\")\n",
    "        return ps\n",
    "\n",
    "    preds = model(x)\n",
    "    if len(model.output_shape) > 1 and model.output_shape[-1] > 1:\n",
    "        preds = _select_target(preds, target)\n",
    "\n",
    "    return preds\n",
    "\n",
    "def _gradients_input(model: Union[tf.keras.models.Model, 'keras.models.Model'],\n",
    "                     x: List[tf.Tensor],\n",
    "                     target: Union[None, tf.Tensor]) -> List[tf.Tensor]:\n",
    "    \"\"\"\n",
    "    Calculates the gradients of the target class output (or the output if the output dimension is equal to 1)\n",
    "    with respect to each input feature.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model\n",
    "        Tensorflow or keras model.\n",
    "    x\n",
    "        Input data point.\n",
    "    target\n",
    "        Target for which the gradients are calculated if the output dimension is higher than 1.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        Gradients for each input feature.\n",
    "\n",
    "    \"\"\"\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(x)\n",
    "        preds = _run_forward(model, x, target)\n",
    "\n",
    "    grads = tape.gradient(preds, x)\n",
    "\n",
    "    return grads\n",
    "\n",
    "\n",
    "def _gradients_layer(model: Union[tf.keras.models.Model, 'keras.models.Model'],\n",
    "                     layer: Union[tf.keras.layers.Layer, 'keras.layers.Layer'],\n",
    "                     orig_call: Callable,\n",
    "                     x: List[tf.Tensor],\n",
    "                     target: Union[None, tf.Tensor]) -> tf.Tensor:\n",
    "    \"\"\"\n",
    "    Calculates the gradients of the target class output (or the output if the output dimension is equal to 1)\n",
    "    with respect to each element of `layer`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model\n",
    "        Tensorflow or keras model.\n",
    "    layer\n",
    "        Layer of the model with respect to which the gradients are calculated.\n",
    "    orig_call\n",
    "        Original `call` method of the layer. This is necessary since the call method is modified by the function\n",
    "        in order to make the layer output visible to the GradientTape.\n",
    "    x\n",
    "        Input data point.\n",
    "    target\n",
    "        Target for which the gradients are calculated if the output dimension is higher than 1.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        Gradients for each element of layer.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def watch_layer(layer, tape):\n",
    "        \"\"\"\n",
    "        Make an intermediate hidden `layer` watchable by the `tape`.\n",
    "        After calling this function, you can obtain the gradient with\n",
    "        respect to the output of the `layer` by calling:\n",
    "\n",
    "            grads = tape.gradient(..., layer.result)\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        def decorator(func):\n",
    "            def wrapper(*args, **kwargs):\n",
    "                # Store the result of `layer.call` internally.\n",
    "                layer.result = func(*args, **kwargs)\n",
    "                # From this point onwards, watch this tensor.\n",
    "                tape.watch(layer.result)\n",
    "                # Return the result to continue with the forward pass.\n",
    "                return layer.result\n",
    "\n",
    "            return wrapper\n",
    "\n",
    "        layer.call = decorator(layer.call)\n",
    "        return layer\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        watch_layer(layer, tape)\n",
    "        preds = _run_forward(model, x, target)\n",
    "\n",
    "    grads = tape.gradient(preds, layer.result)\n",
    "\n",
    "    delattr(layer, 'result')\n",
    "    layer.call = orig_call\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_call = layer.call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = [1 for _ in range(len(x_test_sample))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "grads_tf = _gradients_layer(model, layer, orig_call, x_test_sample, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "grads_tf = grads_tf.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from captum._utils.gradient import compute_layer_gradients_and_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_sample_pt = torch.from_numpy(x_test_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "grads_pt, _ = compute_layer_gradients_and_eval(net,\n",
    "    net.emb,\n",
    "    x_test_sample_pt,\n",
    "    target)\n",
    "grads_pt = grads_pt[0].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(grads_pt, grads_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:intgrads] *",
   "language": "python",
   "name": "conda-env-intgrads-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
