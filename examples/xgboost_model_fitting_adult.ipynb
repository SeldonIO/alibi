{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting an gradient boosted tree model to the Adult dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example example introduces the reader to the fitting and evaluation of gradient boosted tree models. We consider a binary classification task, where a classification into two income brackets  (less than \\\\$50,000 and greater than \\\\$50,000), given attributes such as capital gains and losses, marital status, age, occupation, etc. ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "\n",
    "from alibi.datasets import fetch_adult\n",
    "from copy import deepcopy\n",
    "from functools import partial\n",
    "from itertools import chain, product\n",
    "from scipy.special import expit\n",
    "invlogit=expit\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation\n",
    "### Load and split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `fetch_adult` function returns a `Bunch` object containing the features, the targets, the feature names and a mapping of categorical variables to numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "adult = fetch_adult()\n",
    "adult.keys()\n",
    "\n",
    "data = adult.data\n",
    "target = adult.target\n",
    "target_names = adult.target_names\n",
    "feature_names = adult.feature_names\n",
    "category_map = adult.category_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that for your own datasets you can use the utility function `gen_category_map` imported from `alibi.utils.data`  to create the category map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "data_perm = np.random.permutation(np.c_[data, target])\n",
    "data = data_perm[:,:-1]\n",
    "target = data_perm[:,-1]\n",
    "\n",
    "idx = 30000\n",
    "X_train,y_train = data[:idx,:], target[:idx]\n",
    "X_test, y_test = data[idx+1:,:], target[idx+1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create feature transformation pipeline and preprocess data\n",
    "\n",
    "Unlike in a previous [example](kernel_shap_adult_lr.ipynb), the categorical variables are not encoded. For linear models such as logistic regression, using an encoding of the variable that assigns a unique integer to a category will affect the coefficient estimates as the model will learn patterns based on the ordering of the input, which is incorrect. In contrast, by encoding the into a sequence of binary variables, the model can learn which encoded dimensions are relevant for predicting a given target but cannot represent non-linear relations between the categories and targets. \n",
    "\n",
    "On the other hand, decision trees can naturally handle both data types simultaneously; a categorical feature can be used for splitting a node multiple times. So, hypothetically speaking, if the categorical variable `var` has `4` levels, encoded `0-3` and level `2` correlates well with a particular outcome, then a decision path could contain the splits `var < 3` and `var > 1` if this pattern generalises in the data and thus splitting according to these criteria reduce the splits' impurity. \n",
    "\n",
    "In general, we note that for a categorical variable with $q$ levels there are $2^{q-1}-1$ possible partitions into two groups, and for large $q$ finding an optimal split is intractable. However, for binary classification problems an optimal split can be found efficiently (see references in [[1]](#References)). As $q$ increases, the number of potential splits to choose from increases, so it is more likely that a split that fits the data is found. For large $q$ this can lead to overfitting, so variables with large number of categories can potentially harm model performance.\n",
    "\n",
    "The interested reader is referred to consult these blog posts ([first](https://roamanalytics.com/2016/10/28/are-categorical-variables-getting-lost-in-your-random-forests/), [second](https://towardsdatascience.com/one-hot-encoding-is-making-your-tree-based-ensembles-worse-heres-why-d64b282b5769)), which demonstrate of the pitfalls of encoding categorical data as one-hot when using tree-based models. `sklearn` expects that the categorical data is encoded, and this approach should be followed when working with this library.\n",
    "\n",
    "<a id='source_4'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model optimisation\n",
    "<a id='optimisation'></a>\n",
    "\n",
    "`xgboost` wraps arrays using  `DMatrix` objects, optimised for both memory efficiency and training speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrap(arr):\n",
    "    return np.ascontiguousarray(arr)\n",
    "\n",
    "dtrain = xgb.DMatrix(\n",
    "    wrap(X_train), \n",
    "    label=wrap(y_train), \n",
    "    feature_names=feature_names, \n",
    ")\n",
    "\n",
    "dtest = xgb.DMatrix(wrap(X_test), label=wrap(y_test), feature_names=feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`xgboost` defines three classes of parameters that need to be configured in order to train and/or optimise a model:\n",
    "* general parameters: high level settings such as the type of boosting model\n",
    "* learning parameters: these are parameters that control the boosting process (model hyperparameters)\n",
    "* learning task parameters: define the optimisation objective and the metric on which the validation performance is measured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_params = {}\n",
    "booster_params = {}\n",
    "general_params = {}\n",
    "params = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a binary classification problem, optimised with binary cross-entropy as an objective, defined as:\n",
    "<a id='f_1'></a>\n",
    "\n",
    "$$\n",
    "J (y_i, \\hat{y}_i) = -\\frac{1}{N} \\sum_{i=1}^N [y_i \\log(1 + e^{- \\hat{y}_i}) + (1 - y_i) \\log (1 + e^{\\hat{y}_i})]\n",
    "$$\n",
    "\n",
    "where $y_i$ is the true label for the $i$th observation and $\\hat{y}_i$ is the decision score (logit) <sup>[(1)](#Footnotes) </sup> of the positive class (whose members' income exceeds \\$50,000). Setting the objective to `binary:logitraw` means the call to the `predict` method will output $\\hat{y}_i$. \n",
    "\n",
    "<a id='f_1'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_params.update({\n",
    "    'objective':'binary:logitraw',\n",
    "    'seed': 42,\n",
    "    'eval_metric': ['auc', 'logloss'] # metrics computed for specified dataset\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [AUC](https://en.wikipedia.org/wiki/Receiver_operating_characteristic#Area_under_the_curve) will be used as a target for early stopping during hyperparameter optimisation . Using this metric as opposed to, e.g., accuracy helps deal with the imbalanced data since this metric balances the true positive rate and the false positive rate. However, it should be noted that AUC is an _aggregate_ performance measure since it is derived by matching predicted labels with ground truths across models with different output thresholds. In practice, however, only one such model is selected. Thus, a higher AUC just reflects that on average, the ensemble performs better. However, whether the classifier selected according to this metric is optimal depends on the threshold chosen for converting the predicted probabilities to class labels.\n",
    "\n",
    "Additionally, the weights of the positive are scaled to reflect the class imbalance. A common setting is to scale the positive class by the ratio of the negative to positive examples (approximately 3 for this dataset). Since this is a heuristic approach, this parameter will be cross-validated.\n",
    "\n",
    "\n",
    "The first parameters optimised are:\n",
    "* `max_depth`: the maximum depth of any tree added to the ensemble. Deeper trees are more accurate (on training data) since they represent more specific rules\n",
    "* `min_child_weight`: child nodes are required to have a total weight above this threshold for a split to occur. For a node $L$, this weight is computed according to \n",
    "$$\n",
    "H_L = \\sum_{i}w_i  \\frac{\\partial^2 J (y_i, \\hat{y}_{i,{t-1}})}{\\partial {\\hat{y}_{i,t-1}}^2}\n",
    "$$\n",
    "where the summation of Hessians is over all examples $i$ split at the node, and the subscript $t-1$ indicates that the derivative is with respect to the output evaluated at the previous round in the boosting [process](https://xgboost.readthedocs.io/en/latest/tutorials/model.html). In this example, the weight $w_i$ depends on the class and is controlled through the `scale_pos_weight` argument. The second derivative above is given by\n",
    "\n",
    "$$\n",
    "\\frac{2 e^{\\hat{y}_i}}{{(1 + e^{\\hat{y}_i})}^2},\n",
    "$$\n",
    "whose variation is depicted in Figure 1.\n",
    "\n",
    "![hessian](hess.png)\n",
    "Figure 1: Hessian of the loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 1 shows that when the classifier assigns a high positive or a low negative score, the contribution of data point $i$ to the child weight is very small. Therefore, setting a very small value for `min_child_weight` parameter can result in overfitting since the splitting process will make splits in order to ensure the instances in a leaf are correctly classified at the expense of finding more parsimonious rules that generalise well.\n",
    "* `scale_pos_weight`: a scaling factor applied to the positive class to deal with class imbalance\n",
    "* `gamma`: is a parameter that controls the minimum gain that has to be attained in order for a split to be made\n",
    "\n",
    "To understand `gamma`, recall that the _gain_ of making a particular split is defined as function of the _structure scores_ of the left (L) and right (R) child nodes and the structure score of the parent as \n",
    "\n",
    "$$\n",
    "gain = \\frac{1}{2}\\left[\\frac{G_L^2}{H_L + \\lambda} + \\frac{G_R^2}{H_R + \\lambda} - \\frac{{(G_L+G_R)}^2}{H_L + H_R + \\lambda}  \\right] - \\gamma\n",
    "$$\n",
    "\n",
    "where $\\lambda$ is a regularisation hyperparameter shrinking the model output, $H_L$ is defined above and $G_L$ is given by\n",
    "\n",
    "$$\n",
    "G_L = \\sum_{i}w_i  \\frac{\\partial J (y_i, \\hat{y}_{i,{t-1}})}{\\partial {\\hat{y}_{i,t-1}}}\n",
    "$$\n",
    "\n",
    "and $i$ sums over the points that flow through the node $L$. Note that these structure scores represent minimisers of the objective (which is simply a quadratic in the leaf value). To make a split, the gain should exceed $\\gamma$.\n",
    "\n",
    "\n",
    "The _learning rate_ (`eta`) is fixed. This parameter is the fraction of the output score a member of the ensemble contributes to the decision score. Lower values yield larger ensembles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_params(dtrain, base_params,  param_dict, maximise=True, prev_optimum=None, **kwargs):\n",
    "    \"\"\"\n",
    "        Given a training set `dtrain`, a dictionary of parameters to be optimised `param_dict` and \n",
    "        all the other learning and booster parameters (`base_param`), this function runs an \n",
    "        exhaustive grid search over the tuning parameters.\n",
    "        \n",
    "        NB: Specifying `prev_optimum` allows one to tune parameters in stages. maximise should indicate\n",
    "        if the evaluation metric should be maximised during CV.\n",
    "    \"\"\"\n",
    "\n",
    "    def _statistic(maximise, argument=False):\n",
    "        if maximise:\n",
    "            if argument:\n",
    "                return np.argmax\n",
    "            return np.max\n",
    "        if argument:\n",
    "            return np.argmin\n",
    "        return np.min\n",
    "    \n",
    "    \n",
    "    def _compare(optimum, val, maximise=True):\n",
    "        \n",
    "        eps=1e-4\n",
    "        \n",
    "        if maximise:\n",
    "            if val > optimum + eps:\n",
    "                return True\n",
    "            return False\n",
    "        if val < optimum - eps:\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    statistic = partial(_statistic, maximise)\n",
    "    compare = partial(_compare, maximise=maximise)\n",
    "    metrics = kwargs.get(\"metrics\")\n",
    "    if isinstance(metrics, list):\n",
    "        opt_metric = metrics[-1]\n",
    "    else:\n",
    "        opt_metric = metrics\n",
    "\n",
    "    print(f\"CV with params: {list(param_dict.keys())}\")\n",
    "    print(f\"Tracked metrics: {metrics}\")\n",
    "    print(f\"Cross-validating on: {opt_metric}\")\n",
    "    \n",
    "    if prev_optimum:\n",
    "        optimum = prev_optimum\n",
    "    else:    \n",
    "        optimum = -float(\"Inf\") if maximise else float(\"Inf\")\n",
    "    \n",
    "    params = deepcopy(base_params)\n",
    "    pars, pars_val = list(param_dict.keys()), list(param_dict.values())\n",
    "    combinations = list(product(*pars_val))\n",
    "    best_combination = {}\n",
    "    \n",
    "    # run grid search\n",
    "    for combination in tqdm(combinations):\n",
    "        for p_name, p_val in zip(pars, combination):\n",
    "            params[p_name] = p_val\n",
    "        cv_results = xgb.cv(\n",
    "            params,\n",
    "            dtrain,\n",
    "            **kwargs,\n",
    "        )\n",
    "        mean = statistic()(cv_results[f'test-{opt_metric}-mean'])\n",
    "        boost_rounds = statistic(argument=True)(cv_results[f'test-{opt_metric}-mean'])    \n",
    "        improved = compare(optimum, mean)\n",
    "        if improved:\n",
    "            optimum = mean\n",
    "            for name, val in zip(pars, combination):\n",
    "                best_combination[name]=val\n",
    "            print(f\"{opt_metric} mean value: {mean} at {boost_rounds} rounds\")\n",
    "            msg = 'Best params:' + '\\n{}: {}'*len(pars)\n",
    "            print(msg.format(*list(chain(*best_combination.items()))))\n",
    "        \n",
    "    return optimum, best_combination, boost_rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "booster_params.update({'eta': 0.1})\n",
    "\n",
    "tuning_params={\n",
    "        'scale_pos_weight': [2, 3, 4, 5],\n",
    "        'min_child_weight': [0.1, 0.5, 1.0, 2.0, 5.0],\n",
    "        'max_depth': [3, 4, 5],\n",
    "        'gamma': [0.01, 0.05, 0.08, 0.1, 0.2]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All parameters apart from the ones tuned are included in `params`. The cross-validation process is controlled through `cv_opts`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "params.update(general_params)\n",
    "params.update(learning_params)\n",
    "params.update(booster_params)\n",
    "\n",
    "cv_opts = {\n",
    "    'num_boost_round': 1000,\n",
    "    'nfold': 5, \n",
    "    'stratified': True,\n",
    "    'metrics': ['logloss', 'aucpr', 'auc'],  # can alternatively perform early stopping on log-loss or aucpr\n",
    "    'early_stopping_rounds': 20,\n",
    "    'seed': 42,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimise `scale_pos_weight`, `min_child_weight`, `max_depth` and `gamma`. Note that this section is **long running** since it conducts an extensive grid search. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimum, best_params, boost_rounds = tune_params(dtrain, \n",
    "                                                 params, \n",
    "                                                 tuning_params, \n",
    "                                                 maximise=True, \n",
    "                                                 **cv_opts\n",
    "                                                )\n",
    "\n",
    "if best_params:\n",
    "    params.update(best_params)\n",
    "    params.update({'boost_rounds': boost_rounds})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further optimisation is possible by adjusting the following parameters:\n",
    "* `subsample`: this is the ratio of the total training examples that will be used for training during each boosting round\n",
    "* `colsamplebytree`: this is the ratio of the features used to fit an ensemble member during a boosting round\n",
    "\n",
    "Training on uniformly chosen data subsamples with uniformly chosen subsets of features promotes noise robustness. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_params = {\n",
    "    'subsample': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "    'colsamplebytree': [0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "}\n",
    "\n",
    "optimum, best_params, boost_rounds = tune_params(dtrain, \n",
    "                                                 params, \n",
    "                                                 tuning_params, \n",
    "                                                 maximise=True, \n",
    "                                                 prev_optimum=optimum,\n",
    "                                                 **cv_opts\n",
    "                                                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "None of the stated configuration resulted in an improvement of the AUC, which could be a consequence of the fact that:\n",
    "* the parameters selected in the previous round provide strong model regularisation; in particular, the maximum tree depth for any ensemble member is 3, which means only a subset of features are used anyway to perform the splits in any given tree. Further subsampling may thus not be effective since the subsampling is already implicit in the chosen tree structure\n",
    "* the AUC is insensitive to small model changes since it measures how the proportion of false positives changes as the number of false negatives changes across a range of models. The confidence of the models does not feature in this measure (since a highly confident classifier and one that predicts probabilities near the decision threshold will have identical AUC)\n",
    "\n",
    "if best_params:\n",
    "    params.update(best_params)\n",
    "    params.update({'boost_rounds': boost_rounds})\n",
    "\n",
    "params\n",
    "\n",
    "To prevent overfitting, a regulariser $\\Omega(f_t)$ with the form \n",
    "\n",
    "$$\n",
    "\\Omega(f_t) = \\gamma T + \\frac{\\lambda}{2} \\sum_{j=1}^T s_{j,t}^2\n",
    "$$\n",
    "\n",
    "is added to the objective function at every boosting round $t$. Above $T$ is the total number of leaves and $s_{j,t}$ is the score of the $j$th leaf at round $t$. For the binary logistic objective, a higher $\\lambda$ penalises confident predictions (shinks the scores). \n",
    "\n",
    "By default $\\lambda = 1$. Since subsampling data and features did not improve the performance, we explore with relaxing regularisation in order to adjust the model regularisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_params = {\n",
    "    'lambda': [0.01, 0.1, 0.5, 0.9, 0.95, 1, 2, 5, 10]\n",
    "}\n",
    "\n",
    "optimum, best_params, boost_rounds = tune_params(dtrain, \n",
    "                                                 params, \n",
    "                                                 tuning_params, \n",
    "                                                 maximise=True, \n",
    "                                                 prev_optimum=optimum,\n",
    "                                                 **cv_opts)\n",
    "\n",
    "if best_params:\n",
    "    params.update(best_params)\n",
    "    params.update({'boost_rounds': boost_rounds})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training\n",
    "<a id='training'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model will now be trained with the following parameters (skip the `param` update if you ran the optimisation section):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_params = {\n",
    "    'objective':'binary:logitraw',\n",
    "    'seed': 42,\n",
    "    'eval_metric': ['auc', 'logloss'] # metrics computed for specified dataset\n",
    "}\n",
    "\n",
    "\n",
    "params = { \n",
    "    'scale_pos_weight': 2,\n",
    "    'min_child_weight': 0.1,\n",
    "    'max_depth': 3,\n",
    "    'gamma': 0.01, \n",
    "    'boost_rounds': 541,\n",
    "}\n",
    "\n",
    "params.update(learning_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'boost_rounds' in params:\n",
    "    boost_rounds = params.pop('boost_rounds')\n",
    "\n",
    "model = xgb.train(\n",
    "    params,\n",
    "    dtrain,\n",
    "    num_boost_round=boost_rounds,\n",
    "    evals=[(dtrain, \"Train\"), (dtest, \"Test\")],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_model('adult_xgb.mdl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model assessment\n",
    "\n",
    "The confusion matrix is used to quanity the model performance below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_conf_matrix(y_test, y_pred, class_names):\n",
    "    \"\"\"\n",
    "    Plots confusion matrix. Taken from:\n",
    "    http://queirozf.com/entries/visualizing-machine-learning-models-examples-with-scikit-learn-and-matplotlib\n",
    "    \"\"\"\n",
    "    \n",
    "    matrix = confusion_matrix(y_test,y_pred)\n",
    "\n",
    "\n",
    "    # place labels at the top\n",
    "    plt.gca().xaxis.tick_top()\n",
    "    plt.gca().xaxis.set_label_position('top')\n",
    "\n",
    "    # plot the matrix per se\n",
    "    plt.imshow(matrix, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "\n",
    "    # plot colorbar to the right\n",
    "    plt.colorbar()\n",
    "\n",
    "    fmt = 'd'\n",
    "\n",
    "    # write the number of predictions in each bucket\n",
    "    thresh = matrix.max() / 2.\n",
    "    for i, j in product(range(matrix.shape[0]), range(matrix.shape[1])):\n",
    "\n",
    "        # if background is dark, use a white number, and vice-versa\n",
    "        plt.text(j, i, format(matrix[i, j], fmt),\n",
    "             horizontalalignment=\"center\",\n",
    "             color=\"white\" if matrix[i, j] > thresh else \"black\")\n",
    "\n",
    "    tick_marks = np.arange(len(class_names))\n",
    "    plt.xticks(tick_marks, class_names, rotation=45)\n",
    "    plt.yticks(tick_marks, class_names)\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label',size=14)\n",
    "    plt.xlabel('Predicted label',size=14)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def predict(xgb_model, dataset, proba=False, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Predicts labels given a xgboost model that outputs raw logits. \n",
    "    \"\"\"\n",
    "    \n",
    "    y_pred = model.predict(dataset)  # raw logits are predicted\n",
    "    y_pred_proba = invlogit(y_pred) \n",
    "    if proba:\n",
    "        return y_pred_proba\n",
    "    y_pred_class = np.zeros_like(y_pred)\n",
    "    y_pred_class[y_pred_proba >= threshold] = 1  # assign a label \n",
    "    \n",
    "    return y_pred_class\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy:  87.75  %.\n",
      "Test  accuracy:  86.6797%.\n"
     ]
    }
   ],
   "source": [
    "y_pred_train = predict(model, dtrain)\n",
    "y_pred_test = predict(model, dtest)\n",
    "\n",
    "plot_conf_matrix(y_test, y_pred_test, target_names)\n",
    "\n",
    "print(f'Train accuracy:  {round(100*accuracy_score(y_train, y_pred_train), 4)}  %.')\n",
    "print(f'Test  accuracy:  {round(100*accuracy_score(y_test, y_pred_test), 4)}%.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The small difference between the testing and training error supports the hypothesis that optimising the model hyperparameters governing model complexity early on using the AUC as a metric induced a strong regularisation. In this case, the regularisation was effective in safeguarding the model performance during testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Footnotes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Footnotes'></a>\n",
    "\n",
    "[(1)](#f_1): One can derive the stated formula by noting that the probability of the positive class is $p_i = 1/( 1 + \\exp^{-\\hat{y}_i})$ and taking its logarithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='References'></a>\n",
    "\n",
    "\n",
    "\n",
    "[[1]](#source_4) Hastie, T., Tibshirani, R. and Friedman, J., 2009. The elements of statistical learning: data mining, inference, and prediction, p. 310, Springer Science & Business Media.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
