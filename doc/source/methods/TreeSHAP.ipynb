{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[source]](../api/alibi.explainers.kernel_shap.rst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree SHAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tree SHAP (**SH**apley **A**dditive ex**P**lanations) algorithm is based on the paper [From local explanations to global understanding with explainable AI for trees](https://www.nature.com/articles/s42256-019-0138-9) by Lundberg et al. and builds on the open source  [shap library](https://github.com/slundberg/shap) from the paper's first author."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How it works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Recall that, for a model $f$, the Kernel SHAP algorithm [[1]](#References) explains a certain outcome with respect to a chosen reference (or an expected value) by estimating the shap values of each feature $i \\in F = \\{1, ..., M\\} $, as follows:\n",
    "\n",
    "- enumerate all subsets $S$ of the set $F   \\setminus \\{i\\}$ \n",
    "- for each $S \\subseteq F \\setminus \\{i\\}$, compute the contribution of feature $i$ as $C(i|S) = f(S \\cup \\{i\\}) - f(S)$\n",
    "- compute the shap value according to\n",
    "$$\n",
    "\\phi_i := \\frac{1}{n} \\sum \\limits_{{S \\subseteq F \\setminus \\{i\\}}} \\frac{1}{ n - 1 \\choose |S|} C(i|S).         \n",
    "\\tag{1}\n",
    "$$\n",
    "\n",
    "\n",
    "Since most models do not accept arbitrary patterns of missing values at inference time, $f(S)$ is approximated using _interventional conditional expectations_ [[2]](#References). For Kernel SHAP, these expectations are estimated by replacing the _missing values_ ($\\bar{S}$) with values from a _background dataset_ and taking an expectation over the model output. In the context of tree models, this approximation can be performed by: \n",
    "- traversing the tree with the example to be explained (sometimes referred to as a _foreground sample_) and a sample from the background dataset. This is algorithm is known as interventional feature perturbation\n",
    "- without a background dataset, by exploiting the tree coverage information (i.e., information about how the training data was partitioned by the tree) along with the feature dependencies encoded in the tree paths\n",
    "<a id='source_1'></a>\n",
    "\n",
    "The following sections detail how these methods work and how, unlike Kernel SHAP, compute the exact shap values in polynomial time. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Path dependent feature perturbation\n",
    "<a id='path_dependent'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A way to compute $f(S)$ given an instance $x$ and a set of missing features $\\bar{S}$ is to recursively follow the decision path through the tree and:\n",
    "- return the node value if a split on a feature $i \\in S$ is performed\n",
    "- take a weighted average of the values returned by children if $i \\in \\bar{S}$, where the weighing factor is equal to the proportion of training examples flowing down each branch. This proportion is a property of each node, sometimes referred to as _weight_ or _cover_. \n",
    "\n",
    "Therefore, in the path-dependent perturbation method, we compute the conditional expectations with respect to the training data distribution by weighting the leaf values according to the proportion of the training examples that flow to that leaf.\n",
    "\n",
    "To avoid repeating the above recursion $M2^M$ times, one first notices that for a single decision tree, applying a perturbation would result in the sample ending up in a different leaf. Therefore, following each path from the root to a leaf in the tree is equivalent to perturbing subsets of features of varying cardinalities. Consequently, each leaf will contain a certain proportion of all possible subsets $S \\subseteq F$. Therefore, to compute the Shap values, the following quantities are computed at each leaf, *for every feature $i$ on the path leading to that leaf*:\n",
    "- the proportion of subsets $S$ at the leaf that contain $i$ and the proportion of subsets $S$ that do not contain $i$\n",
    "- for each cardinality, the proportion of the sets of that cardinality contained at the leaf. Tracking each cardinality  as opposed to a single count of subsets falling into a given leaf is necessary since it allows to apply the weighting factor in equation (1), which depends on the subset size, $|S|$.\n",
    "\n",
    "Using the above quantities, one can compute the _contribution_ of each leaf to the Shapley value of every feature. This algorithm has complexity $O(TLD^2)$ for an ensamble of trees where $L$ is the number of leaves, $T$ the number of trees in the ensamble and $D$ the maximum tree depth. If the tree is balanced, then $D=\\log L$ and the complexity of our algorithm is $O(TL\\log^2L)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\phi_i := \\sum \\limits_{j=1}^L \\sum \\limits_{P \\in {S_t}} \\frac {w(|P|, j)}{ M_j {M_j \\choose |P|}} \\left[f(P \\cup \\{i\\}) - f(P)\\right]\n",
    "\\tag{2}\n",
    "$$\n",
    "\n",
    "where $S_t$ is the set of present feature subsets at leaf $j$, $M_j$ is the length of the path and $w(|P|, j)$ is the proportion of all subsets of cardinality $P$ at leaf $j$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Include discussion about zero and one fractions and cover\n",
    "- Include discussion about cover/Hessian \n",
    "- Make sure above is actually correct, add missing stuff to the equation and put it in context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that although a background dataset is not provided, the expected values is computed using the node cover information, stored at each node. The computation proceeds recursively, starting at the root. The contribution of a note to the expected value of the tree is a function of the expected values of the children and is computed as follows:\n",
    "\n",
    "$$\n",
    "c_j = \\frac{c_{r(j)}r_{r(j)} + c_{l(j)}r_{l(j)}}{r_j}\n",
    "$$\n",
    "\n",
    "where $j$ denotes the node index, $c_j$ denotes the node expected value, $r_j$ is the cover of the $j$th node and $r(j)$ and $l(j)$ represent the indices of the right and left children, respectively. The expected value used by the tree is simply $c_{root}$. Note that for tree ensamles, the expected values of the ensamble members is weighted according to the tree weight and the weighted expected values of all trees are summed to obtain a single value.\n",
    "\n",
    "The cover depends on the objective function and the model chosen. For example, in a gradient boosted tree trained with squared loss objective, $r_j$ is simply the number of training examples flowing through $j$. For an arbitrary objective, this is the sum of the hessian of the loss function evaluated at each point flowing through $j$, as explained [here](../../../examples/tree_shap_adult_xgb.ipynb#optimisation). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interventional feature perturbation\n",
    "<a id='interventional'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Describe interventional perturbation concept and why it is a good idea - try and give intuition\n",
    "- Summary of what the difference to the other algorithm is - aka how we estimate the shap values using this method\n",
    "- Foreground sample and background sample concept\n",
    "- State complexity\n",
    "- Applying the interventional feature perturbation algorithm is equivalent to making an independence assumption between the set of features corresponding to the nodes where the foreground sample and the background samples are split in the same subtree and the set of features corresponding to nodes where \n",
    "- Explaining loss functions\n",
    "- Other can only explain output, we can explain a nonlinear transformation on this one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Explaining loss functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One advantage of the interventional approach is that it allows to approximately transform the shap values to account for nonlinear transformation of outputs, such as the loss function. Recall that given $\\phi_i, ..., \\phi_M$ the local accuracy property gurantees that given $\\phi_0 = \\mathbb{E}[f(x)]$\n",
    "\n",
    "$$\n",
    "f(x) = \\phi_0 + \\sum \\limits_{i=1}^M \\phi_i.\n",
    "\\tag{3}\n",
    "$$\n",
    "\n",
    "Hence, in order to account for the effect of the nonlinear transformation $h$, one has to find the functions $g_0, ..., g_M$ such that\n",
    "\n",
    "$$\n",
    "h(f(x)) = g(\\phi_0) + \\sum \\limits_{i=1}^M g_i(\\phi_i)\n",
    "\\tag{4}\n",
    "$$\n",
    "\n",
    "For simplicity, let $y=h(x)$. Then using a first-order Taylor series expansion around $\\mathbb{E}[y]$ one obtains \n",
    "\n",
    "$$\n",
    "f(y) \\approx f(\\mathbb{E}[y]) + \\frac{\\partial f(y) }{\\partial y} \\Bigr|_{y=\\mathbb{E}[y]}(y - \\mathbb{E}[y]).\n",
    "\\tag{5}\n",
    "$$\n",
    "\n",
    "Substituting $(3)$ in $(5)$ and comparing coefficients with $(4)$ yields\n",
    "\n",
    "$$\n",
    "g_0 \\approx f(\\mathbb{E}[y]) \\\\\n",
    "g_i \\approx \\phi_i \\frac{\\partial f(y) }{\\partial y} \\Bigr|_{y=\\mathbb{E}[y]} .\n",
    "$$\n",
    "\n",
    "Hence, an approximate correction is given by simply scaling the shap values using the gradient of the nonlinear function. Note that in practice one may take the Taylor series expasion at a reference point $r$ from the background dataset and average over the entire background dataset to compute the scaling factor. This introduces an additional source of noise since $f(\\mathbb{E}[y]) = \\mathbb{E}[f(y)]$ only for linear functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shapley interaction values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Describe Shapley interactions\n",
    "- Describe the algorithm to compute them with reference to the path-dependent algorithm\n",
    "- State computational complexity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison to other methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Feature importances (traditional)\n",
    "    - inconsistency and arbitrary choice of explanation concept\n",
    "    - global only, not that much info\n",
    "- Kernel SHAP\n",
    "    - post-hoc modelling so slow, but can be shown to converge "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- summary of arguments to `explain` and what they mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Path-dependent perturbation algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Initialiastion and Fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interventional perturbation algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Explaining model output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Initialiastion and Fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explaining loss functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Initialisation and fit "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shapley interaction values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Initialisation and fit "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Miscellaneous"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Runtime considerations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Adjusting the size of the reference dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm automatically warns the user if a background dataset size of more than `300` samples is passed. If the runtime of an explanation with the original dataset is too large, then the algorithm can automatically subsample the background dataset during the `fit` step. This can be achieve by specifying the fit step as \n",
    "\n",
    "```python\n",
    "explainer.fit(\n",
    "    X_reference,\n",
    "    summarise_background=True,\n",
    "    n_background_samples=150,\n",
    ")\n",
    "```\n",
    "\n",
    "or \n",
    "```python\n",
    "explainer.fit(\n",
    "    X_reference,\n",
    "    summarise_background='auto'\n",
    ")\n",
    "```\n",
    "\n",
    "The `auto` option will select `300` examples, whereas using the boolean argument allows the user to directly control the size of the reference set. If categorical variables or grouping options are specified, the algorithm uses subsampling of the data. Otherwise, a kmeans clustering algorithm is used to select the background dataset and the samples are weighted according to the frequency of occurence of the cluster they are assigned to, which is reflected in the `expected_value` attribute of the explainer. \n",
    "\n",
    "As describe above, the explanations are performed with respect to the expected (or weighted-average) output over this dataset so the shap values will be affected by the dataset selection. We recommend experimenting with various ways to choose the background dataset before deploying explanations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The dimensionality of the data and the number of samples used in shap value estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='References'></a>\n",
    "\n",
    "[[1]](#source_1) Lundberg, S.M. and Lee, S.I., 2017. A unified approach to interpreting model predictions. In Advances in neural information processing systems (pp. 4765-4774).\n",
    "\n",
    "[[2]](#source_2) Janzing, D., Minorics, L. and Bl√∂baum, P., 2019. Feature relevance quantification in explainable AI: A causality problem. arXiv preprint arXiv:1910.13413."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuous Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Introductory example: Kernel SHAP on Wine dataset](../examples/kernel_shap_wine_intro.nblink)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Comparison with interpretable models](../examples/kernel_shap_wine_lr.nblink)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mixed Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Handling categorical variables with Kernel SHAP: an income prediction application](../examples/kernel_shap_adult_lr.nblink)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Handlling categorical variables with Kernel SHAP: fitting explainers on data before pre-processing](../examples/kernel_shap_adult_categorical_preproc.nblink)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
