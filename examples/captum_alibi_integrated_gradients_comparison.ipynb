{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF version:  2.3.0\n",
      "Eager execution enabled:  True\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Embedding, Conv1D, GlobalMaxPooling1D, Dropout \n",
    "from tensorflow.keras.layers import MaxPooling1D, Flatten, Activation\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from alibi.explainers import IntegratedGradients\n",
    "from captum.attr import LayerIntegratedGradients, TokenReferenceBase\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "print('TF version: ', tf.__version__)\n",
    "print('Eager execution enabled: ', tf.executing_eagerly()) # True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 10000\n",
    "maxlen = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000 train sequences\n",
      "25000 test sequences\n",
      "Pad sequences (samples x time)\n",
      "x_train shape: (25000, 100)\n",
      "x_test shape: (25000, 100)\n"
     ]
    }
   ],
   "source": [
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "test_labels = y_test.copy()\n",
    "train_labels = y_train.copy()\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "y_train, y_test = to_categorical(y_train), to_categorical(y_test)\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "\n",
    "index = imdb.get_word_index()\n",
    "reverse_index = {value: key for (key, value) in index.items()} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sentence(x, reverse_index):\n",
    "    # the `-3` offset is due to the special tokens used by keras\n",
    "    # see https://stackoverflow.com/questions/42821330/restore-original-text-from-keras-s-imdb-dataset\n",
    "    return \" \".join([reverse_index.get(i - 3, 'UNK') for i in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a powerful study of loneliness sexual UNK and desperation be patient UNK up the atmosphere and pay attention to the wonderfully written script br br i praise robert altman this is one of his many films that deals with unconventional fascinating subject matter this film is disturbing but it's sincere and it's sure to UNK a strong emotional response from the viewer if you want to see an unusual film some might even say bizarre this is worth the time br br unfortunately it's very difficult to find in video stores you may have to buy it off the internet\n"
     ]
    }
   ],
   "source": [
    "print(decode_sentence(x_test[1], reverse_index)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "embedding_dims = 50\n",
    "filters = 250\n",
    "kernel_size = 3\n",
    "hidden_dims = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.emb = nn.Embedding(max_features,\n",
    "                               embedding_dims)\n",
    "        self.linear1 = nn.Linear(5000, hidden_dims)\n",
    "        self.linear2 = nn.Linear(hidden_dims, 2)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.emb(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.linear1(x)\n",
    "        x = F.relu(x) # Adding relu layers makes the attributions different\n",
    "        x = self.linear2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "inputs = Input(shape=(maxlen,), \n",
    "               dtype='int32', \n",
    "               name='inputs')\n",
    "out = Embedding(max_features,\n",
    "                embedding_dims, \n",
    "                name='emb')(inputs)\n",
    "out = Flatten(name='Flat', \n",
    "              data_format='channels_last')(out)\n",
    "out = Dense(hidden_dims, \n",
    "            name='linear1')(out)\n",
    "out = Activation('relu')(out) \n",
    "# Adding relu layers makes the attributions different\n",
    "out = Dense(2, \n",
    "            name='linear2')(out)\n",
    "model = Model(inputs=inputs, outputs=out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (emb): Embedding(10000, 50)\n",
       "  (linear1): Linear(in_features=5000, out_features=250, bias=True)\n",
       "  (linear2): Linear(in_features=250, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = Net()\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# transfer weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lnames = []\n",
    "for name in net.state_dict().keys():\n",
    "    lname = name.split('.')[0]\n",
    "    if lname not in lnames:\n",
    "        lnames.append(lname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emb (10000, 50)\n",
      "linear1 (5000, 250)\n",
      "linear2 (250, 2)\n"
     ]
    }
   ],
   "source": [
    "for name in lnames:\n",
    "\n",
    "    if 'conv' in name:\n",
    "        ws = net.state_dict()[name + '.weight'].cpu().numpy()\n",
    "        ws = np.transpose(ws, (2, 1, 0))\n",
    "        bs = net.state_dict()[name + '.bias'].cpu().numpy()\n",
    "        l = model.get_layer(name)\n",
    "        l.set_weights([ws, bs])\n",
    "    elif 'linear' in name:\n",
    "        ws = net.state_dict()[name + '.weight'].cpu().numpy()\n",
    "        ws = ws.T\n",
    "        bs = net.state_dict()[name + '.bias'].cpu().numpy()\n",
    "        l = model.get_layer(name)\n",
    "        l.set_weights([ws, bs])\n",
    "    elif 'emb' in name:\n",
    "        ws = net.state_dict()[name + '.weight'].cpu().numpy()\n",
    "        l = model.get_layer(name)\n",
    "        l.set_weights([ws])\n",
    "    print(name, ws.shape) #  , bs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_emb_pt = net.state_dict()['emb.weight']\n",
    "weights_emb_tf = model.layers[1].get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(weights_emb_pt, weights_emb_tf, rtol=1e-03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_samples = 10\n",
    "torch_X_test = torch.from_numpy(x_test)\n",
    "torch_y_test = torch.from_numpy(y_test)\n",
    "x_test_sample = torch_X_test[nb_samples:nb_samples + 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10, 2), dtype=float32, numpy=\n",
       "array([[ 0.03181124,  0.01722053],\n",
       "       [ 0.051693  ,  0.13679706],\n",
       "       [ 0.1720075 ,  0.2850133 ],\n",
       "       [ 0.16410483,  0.47410205],\n",
       "       [ 0.12408741,  0.5561405 ],\n",
       "       [-0.14426398,  0.24505275],\n",
       "       [-0.13632366,  0.3933563 ],\n",
       "       [-0.04491811,  0.35326967],\n",
       "       [-0.40871558,  0.09026024],\n",
       "       [-0.00379803,  0.17383164]], dtype=float32)>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(x_test_sample.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0318,  0.0172],\n",
       "        [ 0.0517,  0.1368],\n",
       "        [ 0.1720,  0.2850],\n",
       "        [ 0.1641,  0.4741],\n",
       "        [ 0.1241,  0.5561],\n",
       "        [-0.1443,  0.2451],\n",
       "        [-0.1363,  0.3934],\n",
       "        [-0.0449,  0.3533],\n",
       "        [-0.4087,  0.0903],\n",
       "        [-0.0038,  0.1738]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(x_test_sample.to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intgrads comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 50\n",
    "method = \"gausslegendre\"\n",
    "internal_batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpret_sentence(model, indexed, min_len = 100, label = 1):\n",
    "\n",
    "    input_indices = indexed.to(device)\n",
    "    seq_length = min_len\n",
    "\n",
    "    # predict\n",
    "    pred = net.forward(input_indices)\n",
    "\n",
    "    # generate reference indices for each sample\n",
    "    reference_indices = torch.tensor(np.zeros(input_indices.shape), dtype=int).to(device)\n",
    "\n",
    "    # compute attributions and approximation delta using layer integrated gradients\n",
    "    attributions_ig, delta = lig.attribute(input_indices, \n",
    "                                           reference_indices, \n",
    "                                           target=label,\n",
    "                                           method=method,\n",
    "                                           n_steps=50, \n",
    "                                           return_convergence_delta=True,\n",
    "                                          attribute_to_layer_input=True)\n",
    "    \n",
    "    return attributions_ig, delta, reference_indices.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "lig = LayerIntegratedGradients(net, net.linear1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attributions shape: (10, 5000)\n"
     ]
    }
   ],
   "source": [
    "token_reference = TokenReferenceBase(reference_token_idx=0)\n",
    "# For simplicity, we compute the attribution relative to label = 1 for all samples\n",
    "attributions_pt, delta, reference_indices = interpret_sentence(net, \n",
    "                                                               x_test_sample, \n",
    "                                                               label=1)\n",
    "attributions_pt = attributions_pt.numpy()\n",
    "print('Attributions shape:', attributions_pt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.layers.core.Dense at 0x7fa2d9b98c50>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer = model.layers[3]\n",
    "layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "ig  = IntegratedGradients(model,\n",
    "                          layer=layer,\n",
    "                          n_steps=n_steps, \n",
    "                          method=method,\n",
    "                          internal_batch_size=internal_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attributions shape: (10, 5000)\n"
     ]
    }
   ],
   "source": [
    "x_test_sample = x_test_sample.numpy()\n",
    "#predictions = model(x_test_sample).numpy().argmax(axis=1)\n",
    "explanation = ig.explain(x_test_sample, \n",
    "                         baselines=reference_indices, \n",
    "                         target=1,\n",
    "                        compute_layer_inputs_gradients=True)\n",
    "# Get attributions values from the explanation object\n",
    "attributions_tf = explanation.attributions[0]\n",
    "print('Attributions shape:', attributions_tf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(attributions_tf, attributions_pt, atol=1e-07)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-4.21469571e-03, -1.62378047e-03,  3.91519864e-03, ...,\n",
       "         4.39232335e-05,  2.64565624e-03, -1.53950182e-03],\n",
       "       [-1.35421988e-03,  1.93412980e-03,  4.48370843e-03, ...,\n",
       "        -2.73249235e-04,  2.34585438e-03,  2.02211036e-03],\n",
       "       [-4.58304672e-03, -3.43286137e-03,  1.30007402e-03, ...,\n",
       "        -2.89742109e-03, -2.24289863e-04, -4.13060519e-03],\n",
       "       ...,\n",
       "       [ 7.79471960e-04, -6.50702091e-03, -3.36324445e-05, ...,\n",
       "        -1.22888147e-03,  9.23896788e-04, -3.11811411e-04],\n",
       "       [ 2.74067355e-03, -1.35195815e-03, -9.73411199e-05, ...,\n",
       "        -3.67314182e-04,  6.62322877e-04, -2.21013898e-03],\n",
       "       [-3.47013454e-04, -1.47317675e-03, -6.65119712e-05, ...,\n",
       "         1.34957540e-03,  4.65485775e-03,  3.84821680e-03]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attributions_pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-4.2146957e-03, -1.6237805e-03,  3.9151986e-03, ...,\n",
       "         4.3923235e-05,  2.6456562e-03, -1.5395018e-03],\n",
       "       [-1.3542199e-03,  1.9341299e-03,  4.4837082e-03, ...,\n",
       "        -2.7324923e-04,  2.3458544e-03,  2.0221104e-03],\n",
       "       [-4.5830468e-03, -3.4328613e-03,  1.3000739e-03, ...,\n",
       "        -2.8974211e-03, -2.2428986e-04, -4.1306051e-03],\n",
       "       ...,\n",
       "       [ 7.7947194e-04, -6.5070209e-03, -3.3632445e-05, ...,\n",
       "        -1.2288815e-03,  9.2389685e-04, -3.1181140e-04],\n",
       "       [ 2.7406735e-03, -1.3519581e-03, -9.7341122e-05, ...,\n",
       "        -3.6731418e-04,  6.6232285e-04, -2.2101388e-03],\n",
       "       [-3.4701344e-04, -1.4731766e-03, -6.6511966e-05, ...,\n",
       "         1.3495754e-03,  4.6548578e-03,  3.8482167e-03]], dtype=float32)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attributions_tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(delta, explanation.deltas, atol=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-4.5890e-03, -4.4222e-04,  3.8949e-03,  2.5060e-03, -2.2662e-03,\n",
       "         2.7603e-03, -1.1770e-03,  1.7273e-03, -2.3336e-06,  8.0885e-03],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-4.58897650e-03, -4.42281365e-04,  3.89492512e-03,  2.50585377e-03,\n",
       "       -2.26621330e-03,  2.76022404e-03, -1.17677450e-03,  1.72734261e-03,\n",
       "       -2.22399831e-06,  8.08829069e-03])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "explanation.deltas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF version:  2.3.0\n",
      "Eager execution enabled:  True\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Embedding, Conv1D, GlobalMaxPooling1D, Dropout \n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from transformers import BertTokenizerFast, TFBertModel, BertConfig\n",
    "from transformers import DistilBertTokenizerFast\n",
    "from transformers import TFDistilBertForSequenceClassification, TFTrainer, TFTrainingArguments\n",
    "from alibi.explainers import IntegratedGradients\n",
    "import matplotlib.pyplot as plt\n",
    "print('TF version: ', tf.__version__)\n",
    "print('Eager execution enabled: ', tf.executing_eagerly()) # True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_reviews(reviews):\n",
    "    \n",
    "    REPLACE_NO_SPACE = re.compile(\"[.;:,!\\'?\\\"()\\[\\]]\")\n",
    "    REPLACE_WITH_SPACE = re.compile(\"(<br\\s*/><br\\s*/>)|(\\-)|(\\/)\")\n",
    "    \n",
    "    reviews = [REPLACE_NO_SPACE.sub(\"\", line.lower()) for line in reviews]\n",
    "    reviews = [REPLACE_WITH_SPACE.sub(\" \", line) for line in reviews]\n",
    "    \n",
    "    return reviews\n",
    "\n",
    "def process_sentences(sentence1, \n",
    "                      tokenizer, \n",
    "                      max_len, \n",
    "                      distill=False, \n",
    "                      add_special_tokens=True):\n",
    "\n",
    "    if not distill:\n",
    "        z = tokenizer(sentence1, \n",
    "                      add_special_tokens = add_special_tokens, \n",
    "                      padding = 'max_length', \n",
    "                      max_length = max_len, truncation = True,\n",
    "                      return_token_type_ids=True, \n",
    "                      return_attention_mask = True,  \n",
    "                      return_tensors = 'np')\n",
    "\n",
    "        return [z['input_ids'], z['attention_mask']]\n",
    "    elif distill:\n",
    "        z = tokenizer(sentence1, \n",
    "                      truncation=True, \n",
    "                      padding=True)\n",
    "        \n",
    "        return z\n",
    "\n",
    "def decode_sentence(x, reverse_index):\n",
    "    # the `-3` offset is due to the special tokens used by keras\n",
    "    # see https://stackoverflow.com/questions/42821330/restore-original-text-from-keras-s-imdb-dataset\n",
    "    return \" \".join([reverse_index.get(i - 3, 'UNK') for i in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 10000\n",
    "max_len = 100\n",
    "distill=False\n",
    "add_special_tokens=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000 train sequences\n",
      "25000 test sequences\n",
      "Pad sequences (samples x time)\n",
      "x_train shape: (25000, 100)\n",
      "x_test shape: (25000, 100)\n"
     ]
    }
   ],
   "source": [
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "test_labels = y_test.copy()\n",
    "train_labels = y_train.copy()\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "y_train, y_test = to_categorical(y_train), to_categorical(y_test)\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=max_len)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=max_len)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "\n",
    "index = imdb.get_word_index()\n",
    "reverse_index = {value: key for (key, value) in index.items()} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = [], []\n",
    "for i in range(len(x_train)):\n",
    "    tr_sentence = decode_sentence(x_train[i], reverse_index)\n",
    "    X_train.append(tr_sentence)\n",
    "    te_sentence = decode_sentence(x_test[i], reverse_index)\n",
    "    X_test.append(te_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = preprocess_reviews(X_train)\n",
    "X_train = process_sentences(X_train, \n",
    "                                   tokenizer, \n",
    "                                   max_len,                                    \n",
    "                                   distill=distill, \n",
    "                                   add_special_tokens=add_special_tokens)\n",
    "X_test = preprocess_reviews(X_test)\n",
    "X_test = process_sentences(X_test, \n",
    "                                   tokenizer, \n",
    "                                   max_len,                                    \n",
    "                                   distill=distill, \n",
    "                                   add_special_tokens=add_special_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type distilbert to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n",
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFBertModel: ['activation_13', 'vocab_projector', 'distilbert', 'vocab_layer_norm', 'vocab_transform']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFBertModel were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['bert']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "config = BertConfig.from_pretrained(\"distilbert-base-uncased\", \n",
    "                                    output_hidden_states=False)\n",
    "modelBert = TFBertModel.from_pretrained(\"distilbert-base-uncased\",\n",
    "                                        config=config)\n",
    "\n",
    "modelBert.trainable=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp =  [X_test[0][:2], X_test[1][:2]]\n",
    "input_shape = [x.shape[1:] for x in inp]\n",
    "input_type = [x.dtype for x in inp]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bert \n",
    "Without a wrapper, IG won't work for bert models because of the customized bert output object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 5\n",
    "method = \"gausslegendre\"\n",
    "internal_batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<transformers.models.bert.modeling_tf_bert.TFBertLayer at 0x7fdb42f43310>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer = modelBert.bert.encoder.layer[3]\n",
    "layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "ig  = IntegratedGradients(modelBert,\n",
    "                          layer=layer,\n",
    "                          n_steps=n_steps, \n",
    "                          method=method,\n",
    "                          internal_batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "[<tf.Tensor: shape=(10, 100, 768), dtype=float32, numpy=\n",
      "array([[[ 2.0742157 , -0.8950368 ,  1.8933085 , ...,  0.8202039 ,\n",
      "         -1.897774  ,  2.402349  ],\n",
      "        [ 1.524005  , -0.13682622,  1.255195  , ...,  0.81792676,\n",
      "         -0.6837781 ,  1.349809  ],\n",
      "        [ 1.5790133 , -0.33281478,  1.3452189 , ..., -0.7252396 ,\n",
      "         -0.6827304 ,  0.12341752],\n",
      "        ...,\n",
      "        [ 1.8804123 , -0.98103607,  0.86389995, ...,  0.10786448,\n",
      "         -1.0245299 ,  1.9350705 ],\n",
      "        [ 2.159008  , -0.1955547 ,  2.0331736 , ...,  0.689219  ,\n",
      "         -0.45227218,  0.3811051 ],\n",
      "        [ 1.8842316 , -1.4659264 ,  2.5653646 , ..., -0.797865  ,\n",
      "         -0.98540187,  1.081907  ]],\n",
      "\n",
      "       [[ 2.071913  , -0.8583547 ,  1.8113096 , ...,  0.79668903,\n",
      "         -1.8640118 ,  2.389125  ],\n",
      "        [ 1.5532464 , -0.12048204,  1.2645011 , ...,  0.83791834,\n",
      "         -0.6803481 ,  1.3865451 ],\n",
      "        [ 1.5945048 , -0.27372402,  1.2716355 , ..., -0.7648175 ,\n",
      "         -0.6778127 ,  0.08656603],\n",
      "        ...,\n",
      "        [ 1.8439963 , -1.0358617 ,  0.86696756, ...,  0.09246579,\n",
      "         -1.0215098 ,  1.9482139 ],\n",
      "        [ 2.1544476 , -0.16228233,  2.0014222 , ...,  0.6645392 ,\n",
      "         -0.45670664,  0.39550683],\n",
      "        [ 1.8844907 , -1.4637387 ,  2.5763366 , ..., -0.79693675,\n",
      "         -1.0287709 ,  1.0470735 ]],\n",
      "\n",
      "       [[ 1.7947739 , -0.91802007,  1.9439371 , ...,  0.8947498 ,\n",
      "         -1.7655612 ,  2.307857  ],\n",
      "        [ 1.1134716 ,  0.08621457,  1.0765243 , ...,  0.7981689 ,\n",
      "         -0.5197639 ,  1.2280357 ],\n",
      "        [ 1.3266292 , -0.3548249 ,  1.3647093 , ..., -0.7070456 ,\n",
      "         -0.51975226,  0.01126   ],\n",
      "        ...,\n",
      "        [ 1.6759871 , -0.75396216,  0.76322174, ...,  0.0058807 ,\n",
      "         -0.8639398 ,  1.8616924 ],\n",
      "        [ 1.8210578 , -0.3521043 ,  1.8461251 , ...,  0.6869099 ,\n",
      "         -0.16319424,  0.25834072],\n",
      "        [ 1.6923894 , -1.3792505 ,  2.4801042 , ..., -0.86528885,\n",
      "         -0.7950002 ,  0.8928311 ]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[ 0.93859196, -0.3838156 ,  0.74758816, ...,  0.7274846 ,\n",
      "         -0.8247038 ,  1.8142532 ],\n",
      "        [ 0.3906175 ,  1.007462  ,  0.7058437 , ...,  1.0681229 ,\n",
      "          0.01684201,  1.4737947 ],\n",
      "        [ 0.8414878 ,  0.5496842 ,  0.21516228, ..., -1.3027593 ,\n",
      "          0.03821099, -0.9215104 ],\n",
      "        ...,\n",
      "        [ 0.48011935, -0.98794854,  0.51865995, ..., -0.5453148 ,\n",
      "         -0.34408504,  1.8623111 ],\n",
      "        [ 0.7565125 , -0.26500016,  0.7776401 , ...,  0.27544677,\n",
      "          0.61073446,  0.13495103],\n",
      "        [ 1.1347785 , -1.0895163 ,  2.4103055 , ..., -1.0475365 ,\n",
      "         -0.9485307 , -0.2321378 ]],\n",
      "\n",
      "       [[ 0.69691265, -1.0083159 ,  2.1428444 , ...,  1.1876235 ,\n",
      "         -1.2461278 ,  1.9366205 ],\n",
      "        [-0.49941874,  0.9624899 ,  0.3745684 , ...,  0.7205449 ,\n",
      "          0.12460983,  0.74961615],\n",
      "        [ 0.33507025, -0.44129765,  1.4412819 , ..., -0.6355658 ,\n",
      "          0.12055105, -0.42938083],\n",
      "        ...,\n",
      "        [ 0.8728479 ,  0.13815832,  0.36768025, ..., -0.39478981,\n",
      "         -0.2330187 ,  1.5734068 ],\n",
      "        [ 0.49332976, -0.96715117,  1.1112546 , ...,  0.677838  ,\n",
      "          0.9725257 , -0.22397202],\n",
      "        [ 0.93868595, -1.0387211 ,  2.1451366 , ..., -1.1301816 ,\n",
      "         -0.04695612,  0.14999568]],\n",
      "\n",
      "       [[ 0.65012467, -0.2630298 ,  0.4768362 , ...,  0.7098698 ,\n",
      "         -0.5601659 ,  1.6679294 ],\n",
      "        [ 0.09469032,  1.2945607 ,  0.56364703, ...,  1.1267176 ,\n",
      "          0.19429982,  1.4960027 ],\n",
      "        [ 0.64982027,  0.7592686 , -0.05374491, ..., -1.4396832 ,\n",
      "          0.22046244, -1.1780989 ],\n",
      "        ...,\n",
      "        [ 0.13296795, -0.97575307,  0.4300043 , ..., -0.70765084,\n",
      "         -0.17165822,  1.840446  ],\n",
      "        [ 0.40069222, -0.2911452 ,  0.46614742, ...,  0.17641002,\n",
      "          0.8824332 ,  0.06863102],\n",
      "        [ 0.94395214, -0.9942644 ,  2.3680449 , ..., -1.1113224 ,\n",
      "         -0.9281069 , -0.557739  ]]], dtype=float32)>]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'>' not supported between instances of 'tuple' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-81-81504eab8463>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m explanation = ig.explain(inp, \n\u001b[1;32m      2\u001b[0m                          \u001b[0mbaselines\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m                          target=1)\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m# Get attributions values from the explanation object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mattributions_tf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexplanation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattributions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/fork-alibi-2/alibi/explainers/integrated_gradients.py\u001b[0m in \u001b[0;36mexplain\u001b[0;34m(self, X, baselines, target)\u001b[0m\n\u001b[1;32m    687\u001b[0m                                                                                  \u001b[0mstep_sizes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m                                                                                  \u001b[0malphas\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 689\u001b[0;31m                                                                                  nb_samples)\n\u001b[0m\u001b[1;32m    690\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m                     attributions, deltas = self._compute_attributions_tensor_input(X_layer,\n",
      "\u001b[0;32m~/git/fork-alibi-2/alibi/explainers/integrated_gradients.py\u001b[0m in \u001b[0;36m_compute_attributions_list_input\u001b[0;34m(self, X, baselines, target, step_sizes, alphas, nb_samples)\u001b[0m\n\u001b[1;32m    860\u001b[0m                                            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morig_dummy_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m                                            \u001b[0mpaths_b\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 862\u001b[0;31m                                            target_b)\n\u001b[0m\u001b[1;32m    863\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    864\u001b[0m             \u001b[0mbatches\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads_b\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/fork-alibi-2/alibi/explainers/integrated_gradients.py\u001b[0m in \u001b[0;36m_gradients_layer\u001b[0;34m(model, layer, orig_call, orig_dummy_input, x, target)\u001b[0m\n\u001b[1;32m    341\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m         \u001b[0mwatch_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 343\u001b[0;31m         \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morig_dummy_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/fork-alibi-2/alibi/explainers/integrated_gradients.py\u001b[0m in \u001b[0;36m_run_forward\u001b[0;34m(model, x, target)\u001b[0m\n\u001b[1;32m    132\u001b[0m     \"\"\"\n\u001b[1;32m    133\u001b[0m     \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_shape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m         \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_select_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: '>' not supported between instances of 'tuple' and 'int'"
     ]
    }
   ],
   "source": [
    "explanation = ig.explain(inp, \n",
    "                         baselines=None, \n",
    "                         target=1)\n",
    "# Get attributions values from the explanation object\n",
    "attributions_tf = explanation.attributions[0]\n",
    "print('Attributions shape:', attributions_tf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bert Wrapper\n",
    "Using a wrapper such that the output is a tensor, IG should work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertWrapper(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, \n",
    "                modelBert):\n",
    "        super(BertWrapper, self).__init__()\n",
    "        self.bert_layer = modelBert\n",
    "        \n",
    "    def call(self, x):\n",
    "        return self.bert_layer(x)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_wrapper = BertWrapper(modelBert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 5\n",
    "method = \"gausslegendre\"\n",
    "internal_batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<transformers.models.bert.modeling_tf_bert.TFBertLayer at 0x7f95a642ac90>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer = bert_wrapper.layers[0].bert.encoder.layer[3]\n",
    "layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ig  = IntegratedGradients(bert_wrapper,\n",
    "                          layer=layer,\n",
    "                          n_steps=n_steps, \n",
    "                          method=method,\n",
    "                          internal_batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7f9664fb6b40>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7f9664fb6b40>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "Attributions shape: (2, 100, 768)\n"
     ]
    }
   ],
   "source": [
    "explanation = ig.explain(inp, \n",
    "                         baselines=None, \n",
    "                         target=1,\n",
    "                        compute_layer_inputs_gradients=True)\n",
    "# Get attributions values from the explanation object\n",
    "attributions_tf = explanation.attributions[0]\n",
    "print('Attributions shape:', attributions_tf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-5.3308293e-04, -5.6819106e-04,  1.3113442e-04, ...,\n",
       "         -6.6759887e-05,  7.3145190e-04,  1.0570344e-03],\n",
       "        [-1.6599990e-01,  8.5477479e-02,  3.1553423e-01, ...,\n",
       "          2.1606695e-02, -4.2951778e-03, -7.0996985e-02],\n",
       "        [ 6.8114678e-05, -7.7692693e-04, -1.0643669e-04, ...,\n",
       "         -1.3060689e-04,  1.8519655e-04,  9.0612390e-04],\n",
       "        ...,\n",
       "        [-1.6729332e-06,  8.8790970e-05,  2.9183851e-04, ...,\n",
       "         -2.7614128e-04,  3.5111469e-04,  2.8110386e-05],\n",
       "        [ 5.1933084e-06, -3.7488635e-04, -6.3839936e-05, ...,\n",
       "          8.2263956e-04, -1.1921322e-04,  5.1259394e-05],\n",
       "        [-1.4389029e-05, -1.1729125e-03, -6.6376990e-04, ...,\n",
       "         -6.6033425e-04,  1.3904712e-05,  1.3005122e-03]],\n",
       "\n",
       "       [[-9.0011854e-05,  7.5423100e-05,  5.5642970e-05, ...,\n",
       "          3.6419783e-04, -2.0745641e-05,  1.1699090e-03],\n",
       "        [-4.7081542e-01,  6.3265628e-01, -3.5147730e-01, ...,\n",
       "          1.1277145e-02, -9.5583908e-02, -6.0068276e-02],\n",
       "        [ 5.7949310e-06, -1.4460691e-04,  2.6365989e-04, ...,\n",
       "          7.6020369e-04, -7.7744517e-06, -1.4906930e-04],\n",
       "        ...,\n",
       "        [-8.9077228e-05,  1.3102338e-04, -1.2552928e-05, ...,\n",
       "         -1.3400393e-03, -1.8219730e-04,  2.1193075e-05],\n",
       "        [ 1.9794026e-04, -7.5361796e-04,  1.1470889e-05, ...,\n",
       "          1.5102306e-04,  3.6390786e-04,  3.6578762e-04],\n",
       "        [-2.1781298e-06, -8.7685192e-05,  8.4386178e-05, ...,\n",
       "          2.2025574e-04,  9.6740856e-05,  8.7855413e-04]]], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attributions_tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ModelOut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_filters=64\n",
    "dropout_1=0.4\n",
    "dropout_2=0.\n",
    "hidden_dims=128\n",
    "batch_size = 128\n",
    "epochs = 20\n",
    "skip_conv=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelOut(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, \n",
    "                 nb_filters=32,\n",
    "                 dropout_1=0.2,\n",
    "                 dropout_2=0.2, \n",
    "                 hidden_dims=32,\n",
    "                skip_conv=True):\n",
    "        super(ModelOut, self).__init__()\n",
    "        \n",
    "        self.nb_filters = nb_filters\n",
    "        self.dropout_1 = dropout_1\n",
    "        self.dropout_2 = dropout_2\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.skip_conv = skip_conv\n",
    "        \n",
    "        if not self.skip_conv:\n",
    "            self.conv = tf.keras.layers.Conv1D(nb_filters, \n",
    "                                               kernel_size=3, \n",
    "                                               padding=\"valid\", \n",
    "                                               strides=1)\n",
    "            self.dropoutl_1 = tf.keras.layers.Dropout(dropout_1)\n",
    "            self.maxpool = tf.keras.layers.GlobalMaxPool1D()\n",
    "        else:\n",
    "            self.flat = tf.keras.layers.Flatten()\n",
    "            \n",
    "        #self.dense_1 =  tf.keras.layers.Dense(hidden_dims, \n",
    "        #                                      activation='relu')\n",
    "        #self.dropoutl_2 = tf.keras.layers.Dropout(dropout_2)\n",
    "        self.dense_2 = tf.keras.layers.Dense(2, \n",
    "                                             activation='softmax')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        if not self.skip_conv:\n",
    "            x = self.conv(inputs)\n",
    "            x = self.dropoutl_1(x)\n",
    "            x = self.maxpool(x)\n",
    "        else:\n",
    "            x = self.flat(inputs)\n",
    "            \n",
    "        #x = self.dense_1(x)\n",
    "        #x = self.dropoutl_2(x)\n",
    "        x = self.dense_2(x)\n",
    "        return x\n",
    "    \n",
    "    def get_config(self):\n",
    "        return {\"nb_filters\": self.nb_filters,\n",
    "                \"dropout_1\": self.dropout_1,\n",
    "                \"dropout_2\": self.dropout_2, \n",
    "                \"hidden_dims\": self.hidden_dims,\n",
    "               \"skip_conv\": self.skip_conv}\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_out = ModelOut(nb_filters=nb_filters,\n",
    "                 dropout_1=dropout_1,\n",
    "                 dropout_2=dropout_2, \n",
    "                 hidden_dims=hidden_dims,\n",
    "                    skip_conv=skip_conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    }
   ],
   "source": [
    "input_ids_in = tf.keras.layers.Input(shape=(max_len,), \n",
    "                                     name='input_ids', \n",
    "                                     dtype=tf.int32)\n",
    "attention_masks_in = tf.keras.layers.Input(shape=(max_len,), \n",
    "                                           name='attention_mask', \n",
    "                                           dtype=tf.int32)\n",
    "X = modelBert([input_ids_in, attention_masks_in])[0]\n",
    "X = model_out(X)\n",
    "frozenModelOut = tf.keras.Model(inputs=[input_ids_in, \n",
    "                                        attention_masks_in], \n",
    "                                outputs=X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       "array([[0.4026964 , 0.59730357],\n",
       "       [0.26494804, 0.73505193]], dtype=float32)>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frozenModelOut(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 5\n",
    "method = \"gausslegendre\"\n",
    "internal_batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<transformers.models.bert.modeling_tf_bert.TFBertLayer at 0x7fa2d4229fd0>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer = frozenModelOut.layers[2].bert.encoder.layer[1]\n",
    "layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "ig  = IntegratedGradients(frozenModelOut,\n",
    "                          layer=layer,\n",
    "                          n_steps=n_steps, \n",
    "                          method=method,\n",
    "                          internal_batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = frozenModelOut(inp).numpy().argmax(axis=1)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attributions shape: (2, 100, 768)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "explanation = ig.explain(inp, \n",
    "                         baselines=None, \n",
    "                         target=predictions,\n",
    "                        compute_layer_inputs_gradients=True)\n",
    "# Get attributions values from the explanation object\n",
    "attributions_tf = explanation.attributions[0]\n",
    "print('Attributions shape:', attributions_tf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-9.9543695e-07, -2.2059334e-05, -1.0523251e-03, ...,\n",
       "         -1.2000428e-04, -7.1018934e-04,  1.5115821e-04],\n",
       "        [-3.8008086e-04,  4.9328115e-05, -1.2376311e-05, ...,\n",
       "         -1.3036310e-05,  3.9209600e-04,  4.6793887e-04],\n",
       "        [-2.6385476e-06,  4.7545458e-05,  7.4247940e-04, ...,\n",
       "          1.7666531e-04, -4.4124055e-04, -6.2729610e-04],\n",
       "        ...,\n",
       "        [ 9.0255175e-04,  3.5623176e-05, -6.8455399e-04, ...,\n",
       "          3.5659878e-03, -3.7591232e-04, -3.7780768e-04],\n",
       "        [-2.3484734e-04, -1.1101482e-05, -5.5904465e-04, ...,\n",
       "         -1.1464040e-03, -1.8696446e-03,  3.2213450e-04],\n",
       "        [ 8.5562188e-04,  4.8604206e-04, -1.7912698e-04, ...,\n",
       "          1.1792754e-04,  5.0187798e-04,  2.1630540e-04]],\n",
       "\n",
       "       [[ 7.9523292e-05,  4.6781803e-04, -1.0212285e-03, ...,\n",
       "         -5.7124951e-05,  4.9771642e-04, -2.4902020e-05],\n",
       "        [ 8.1456332e-05,  1.2103335e-05, -2.4234883e-05, ...,\n",
       "         -4.2496040e-04, -3.9054572e-05,  6.4270105e-04],\n",
       "        [ 8.9010333e-05, -1.0763438e-04, -9.4033421e-05, ...,\n",
       "         -1.2791295e-03,  8.5418369e-04,  4.8563813e-04],\n",
       "        ...,\n",
       "        [ 6.9110701e-04,  1.0661990e-03,  7.5533171e-04, ...,\n",
       "          1.5782336e-03,  1.3999618e-03, -4.9299339e-04],\n",
       "        [ 9.9767036e-05,  4.2103425e-06,  4.1255122e-04, ...,\n",
       "         -5.2672857e-04,  4.1759195e-04, -3.6519389e-06],\n",
       "        [ 5.3783070e-04,  3.0905419e-04,  2.5024486e-04, ...,\n",
       "          2.0431196e-04,  2.7535402e-04, -2.0046558e-05]]], dtype=float32)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attributions_tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# It gets messy below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, Callable, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_sample = x_test_sample.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = [1 for _ in range(len(x_test_sample))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_dummy_input = np.zeros((1,) + x_test_sample.shape[1:], dtype=x_test_sample.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _gradients_layer(model: Union[tf.keras.models.Model],\n",
    "                     layer: Union[tf.keras.layers.Layer],\n",
    "                     orig_call: Callable,\n",
    "                     orig_dummy_input: Union[list, np.ndarray],\n",
    "                     x: tf.Tensor,\n",
    "                     target: Union[None, tf.Tensor],\n",
    "                    gradients_layer_inputs = False) -> tf.Tensor:\n",
    "    \"\"\"\n",
    "    Calculates the gradients of the target class output (or the output if the output dimension is equal to 1)\n",
    "    with respect to each element of `layer`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model\n",
    "        Tensorflow or keras model.\n",
    "    layer\n",
    "        Layer of the model with respect to which the gradients are calculated.\n",
    "    orig_call\n",
    "        Original `call` method of the layer. This is necessary since the call method is modified by the function\n",
    "        in order to make the layer output visible to the GradientTape.\n",
    "    x\n",
    "        Input data point.\n",
    "    target\n",
    "        Target for which the gradients are calculated if the output dimension is higher than 1.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        Gradients for each element of layer.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def watch_layer(layer, tape):\n",
    "        \"\"\"\n",
    "        Make an intermediate hidden `layer` watchable by the `tape`.\n",
    "        After calling this function, you can obtain the gradient with\n",
    "        respect to the output of the `layer` by calling:\n",
    "\n",
    "            grads = tape.gradient(..., layer.result)\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        def decorator(func):\n",
    "            def wrapper(*args, **kwargs):\n",
    "                # Store the result and the input of `layer.call` internally.\n",
    "                if gradients_layer_inputs:\n",
    "                    layer.inp = x\n",
    "                    layer.result = func(x, **kwargs)\n",
    "                else:\n",
    "                    layer.inp = args\n",
    "                    layer.result = x\n",
    "                # From this point onwards, watch this tensors.\n",
    "                tape.watch(layer.inp)\n",
    "                tape.watch(layer.result)\n",
    "                # Return the result to continue with the forward pass.\n",
    "                return layer.result\n",
    "\n",
    "            return wrapper\n",
    "\n",
    "        layer.call = decorator(layer.call)\n",
    "        return layer\n",
    "    print(orig_dummy_input)\n",
    "    with tf.GradientTape() as tape:\n",
    "        watch_layer(layer, tape)\n",
    "        preds = _run_forward(model, orig_dummy_input, target)\n",
    "    \n",
    "    if gradients_layer_inputs:\n",
    "        grads = tape.gradient(preds, layer.inp)\n",
    "    else:\n",
    "        grads = tape.gradient(preds, layer.result)\n",
    "    \n",
    "    delattr(layer, 'inp')\n",
    "    delattr(layer, 'result')\n",
    "    layer.call = orig_call\n",
    "\n",
    "    return grads\n",
    "\n",
    "def _run_forward(model: Union[tf.keras.models.Model],\n",
    "                 x: Union[List[tf.Tensor], List[np.ndarray]],\n",
    "                 target: Union[None, tf.Tensor, np.ndarray, list]) -> tf.Tensor:\n",
    "    \"\"\"\n",
    "    Returns the output of the model. If the target is not `None`, only the output for the selected target is returned.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model\n",
    "        Tensorflow or keras model.\n",
    "    x\n",
    "        Input data point.\n",
    "    target\n",
    "        Target for which the gradients are calculated for classification models.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        Model output or model output after target selection for classification models.\n",
    "\n",
    "    \"\"\"\n",
    "    print(x)\n",
    "    preds = model(x)\n",
    "    if len(model.output_shape) > 1 and model.output_shape[-1] > 1:\n",
    "        preds = _select_target(preds, target)\n",
    "\n",
    "    return preds\n",
    "\n",
    "def _select_target(ps, ts):\n",
    "    if ts is not None:\n",
    "        if isinstance(ps, tf.Tensor):\n",
    "            ps = tf.linalg.diag_part(tf.gather(ps, ts, axis=1))\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "    else:\n",
    "        raise ValueError(\"target cannot be `None` if `model` output dimensions > 1\")\n",
    "    return ps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _run_forward_to_layer(model: Union[tf.keras.models.Model, 'keras.models.Model'],\n",
    "                            layer: Union[tf.keras.layers.Layer, 'keras.layers.Layer'],\n",
    "                            orig_call: Callable,\n",
    "                            x: tf.Tensor,\n",
    "                         run_forward_to_layer_input: bool = False) -> tf.Tensor:\n",
    "\n",
    "    def take_layer(layer):\n",
    "        \"\"\"\n",
    "        Make an intermediate hidden `layer` watchable by the `tape`.\n",
    "        After calling this function, you can obtain the gradient with\n",
    "        respect to the output of the `layer` by calling:\n",
    "\n",
    "            grads = tape.gradient(..., layer.result)\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        def decorator(func):\n",
    "            def wrapper(*args, **kwargs):\n",
    "                # Store the result of `layer.call` internally.\n",
    "                layer.inp = args\n",
    "                layer.result = func(*args, **kwargs)\n",
    "                # Return the result to continue with the forward pass.\n",
    "                return layer.result\n",
    "\n",
    "            return wrapper\n",
    "\n",
    "        layer.call = decorator(layer.call)\n",
    "        return layer\n",
    "\n",
    "    #inp = tf.zeros((x.shape[0], ) + model.input_shape[1:])\n",
    "    take_layer(layer)\n",
    "    _ = model(x)\n",
    "    layer_inp = layer.inp\n",
    "    layer_out = layer.result\n",
    "    \n",
    "    delattr(layer, 'inp')    \n",
    "    delattr(layer, 'result')\n",
    "    layer.call = orig_call\n",
    "    \n",
    "    if run_forward_to_layer_input:\n",
    "        return layer_inp\n",
    "    else:\n",
    "        return layer_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.layers.core.Dense at 0x7f22d4433190>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer = model.layers[3]\n",
    "layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_call = layer.call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_layer_out = _run_forward_to_layer(model, layer, orig_call, x_test_sample, run_forward_to_layer_input=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10, 250), dtype=float32, numpy=\n",
       "array([[-1.8045537 , -0.4056986 ,  0.02945525, ...,  0.5100677 ,\n",
       "         1.0586026 ,  0.2051692 ],\n",
       "       [-0.50673413, -0.67081714, -0.13903496, ...,  0.66713613,\n",
       "        -0.33622786, -0.4687015 ],\n",
       "       [-0.5948938 , -0.31598407, -0.5050943 , ..., -0.22554867,\n",
       "         0.9042289 , -0.46439877],\n",
       "       ...,\n",
       "       [-0.7473105 ,  0.08260478, -0.8872134 , ..., -0.2511767 ,\n",
       "        -0.05167651,  0.4660214 ],\n",
       "       [-0.5327522 ,  0.592322  , -0.5168347 , ...,  0.3552242 ,\n",
       "        -1.0856992 , -0.42254546],\n",
       "       [ 0.20351203, -0.8673625 ,  0.66974527, ...,  0.22718716,\n",
       "        -0.7815447 ,  0.5024527 ]], dtype=float32)>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_layer_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "grads_out = _gradients_layer(model, \n",
    "                             layer, \n",
    "                             orig_call, \n",
    "                             orig_dummy_input, \n",
    "                             X_layer_out, \n",
    "                             target, \n",
    "                             gradients_layer_inputs=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10, 250), dtype=float32, numpy=\n",
       "array([[-0.        ,  0.        ,  0.04052283, ..., -0.02394654,\n",
       "         0.01421037, -0.05370912],\n",
       "       [-0.        ,  0.        ,  0.        , ..., -0.02394654,\n",
       "         0.        , -0.        ],\n",
       "       [-0.        ,  0.        ,  0.        , ..., -0.        ,\n",
       "         0.01421037, -0.        ],\n",
       "       ...,\n",
       "       [-0.        ,  0.05601896,  0.        , ..., -0.        ,\n",
       "         0.        , -0.05370912],\n",
       "       [-0.        ,  0.05601896,  0.        , ..., -0.02394654,\n",
       "         0.        , -0.        ],\n",
       "       [-0.06189159,  0.        ,  0.04052283, ..., -0.02394654,\n",
       "         0.        , -0.05370912]], dtype=float32)>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grads_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([10, 250])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grads_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_layer_inp = _run_forward_to_layer(model, layer, orig_call, x_test_sample, run_forward_to_layer_input=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_layer_inp = list(X_layer_inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "grads_inp = _gradients_layer(model, \n",
    "                             layer, \n",
    "                             orig_call, \n",
    "                             orig_dummy_input, \n",
    "                             X_layer_inp[0], \n",
    "                             target, \n",
    "                             gradients_layer_inputs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([10, 5000])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grads_inp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10, 5000), dtype=float32, numpy=\n",
       "array([[ 0.00215829,  0.00053226, -0.00132799, ...,  0.00082246,\n",
       "        -0.00292184, -0.00209101],\n",
       "       [ 0.00397081,  0.00129692,  0.00639185, ..., -0.00112882,\n",
       "        -0.00440302,  0.0033445 ],\n",
       "       [-0.00123563,  0.00191363,  0.00069667, ..., -0.00538727,\n",
       "        -0.00228617, -0.00169568],\n",
       "       ...,\n",
       "       [ 0.00401346, -0.00294157, -0.0004713 , ...,  0.00027939,\n",
       "        -0.0032652 ,  0.00195899],\n",
       "       [ 0.00205418,  0.0014013 ,  0.00123545, ..., -0.00320662,\n",
       "        -0.00349352,  0.00186839],\n",
       "       [ 0.00040617,  0.00227149, -0.00106775, ..., -0.00035076,\n",
       "        -0.00024771,  0.00031978]], dtype=float32)>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grads_inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_call = layer.call\n",
    "orig_dummy_input = [np.zeros((1,) + inpp.shape[1:]) for inpp in inp]\n",
    "target = [1 for _ in range(nb_samples)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor: shape=(10, 100, 768), dtype=float32, numpy=\n",
      "array([[[ 0.03096277, -1.328463  , -0.58381706, ..., -1.542637  ,\n",
      "         -1.6382638 ,  1.5585074 ],\n",
      "        [ 1.2325355 , -0.08492582, -2.1955416 , ..., -1.0991684 ,\n",
      "         -0.85656124,  0.4230998 ],\n",
      "        [ 2.0808983 , -2.5597546 , -0.48032314, ..., -2.0058136 ,\n",
      "         -0.80445063,  1.6266632 ],\n",
      "        ...,\n",
      "        [ 2.8055623 , -1.453405  ,  0.88466865, ..., -2.4409838 ,\n",
      "         -1.542648  ,  1.0648844 ],\n",
      "        [-1.363974  , -0.9465396 , -2.29329   , ..., -0.30235538,\n",
      "         -1.2025822 ,  0.765783  ],\n",
      "        [ 2.048068  , -1.3881792 ,  1.1389545 , ..., -0.78310555,\n",
      "         -0.54225874, -0.32534844]],\n",
      "\n",
      "       [[-0.17857285, -1.4808518 , -0.613645  , ..., -0.76305103,\n",
      "         -2.339498  , -0.4367423 ],\n",
      "        [ 0.12191792, -1.9209031 , -1.2052633 , ..., -1.2629689 ,\n",
      "         -1.2369899 ,  0.527245  ],\n",
      "        [ 1.8495976 , -3.388928  ,  0.9084114 , ..., -0.3409986 ,\n",
      "         -1.3057836 , -0.5260624 ],\n",
      "        ...,\n",
      "        [ 1.4606541 , -0.98383254,  0.6204835 , ..., -0.5557766 ,\n",
      "         -2.1143317 , -0.33635616],\n",
      "        [-0.33224845, -1.655739  , -1.9336991 , ..., -0.97441256,\n",
      "         -0.7133225 ,  0.878886  ],\n",
      "        [ 1.5527983 , -1.0324365 ,  0.6019331 , ..., -2.0748267 ,\n",
      "         -1.434558  , -0.3562412 ]],\n",
      "\n",
      "       [[-0.55980355, -0.53993726, -0.02758983, ..., -1.3590242 ,\n",
      "         -1.3572825 , -0.02672208],\n",
      "        [ 1.4578927 , -1.7870251 , -1.676738  , ..., -0.22810428,\n",
      "         -2.0347447 ,  0.18414572],\n",
      "        [ 1.1356177 , -1.7270851 ,  0.72832143, ..., -1.3781059 ,\n",
      "          0.00738621,  0.27106145],\n",
      "        ...,\n",
      "        [ 1.9509788 , -2.8788044 ,  0.14302087, ..., -1.7726672 ,\n",
      "         -1.4290007 ,  0.27753142],\n",
      "        [-0.0946225 , -1.3519982 , -1.7353942 , ..., -0.7476701 ,\n",
      "         -1.6829932 , -0.67428416],\n",
      "        [ 1.0945674 , -1.9222606 , -0.45561767, ..., -0.7887287 ,\n",
      "         -0.51281846,  0.95655286]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[-0.56773657, -0.54933447, -0.00643484, ..., -1.3608326 ,\n",
      "         -1.3659931 ,  0.01060203],\n",
      "        [ 1.2365402 , -1.6340032 , -0.93968004, ..., -1.4643162 ,\n",
      "         -2.1922297 , -0.20896281],\n",
      "        [ 1.856956  , -2.3290217 , -0.65582895, ..., -1.2478646 ,\n",
      "         -1.434375  ,  0.02157877],\n",
      "        ...,\n",
      "        [ 0.85669345, -1.9305118 , -0.54887074, ..., -1.8490208 ,\n",
      "         -0.51673037,  1.6338296 ],\n",
      "        [-1.3539139 , -0.517353  , -1.7139237 , ..., -1.2466317 ,\n",
      "         -1.4082516 ,  1.0148484 ],\n",
      "        [ 2.3219252 , -0.91670567,  1.1649079 , ..., -0.50228024,\n",
      "         -1.8202362 , -1.1035713 ]],\n",
      "\n",
      "       [[-0.26476848, -2.0408547 , -0.6635067 , ..., -1.1140864 ,\n",
      "         -1.2913975 ,  1.115398  ],\n",
      "        [ 0.6640927 , -0.87703806, -1.8219388 , ..., -0.6721711 ,\n",
      "         -0.7931881 ,  1.2308602 ],\n",
      "        [ 1.7973366 , -1.8858703 ,  0.33930528, ..., -1.5323637 ,\n",
      "         -0.7005139 , -0.17354746],\n",
      "        ...,\n",
      "        [ 2.0671973 , -0.61860734, -0.7337983 , ..., -2.3256204 ,\n",
      "         -0.91117   ,  0.9016553 ],\n",
      "        [-0.615239  , -0.25577557, -0.41926354, ..., -0.7601588 ,\n",
      "         -1.3402363 ,  0.5025861 ],\n",
      "        [ 2.166276  , -1.5914075 ,  0.50686914, ..., -2.9525335 ,\n",
      "         -2.7100484 , -0.31858817]],\n",
      "\n",
      "       [[-0.24638948, -1.1926562 ,  1.4232162 , ..., -1.4157615 ,\n",
      "         -1.6791648 ,  0.19370091],\n",
      "        [ 0.92016226, -0.62660867, -1.8108704 , ...,  0.06704824,\n",
      "         -0.7945703 ,  0.3168413 ],\n",
      "        [ 1.0318904 , -3.306516  , -0.15023209, ..., -0.5549098 ,\n",
      "         -0.9408676 ,  0.36482397],\n",
      "        ...,\n",
      "        [ 1.274646  , -2.3487499 , -0.29164103, ..., -0.06757417,\n",
      "         -0.7191749 ,  1.4780425 ],\n",
      "        [-0.65564597, -1.1910753 , -1.1519271 , ..., -1.2702265 ,\n",
      "         -1.7005659 ,  0.84637517],\n",
      "        [ 1.8378769 , -2.1421733 , -0.7611267 , ..., -2.1111052 ,\n",
      "         -1.130236  ,  0.8408136 ]]], dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "grads = _gradients_layer(frozenModelOut, layer, orig_call, orig_dummy_input, X_layer, target )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10, 100, 768), dtype=float32, numpy=\n",
       "array([[[ 4.92075109e-04,  6.02714681e-05, -2.32987673e-04, ...,\n",
       "          3.59852042e-04, -3.63110921e-05,  4.38389921e-04],\n",
       "        [ 2.30642516e-04, -5.50971075e-04,  2.32170787e-04, ...,\n",
       "         -7.54989014e-05, -1.28455751e-04, -5.20038127e-04],\n",
       "        [ 3.85588964e-06,  9.03854816e-05, -3.36457997e-05, ...,\n",
       "          1.92570878e-04,  5.98768820e-04, -9.02307729e-05],\n",
       "        ...,\n",
       "        [-8.38256019e-05, -2.25406053e-04,  5.53932368e-05, ...,\n",
       "          3.71104310e-04, -1.60682655e-04, -7.47951155e-04],\n",
       "        [ 2.16036497e-04, -2.48291064e-04, -4.66909805e-05, ...,\n",
       "          1.11414547e-05,  1.36700692e-04,  1.63533186e-04],\n",
       "        [-2.63755064e-04,  5.33933417e-05, -3.49554408e-04, ...,\n",
       "         -9.18781079e-05, -1.38716758e-04, -5.60689543e-04]],\n",
       "\n",
       "       [[ 4.75050125e-04,  4.77776957e-05, -3.33185104e-04, ...,\n",
       "          3.04283283e-04, -1.69587787e-04,  4.77981550e-04],\n",
       "        [ 1.33143709e-04, -5.38074994e-04,  1.18032964e-04, ...,\n",
       "          5.83869260e-05, -6.30836512e-05, -5.82008564e-04],\n",
       "        [-4.95464956e-05,  1.41017983e-04, -5.63348949e-05, ...,\n",
       "          7.28120503e-05,  3.91068257e-04,  8.58694193e-07],\n",
       "        ...,\n",
       "        [ 1.72935052e-05, -9.74636569e-05,  1.27396474e-04, ...,\n",
       "          4.25519247e-04, -4.69491752e-05, -5.13459381e-04],\n",
       "        [ 2.33757630e-04, -1.80755451e-04,  1.15131406e-04, ...,\n",
       "          1.80612376e-04,  9.09620940e-05,  4.32860816e-07],\n",
       "        [-2.59676017e-04,  8.46821204e-05, -4.29391890e-04, ...,\n",
       "         -5.74107253e-05, -1.73686014e-04, -4.89245576e-04]],\n",
       "\n",
       "       [[ 5.49422228e-04,  4.17502706e-05, -4.12323221e-04, ...,\n",
       "          3.90335132e-04, -7.16172362e-05,  3.84455285e-04],\n",
       "        [ 1.87482263e-04, -6.63924497e-04,  2.31719925e-04, ...,\n",
       "         -1.21041398e-04, -3.81496138e-05, -6.52946765e-04],\n",
       "        [-7.60204712e-05,  9.27221481e-05, -1.80884846e-04, ...,\n",
       "          3.31302406e-04,  4.70453291e-04, -7.01616445e-05],\n",
       "        ...,\n",
       "        [-5.54825347e-05, -3.92376569e-05,  4.54963119e-05, ...,\n",
       "          5.51008154e-04, -1.51110115e-04, -7.31984619e-04],\n",
       "        [ 2.11213046e-04, -2.17350927e-04, -1.68405575e-04, ...,\n",
       "          1.45271115e-04, -1.54843165e-05,  6.32718074e-05],\n",
       "        [-2.36608277e-04,  5.20655522e-06, -3.60899838e-04, ...,\n",
       "         -8.25489042e-05, -1.71738298e-04, -5.48158248e-04]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 9.30481765e-04,  6.93392649e-05, -7.13460671e-04, ...,\n",
       "          6.95532188e-04, -1.16524709e-04,  6.73054776e-04],\n",
       "        [ 5.12318453e-04, -1.32981897e-03,  3.53149284e-04, ...,\n",
       "          7.89325059e-05, -9.56611329e-05, -9.24194115e-04],\n",
       "        [-2.64027658e-05,  2.95609294e-04, -2.83802801e-04, ...,\n",
       "          5.29603160e-04,  9.29831353e-04, -1.22781130e-04],\n",
       "        ...,\n",
       "        [-8.67855852e-05, -2.92958313e-04, -1.46899678e-04, ...,\n",
       "          8.76209524e-04, -2.55193998e-04, -1.23286562e-03],\n",
       "        [ 3.33537551e-04, -8.36245308e-04, -2.42482765e-05, ...,\n",
       "          1.32734087e-04,  2.16935368e-04,  1.36010509e-04],\n",
       "        [-3.19927261e-04,  1.67524384e-04, -6.46963366e-04, ...,\n",
       "          3.10246280e-04,  2.09061473e-04, -1.01707596e-03]],\n",
       "\n",
       "       [[ 5.41572517e-04,  4.46965605e-05, -4.13876696e-04, ...,\n",
       "          3.30231909e-04, -2.58763794e-05,  3.45650886e-04],\n",
       "        [ 2.07096396e-04, -4.38843621e-04,  2.50274716e-05, ...,\n",
       "          7.53583081e-05, -7.52474225e-05, -3.53468320e-04],\n",
       "        [-6.45624968e-05,  6.27727786e-05,  2.31326885e-05, ...,\n",
       "          1.90897394e-04,  3.70913680e-04,  4.31801236e-05],\n",
       "        ...,\n",
       "        [-5.87946270e-05, -1.35186739e-04,  1.59663614e-04, ...,\n",
       "          5.55496663e-04, -8.27294498e-05, -5.71633689e-04],\n",
       "        [ 2.34171457e-04, -3.30824492e-04, -1.60943164e-04, ...,\n",
       "          5.00714195e-05,  1.09098175e-04,  1.42811559e-05],\n",
       "        [-2.04462092e-04,  1.17258778e-05, -2.37208689e-04, ...,\n",
       "          1.70011961e-04, -6.80922603e-05, -4.28395288e-04]],\n",
       "\n",
       "       [[ 1.34502281e-03, -3.54035612e-04, -4.71413892e-04, ...,\n",
       "          9.69196903e-04,  1.58614112e-05,  1.12571381e-03],\n",
       "        [ 4.20296506e-04, -1.34296319e-03,  3.96030024e-04, ...,\n",
       "         -1.14150185e-04, -3.48619033e-05, -9.01092251e-04],\n",
       "        [ 1.59727351e-05,  2.49981589e-04,  9.90440021e-05, ...,\n",
       "          4.10914246e-04,  1.03175698e-03, -3.14203906e-04],\n",
       "        ...,\n",
       "        [-4.68150160e-04, -4.09302593e-04,  1.55519156e-04, ...,\n",
       "          8.59722903e-04, -3.09519179e-04, -1.56121794e-03],\n",
       "        [ 6.41624152e-04, -1.06159423e-03, -3.88076296e-04, ...,\n",
       "         -1.18436801e-05,  3.53920390e-04,  1.93693078e-04],\n",
       "        [-2.96529965e-04, -9.96360814e-06, -1.09962793e-03, ...,\n",
       "          3.71976748e-05, -9.20686434e-05, -1.16156240e-03]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grads[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feed layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _run_forward_from_layer(model,\n",
    "                            layer,\n",
    "                            orig_call,\n",
    "                            orig_dummy_input,\n",
    "                            x,\n",
    "                            run_from_layer_inputs = False):\n",
    "\n",
    "    def feed_layer(layer):\n",
    "        \"\"\"\n",
    "        Overwrites the intermediate layer status with the precomputed values `x`.\n",
    "\n",
    "        \"\"\"\n",
    "        def decorator(func):\n",
    "            def wrapper(*args, **kwargs):\n",
    "                # Store the result and the inputs of `layer.call` internally.\n",
    "                if run_from_layer_inputs:\n",
    "                    layer.inp = x\n",
    "                    layer.result = func(*x, **kwargs)\n",
    "                else:\n",
    "                    layer.inp = args\n",
    "                    layer.result = x\n",
    "                # Return the result to continue with the forward pass.\n",
    "                return layer.result\n",
    "\n",
    "            return wrapper\n",
    "\n",
    "        layer.call = decorator(layer.call)\n",
    "        return layer\n",
    "\n",
    "    feed_layer(layer)\n",
    "    preds = model(orig_dummy_input)\n",
    "\n",
    "    delattr(layer, 'inp')\n",
    "    delattr(layer, 'result')\n",
    "    layer.call = orig_call\n",
    "\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "initializer = tf.keras.initializers.Ones()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.keras.Input(shape=(16,))\n",
    "out = tf.keras.layers.Dense(8, \n",
    "                            kernel_initializer=tf.keras.initializers.Ones(),\n",
    "                            name='linear1')(inputs)\n",
    "out = tf.keras.layers.Dense(1, \n",
    "                            kernel_initializer=tf.keras.initializers.Ones(), \n",
    "                            name='linear3')(out)\n",
    "model = tf.keras.Model(inputs=inputs, outputs=out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = model.layers[1]\n",
    "layer\n",
    "orig_call = layer.call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 16)]              0         \n",
      "_________________________________________________________________\n",
      "linear1 (Dense)              (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "linear3 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 145\n",
      "Trainable params: 145\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_input = np.zeros((1, 16))\n",
    "x_input_layer_1 = np.ones((2, 16))\n",
    "x_output_layer_1 = np.ones((3, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(dummy_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_from_layer_inputs = _run_forward_from_layer(model, \n",
    "                                                  layer, \n",
    "                                                  orig_call,\n",
    "                                                  dummy_input,\n",
    "                                                  [tf.convert_to_tensor(x_input_layer_1)],\n",
    "                                                  run_from_layer_inputs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 1), dtype=float32, numpy=\n",
       "array([[128.],\n",
       "       [128.]], dtype=float32)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_from_layer_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer linear3 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preds_from_layer_output = _run_forward_from_layer(model,\n",
    "                                                  layer,\n",
    "                                                  orig_call,\n",
    "                                                  dummy_input,\n",
    "                                                  x_output_layer_1,\n",
    "                                                  run_from_layer_inputs=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 1), dtype=float32, numpy=\n",
       "array([[8.],\n",
       "       [8.],\n",
       "       [8.]], dtype=float32)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_from_layer_output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:intgrad-multiple-inputs]",
   "language": "python",
   "name": "conda-env-intgrad-multiple-inputs-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
